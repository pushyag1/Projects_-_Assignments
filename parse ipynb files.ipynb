{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 10 - assignment - parse ipynb files\n",
    "\n",
    "Submit a notebook that finds all other notebooks used for Data 601 on your computer and generates a list of the modules used​.\n",
    "\n",
    "Look for .ipynb files using the glob module in Python\n",
    "Your analysis should include at least eight .ipynb files. \n",
    "Files used for code other than assignments (eg projects, work from other courses) is acceptable to include\n",
    "Create functions that parse the file and find modules.\n",
    "Use a loop to call the functions on each file.\n",
    "Look for code cells (ignore Markdown and raw cells)\n",
    "Look for lines of code that start with either \"from\" or \"import\"​\n",
    "Look for the keywords only in code input (rather than including cell output content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "\n",
    "import glob\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markdown ['# week 5: openpyxl']\n",
      "markdown ['You get an Excel spreadsheet that has two worksheets, \"patient info\" and \"records\"\\u200b\\n', '\\n', 'Each worksheet has data and visualizations and formulas\\n', 'Each row in each worksheet represents information associated with a single person.. \\u200b\\n', 'Worksheet \"patient info\" has a \"patient id\" column; \\u200b\\n', 'Worksheet \"records\" has a \"p_id\" column\\u200b\\n', 'The \"patient id\" values in \"patient info\" tab are the same variable as \"p_id\" in \"records\". Some people have information on both sheets, while other people have information only on one sheet.\\n', 'Use openpyxl to copy information about patients from \"records\" to \"patient info\"\\u200b. Submit a notebook that reads the Excel spreadsheet and produces a separate spreadsheet with the following modifications:\\u200b\\n', '\\n', 'For each patient identified by \"p_id\" listed in the \"records\" sheet that don\\'t exist in \"patient info,\" create a new row in \"patient info\". The row from \"records\" should be copied to the row in \"patient info.\"\\n', 'For each patient listed in the \"records\" sheet where \"p_id\" appears \"patient info\" under \"patient id,\" copy the data in the row from records to \"patient info\" sheet\\n', 'Make no changes to the visualizations that exist in each worksheet\\u200b\\n', 'Make no changes to the data in the \"records\"\\u200b sheet\\n', \"Write your changes to a new .xlsx file (don't overwrite the original)\\u200b \\u200b\\n\", 'Observations:\\u200b\\n', '\\n', '\"patient info\" worksheet will have new columns (because those new columns exist in the \"records\" worksheet)\\u200b\\n', '\"patient info\" worksheet will have new rows (one new row per patient)\\u200b\\n', 'There will be empty cells in \"patient info\" worksheet\\u200b\\n', 'Use a programmatic (rather than manual) approach to identify which patients appear in both worksheets\\u200b\\n', 'Some cells in both worksheets contain formulas. Copy only values from \"records\" to \"patient info\"\\u200b']\n",
      "code ['#installing openpyxl\\n', '\\n', '!pip install openpyxl\\n', '\\n', '#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/']\n",
      "code ['#importing openpyxl\\n', '\\n', 'import openpyxl\\n', 'import pandas as pd\\n', '\\n', '#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/']\n",
      "code [\"#loading the wrokbook using openpyxl, by uploading the xlsx file to our jupyternote book and then loading it to 'wb'\\n\", '\\n', \"wb = openpyxl.load_workbook('week_05_homework_XLSX_openpyxl.xlsx')\\n\", '\\n', '#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/']\n",
      "code [\"#assigning sheet 'patient info' to sheet1\\n\", \"sheet1 = wb['patient info']\\n\", 'sheet1\\n', '#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/']\n",
      "code [\"#assigning sheet 'records' to sheet2\\n\", \"sheet2 = wb['records']\\n\", 'sheet2\\n', '#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/']\n",
      "code ['#reading the excel sheet for data present in the excel\\n', '\\n', \"df = pd.read_excel (r'week_05_homework_XLSX_openpyxl.xlsx') \\n\", '\\n', '#citation: https://stackoverflow.com/questions/20225110/comparing-two-dataframes-and-getting-the-differences/42652112#42652112']\n",
      "code ['#assigning the workbook and sheets to the respective variables\\n', '\\n', \"xls = pd.ExcelFile('week_05_homework_XLSX_openpyxl.xlsx')\\n\", \"df1 = pd.read_excel(xls, 'patient info')\\n\", \"df2 = pd.read_excel(xls, 'records')\\n\", '\\n', '#citation: https://www.freecodecamp.org/news/how-to-create-read-update-and-search-through-excel-files-using-python-c70680d811d4/']\n",
      "code ['df2']\n",
      "code [\"#renaming the column p_id to patient id, so as to merge both the columns and find the common id's in both the columns\\n\", '\\n', 'df3=df2.rename(columns={\"p_id\": \"patient id\"})\\n', '\\n', '#citation:https://www.datacamp.com/community/tutorials/joining-dataframes-pandas']\n",
      "code ['#column heading of all the columns present in df1, i.e., patient info\\n', '\\n', 'df1.columns']\n",
      "code ['#column heading of all the columns present in df3, i.e., records, after renaming the column\\n', '\\n', 'df3.columns']\n",
      "code ['#the renamed data set\\n', '\\n', 'df3']\n",
      "code []\n",
      "code ['#merging df1 and renamed dataframe (df3) using outer join\\n', '\\n', \"df3_col = pd.merge(df1, df3, on='patient id',how='outer')\\n\", '\\n', 'df3_col\\n', '\\n', '\\n', '#citation: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html']\n",
      "code ['#importing the data to the new xlsx file into our local.\\n', '\\n', 'df3_col.to_excel(\"patient_info_output.xlsx\")\\n', '\\n', '#citation: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html']\n",
      "code ['#(trial not the output)\\n', '# merging the common values using outer join \\n', '\\n', \"df_op=pd.merge(df1,df2,left_on='patient id',right_on='p_id',how='outer')\\n\", 'df_op\\n', '\\n', '#citation: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html']\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "markdown ['# Week -4\\n', 'Extract the XML from the .zip file. In Python, use a module to parse the XML (do not write an XML parser)\\u200b\\n', 'Using Python, extract the HTML from the XML. Then use BeautifulSoup4 to parse the HTML \\n', 'For each HTML page, report the number of links (URLs with the tag < a href=\"URL\">text) in each HTML page \\n', 'Submit a single Jupyter notebook that parses the XML file and produces the count of links per HTML file.\\u200b']\n",
      "code ['#Importing ElementTree, beautifulSoup\\n', '\\n', 'import xml.etree.ElementTree as ET\\n', 'from bs4 import BeautifulSoup\\n', '\\n', '#The source document is a table of contents written in XML format, and we want to get it displayed in HTML. We will use XSLT module of the lxml library in Python\\n', 'from lxml import etree\\n', '\\n', '#Citation : https://docs.python.org/2/library/xml.etree.elementtree.html\\n', '#citation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/']\n",
      "code ['#import the data. Reading the file from disk:\\n', '\\n', \"tree=ET.parse('xml_containing_html.xml')\\n\", '\\n', '#Citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n']\n",
      "code ['#Using ElementTree to parse and get the root\\n', '\\n', \"tree = ET.parse('xml_containing_html.xml')\\n\", 'root = tree.getroot()\\n', 'root.tag\\n', '\\n', '#citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n']\n",
      "code ['#displaying the tags and attributes\\n', '\\n', 'for child in root:\\n', '    print(child.tag,child.attrib)\\n', '    \\n', '#citation: https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n', '#citation: https://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/']\n",
      "code ['root[0][1].text\\n', '\\n', '#citation : https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1']\n",
      "code ['#The HTML File\\n', '\\n', 'root=x1.getroot()\\n', 'list=[]\\n', 'for child in root:\\n', '    for children in child:\\n', '        print(children.text)\\n', '\\n', '#citation: http://makble.com/convert-xml-to-html-with-lxml-xslt-in-python\\n', '#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementinclude-functions\\n', '#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementtree-objects\\n', '#citation :https://wiki.python.org/moin/ForLoop\\n']\n",
      "code ['#displaying the links and number of links for the page\\n', 'count=0\\n', 'for child in root:\\n', '    for children in child:\\n', '        data=children.text\\n', \"        soup=BeautifulSoup(data,'html.parser')\\n\", \"        for link in soup.find_all('a'):\\n\", \"            ab=(link.get('href'))\\n\", '            print(ab)\\n', '            count+=1\\n', '            b=count\\n', \"    print('----number of links for the page = ',count)\\n\", '    count=0\\n', '    \\n', '#citation :https://stackoverflow.com/questions/3075550/how-can-i-get-href-links-from-html-using-python\\n', '#citation :https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)\\n', '#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n', '#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n', '#citation :https://wiki.python.org/moin/ForLoop']\n",
      "code ['#Displaying only the links present in the HTML page\\n', 'count=0\\n', 'for child in root:\\n', '    for children in child:\\n', '        data=children.text\\n', \"        soup=BeautifulSoup(data,'html.parser')\\n\", \"        for link in soup.find_all('a'):\\n\", \"            ab=(link.get('href'))\\n\", '            print(ab)\\n', '\\n', '#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n', '#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n', '#citation :https://wiki.python.org/moin/ForLoop']\n",
      "code ['#displaying the number links for a single page\\n', '\\n', 'count=0\\n', 'for child in root:\\n', '    for children in child:\\n', '        data=children.text\\n', \"        soup=BeautifulSoup(data,'html.parser')\\n\", \"        for link in soup.find_all('a'):\\n\", '            count+=1\\n', '            b=count\\n', \"    print('total links for single page=',count)\\n\", '    count=0']\n",
      "code []\n",
      "markdown ['# Week -4\\n', 'Extract the XML from the .zip file. In Python, use a module to parse the XML (do not write an XML parser)\\u200b\\n', 'Using Python, extract the HTML from the XML. Then use BeautifulSoup4 to parse the HTML \\n', 'For each HTML page, report the number of links (URLs with the tag < a href=\"URL\">text) in each HTML page \\n', 'Submit a single Jupyter notebook that parses the XML file and produces the count of links per HTML file.\\u200b']\n",
      "code ['#Importing ElementTree, beautifulSoup\\n', '\\n', 'import xml.etree.ElementTree as ET\\n', 'from bs4 import BeautifulSoup\\n', '\\n', '#The source document is a table of contents written in XML format, and we want to get it displayed in HTML. We will use XSLT module of the lxml library in Python\\n', 'from lxml import etree\\n', '\\n', '#Citation : https://docs.python.org/2/library/xml.etree.elementtree.html\\n', '#citation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/']\n",
      "code ['#import the data. Reading the file from disk:\\n', '\\n', \"tree=ET.parse('xml_containing_html.xml')\\n\", '\\n', '#Citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n']\n",
      "code ['#Using ElementTree to parse and get the root\\n', '\\n', \"tree = ET.parse('xml_containing_html.xml')\\n\", 'root = tree.getroot()\\n', 'root.tag\\n', '\\n', '#citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n']\n",
      "code ['#displaying the tags and attributes\\n', '\\n', 'for child in root:\\n', '    print(child.tag,child.attrib)\\n', '    \\n', '#citation: https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n', '#citation: https://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/']\n",
      "code ['root[0][1].text\\n', '\\n', '#citation : https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1']\n",
      "code ['#The HTML File\\n', '\\n', 'root=x1.getroot()\\n', 'list=[]\\n', 'for child in root:\\n', '    for children in child:\\n', '        print(children.text)\\n', '\\n', '#citation: http://makble.com/convert-xml-to-html-with-lxml-xslt-in-python\\n', '#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementinclude-functions\\n', '#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementtree-objects\\n', '#citation :https://wiki.python.org/moin/ForLoop\\n']\n",
      "code ['#displaying the links and number of links for the page\\n', 'count=0\\n', 'for child in root:\\n', '    for children in child:\\n', '        data=children.text\\n', \"        soup=BeautifulSoup(data,'html.parser')\\n\", \"        for link in soup.find_all('a'):\\n\", \"            ab=(link.get('href'))\\n\", '            print(ab)\\n', '            count+=1\\n', '            b=count\\n', \"    print('----number of links for the page = ',count)\\n\", '    count=0\\n', '    \\n', '#citation :https://stackoverflow.com/questions/3075550/how-can-i-get-href-links-from-html-using-python\\n', '#citation :https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)\\n', '#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n', '#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n', '#citation :https://wiki.python.org/moin/ForLoop']\n",
      "code ['#Displaying only the links present in the HTML page\\n', 'count=0\\n', 'for child in root:\\n', '    for children in child:\\n', '        data=children.text\\n', \"        soup=BeautifulSoup(data,'html.parser')\\n\", \"        for link in soup.find_all('a'):\\n\", \"            ab=(link.get('href'))\\n\", '            print(ab)\\n', '\\n', '#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n', '#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n', '#citation :https://wiki.python.org/moin/ForLoop']\n",
      "code ['#displaying the number links for a single page\\n', '\\n', 'count=0\\n', 'for child in root:\\n', '    for children in child:\\n', '        data=children.text\\n', \"        soup=BeautifulSoup(data,'html.parser')\\n\", \"        for link in soup.find_all('a'):\\n\", '            count+=1\\n', '            b=count\\n', \"    print('total links for single page=',count)\\n\", '    count=0']\n",
      "code []\n",
      "markdown ['# week 9: removing punctuation and stop words from a corpus\\n', 'Attached Files:\\n', '\\n', 'File week_10_txt_and_docx.zip (596.791 KB)\\n', 'I provide a .zip containing .txt and .docx files\\n', '\\n', 'For each file, remove punctuation and stop words\\n', '\\n', 'Produce a single .dat file containing the name of each file in quotes, a colon, then a list of words separated by commas. The list of words per file should be unique for that file. Do not include URLs or phone numbers. Words should be made lowercase. \\n', '\\n', 'Example output:\\n', '\\n', '\"File 1.txt\" : word1, word2, word3, word7\\n', '\"name of file.docx\" : word8, word2, word1, word10\\n', '\"another file.doc\" : word1, word12, word6']\n",
      "markdown ['Importing necessary libraries required ']\n",
      "code ['import os \\n', 'import nltk\\n', 'from nltk.tokenize import RegexpTokenizer\\n', 'from string import punctuation\\n', 'from nltk.tokenize import sent_tokenize,word_tokenize \\n', 'from nltk.corpus import stopwords\\n', 'import string\\n', 'import re\\n', 'import os.path\\n', 'import glob']\n",
      "code ['# path of the file location and listing the files ']\n",
      "code ['#path=\"C:/Users/Shrimanth/DATA_601/Week_9/Assignment\"\\n', '#os.listdir(path)']\n",
      "markdown ['# printing the files with .txt and .docx extension into a list \\n', 'filelist=[]\\n', 'for file in os.listdir(path):\\n', '    if file.endswith(\".docx\") or file.endswith(\".txt\"):\\n', '        filelist.append(file)\\n', 'print(filelist)']\n",
      "code ['#filetxtList=filelist[:-2]\\n', \"filetxtList=['52256-0.txt', '53031-0.txt', '58108-0.txt', 'blind_text.txt', 'dr_yawn.txt', 'how_rubber_goods_are_made.txt', 'most_boring_ever.txt', 'most_boring_part2.txt', 'pg12814.txt', 'pg14895.txt', 'pg43994.txt', 'random_text.txt', 'smiley_the_bunny.txt']\\n\", 'type(filetxtList)']\n",
      "code ['filetxtList']\n",
      "code ['len(filelist) # total number of files ']\n",
      "markdown ['Importing the set of stopwords from nltk library']\n",
      "code ['from nltk.corpus import stopwords\\n', \"stop_word = set(stopwords.words('english'))\\n\", 'stop_word']\n",
      "markdown ['My approach to the text in each file by\\n', '1. convert to lowercase \\n', '2. remove numbers\\n', '3. remove punctuation \\n', '4. remove metacharcters\\n', '\\n', 'As text files and documents have different encoding we will first loop through all the text files and then the document files ']\n",
      "code ['# function to get the words other than stopwords\\n', 'def filewords(text):\\n', '    from nltk.corpus import stopwords\\n', \"    stop_words = set(stopwords.words('english'))\\n\", '    text=text.lower()\\n', '    #table=str.maketrans(\"!?.@*$“’^\",9*\" \")\\n', '    #text.translate(table)\\n', '    word_tokens = word_tokenize(text)\\n', '    removing_stopwords = [word for word in word_tokens if word not in stop_words]\\n', \"    text=''.join(c for c in text if c not in c.isdigit())\\n\", \"    text=''.join(w for w in text if w not in punctuation)\\n\", \"    print('words',removing_stopwords)\\n\", '    ']\n",
      "markdown ['First we will try with one file as file1']\n",
      "code ['with open(\\'52256-0.txt\\',\\'r\\',encoding=\"utf8\") as file1:\\n', '    text1=(file1.read())\\n', '    #filewords(text1)']\n",
      "code ['# to convert to lowercase\\n', 'text1=text1.lower()']\n",
      "code ['# to remove numbers\\n', \"text1 = ''.join(c for c in text1 if not c.isdigit())\\n\", 'print(text1)']\n",
      "code ['# Reference- https://machinelearningmastery.com/clean-text-machine-learning-python/\\n', '# to remove punctuation\\n', 'from string import punctuation\\n', \"text1=''.join(c for c in text1 if c not in punctuation)\\n\", 'print(text1)\\n']\n",
      "code ['# to remove metacharcters by replacing \\n', \"text1=text1.replace('”','')\\n\", '#print(output)\\n', \"text1=text1.replace('“','')\\n\", \"text1=text1.replace('’','')\\n\", \"text1=text1.replace('_','')\\n\", 'print(text1)']\n",
      "code ['# to convert to word tokens \\n', 'from nltk.tokenize import sent_tokenize,word_tokenize \\n', 'word_tokens = word_tokenize(text1) \\n', 'print(word_tokens)']\n",
      "code [\"stopword = stopwords.words('english')\\n\", 'removing_stopwords = [word for word in word_tokens if word not in stopword]\\n', 'print (removing_stopwords)']\n",
      "markdown ['Now looping it through all the fies ']\n",
      "code ['for file in filetxtList: \\n', \"    file= open (file , 'rb')\\n\", '    textcont= file.read().decode(\"utf-8\")\\n', '    textcont= textcont.lower()\\n', '    link_rem = textcont.replace(\"www.\",\"\")\\n', '    text = link_rem.replace(\"_\", \" \")\\n', \"    tokenizer = RegexpTokenizer(r'\\\\w+')\\n\", '    word_tokens= tokenizer.tokenize(text)\\n', '    words = list(filter(lambda w: w not in string.punctuation, word_tokens))\\n', '    stop_words = set(stopwords.words(\"english\"))\\n', '    words_aftstp=list(filter(lambda w:w not in stop_words, words))\\n', \"    output_text = ''.join([a for a in words_aftstp if not a.isdigit()])\\n\", \"    final_text = ''.join(output_text)\\n\", \"    print(':', final_text, '\\\\n','\\\\n')\\n\", '   \\n', '    #print(final_text)\\n', \"    #print(file,final_text, '\\\\n')\\n\"]\n",
      "code ['filetxtList\\n']\n",
      "code ['for file in filetxtList: \\n', \"    f= open (file , 'rb')\\n\", \"    lines= f.read().decode('UTF-8')\\n\", '    s_to_lower= lines.lower()\\n', '    links = s_to_lower.replace(\"www.\",\"\")\\n', \"    tokenizer = RegexpTokenizer(r'\\\\w+')\\n\", '    tok= tokenizer.tokenize(links)\\n', '    w = list(filter(lambda n: n not in string.punctuation, tok))\\n', '    s_words = set(stopwords.words(\"english\"))\\n', '    after=list(filter(lambda n:n not in s_words, w))\\n', \"    res = ' '.join([i for i in after if not i.isdigit()])\\n\", \"    f_text = ''.join(res)\\n\", '    text = f_text.replace(\"_\", \" \")\\n', '    print(\\'\"\\'+ file + \\'\"\\', \\':\\', text, \\'\\\\n\\',\\'\\\\n\\')\\n', \"    saveFile = open('outputFile2.dat','w')\\n\", '    saveFile.write(finals_text)']\n",
      "code []\n",
      "code []\n",
      "markdown ['Download data in CSV format from one of the following sites: here or here. Load the data into a Pandas dataframe. Use Pandas to count the number of rows of data and number of columns. Display the first 8 rows of the dataframe. Display the bottom 3 rows of the dataframe. Display the transpose of the first 4 rows.\\n', 'Your selection of a CSV should have at least 2 columns and at least 10 rows. Include a link to the data source in the Juptyer notebook you submit.']\n",
      "code ['import pandas as pd\\n', 'import os']\n",
      "code ['#Loading the data into pandas data frame\\n', '\\n', '#https://people.sc.fsu.edu/~jburkardt/data/csv/oscar_age_male.csv ---> csv data used\\n', '\\n', 'path_to_file = \"C:/Users/pushy/OneDrive/Desktop/UMBC Sem-1/601/assignment.csv\"\\n', \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\", 'print(type(data))\\n', '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/']\n",
      "code ['# count the number of rows of data \\n', '\\n', 'len(df.index)\\n', '\\n', '#citation: https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe']\n",
      "code ['#  count the number of columns. \\n', '\\n', 'len(df.columns)\\n', '\\n', '#citation: https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe']\n",
      "code ['df.shape']\n",
      "code ['#dimensions of the data frame \\n', '\\n', 'df.shape\\n', '\\n', '#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html']\n",
      "code ['#transpose of the data frame\\n', '\\n', 'result = df[:4].T\\n', 'print(result)\\n', '\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-transpose/#targetText=Pandas%20DataFrame.transpose()%20function,as%20columns%20and%20vice%2Dversa.&targetText=Parameter%20%3A,the%20underlying%20data%20is%20copied.']\n",
      "code ['#Display the bottom 8 rows of the dataframe\\n', '\\n', 'df.head(8)\\n', '\\n', '#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html']\n",
      "code ['#Display the bottom 3 rows of the dataframe\\n', '\\n', 'df.tail(3)\\n', '\\n', '#citation :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html']\n",
      "code ['#displaying the complete data\\n', '\\n', 'data']\n",
      "code ['data\\n']\n",
      "code ['path_to_file = \"C:/Users/pushy/OneDrive/Desktop/UMBC Sem-1/601/assignment.csv\"']\n",
      "code [\"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\", 'print(type(data))']\n",
      "code []\n",
      "markdown ['https://swcarpentry.github.io/python-novice-inflammation/07-errors/']\n",
      "code ['print(\"hello\")\\n', 'print(a)']\n",
      "markdown ['The traceback indicates the line on which the error is observed\\n', '\\n', 'List of potential error conditions in Python: https://docs.python.org/3/library/exceptions.html']\n",
      "markdown ['# Syntax as source of error']\n",
      "code ['def a_func(my_lst)\\n', '    return my_lst[5]']\n",
      "markdown ['solution: provide valid Python\\n', '\\n', 'Often Python will complain about the line following the invalid syntax']\n",
      "code ['def a_func(my_lst)\\n', \"     print('hello')\\n\", '    return my_lst[5]']\n",
      "markdown ['# dealing with errors\\n', '\\n', 'Create a list with three elements']\n",
      "code [\"lst = ['a', 'b', 'c']\"]\n",
      "code ['len(lst)']\n",
      "markdown ['Try to access the element at index 5']\n",
      "code ['lst[5]']\n",
      "markdown ['If there are expected types of errors, we can catch these']\n",
      "code ['try:\\n', '    lst[5]\\n', 'except IndexError:\\n', \"    print('index out of bounds for lst')\"]\n",
      "markdown ['The error still occurs, but now the Python program does not halt when the error exists']\n",
      "markdown ['# Nested calls\\n', '\\n', 'A common issue is that an error occurs within functions']\n",
      "code ['def a_func(my_lst):\\n', \"#   print('reached a_func')\\n\", '    return my_lst[5]\\n', '\\n', 'def speaker(phone_number,lst):\\n', '#    print(\"reached speaker\")\\n', '    val = a_func(lst)\\n', '    return val']\n",
      "markdown ['Both functions are valid Python, so no error is indicated.']\n",
      "code [\"the_value = speaker('424-5241',lst)\"]\n",
      "markdown ['There are three arrows. The error is the bottom arrow\\n', '\\n', 'We can verify the function executions by using print statements']\n",
      "code ['def a_func(my_lst):\\n', \"    print('reached a_func')\\n\", '    return my_lst[5]\\n', '\\n', 'def speaker(phone_number,lst):\\n', '    print(\"reached speaker\")\\n', '    val = a_func(lst)\\n', '    return val']\n",
      "code [\"the_value = speaker('424-5241',lst)\"]\n",
      "markdown ['Add error handling']\n",
      "code ['def a_func(my_lst):\\n', \"    print('reached a_func')\\n\", '    try:\\n', '        return my_lst[5]\\n', '    except IndexError:\\n', \"        print('index out of range, returning None')\\n\", '        return None\\n', '\\n', 'def speaker(phone_number,lst):\\n', '    print(\"reached speaker\")\\n', '    val = a_func(lst)\\n', '    return val']\n",
      "code [\"the_value = speaker('424-5241',lst)\"]\n",
      "markdown ['# checking assumptions\\n', '\\n', '* check the variable type\\n', '* check the size of the list\\n', '* check whether keys are present in dictionary\\n', '\\n', 'Every variable has a type']\n",
      "code [\"type('hello')\"]\n",
      "markdown ['wrong way to verify type match\\n', '\\n', 'https://docs.quantifiedcode.com/python-anti-patterns/readability/do_not_compare_types_use_isinstance.html\\n', 'https://stackoverflow.com/questions/707674/how-to-compare-type-of-an-object-in-python']\n",
      "code [\"type('hello')=='str'\"]\n",
      "markdown ['correct way to compare variable type']\n",
      "code [\"isinstance('hello', str)\"]\n",
      "code [\"isinstance(['a','b'],list)\"]\n",
      "markdown ['We can use `isinstance()` to validate the argument types']\n",
      "code ['def a_func(my_lst):\\n', \"    print('reached a_func')\\n\", '    if not isinstance(my_lst,list):\\n', \"        raise TypeError('a_func was provided the wrong type',type(my_lst))\\n\", '    try:\\n', '        return my_lst[5]\\n', '    except IndexError:\\n', \"        print('index out of range, returning None')\\n\", '        return None\\n', '\\n', 'def speaker(phone_number,lst):\\n', '    print(\"reached speaker\")\\n', '    val = a_func(lst)\\n', '    return val']\n",
      "code [\"the_value = speaker('424-5241','bob')\"]\n",
      "markdown ['Use of `isinstance()` is considered a Python <a href=\"https://en.wikipedia.org/wiki/Anti-pattern\">anti-pattern</a> (see http://canonical.org/~kragen/isinstance/) because Python is a <a href=\"https://en.wikipedia.org/wiki/Duck_typing\">duck type language</a>\\n', '\\n', 'For example, even though a_func is expecting a list, the function still works when I pass a string because strings are indexed like lists']\n",
      "code ['def a_func(my_lst):\\n', \"    print('reached a_func')\\n\", '    try:\\n', '        return my_lst[5]\\n', '    except IndexError:\\n', \"        print('index out of range, returning None')\\n\", '        return None\\n', '\\n', 'def speaker(phone_number,lst):\\n', '    print(\"reached speaker\")\\n', '    val = a_func(lst)\\n', '    return val']\n",
      "code [\"speaker('424-5241','asdfjbas')\"]\n",
      "code []\n",
      "code ['!pip install xlsxwriter']\n",
      "markdown ['# Problem: Pandas overwrites formulas and formatting\\n', '\\n', \"First, check out the contents of 'week_05_in-class_activity.xlsx' -- there's a formula in use in the fourth column\"]\n",
      "code ['import xlsxwriter\\n', 'import pandas\\n', \"print('Pandas',pandas.__version__)\"]\n",
      "code [\"df=pandas.read_excel('week_05_in-class_activity.xlsx')\"]\n",
      "code ['df.head()']\n",
      "code [\"df['fantastic']=df['cool']+2*df['rad']\"]\n",
      "code [\"df.to_excel('tmp_overwrite.xlsx', engine='xlsxwriter')\"]\n",
      "markdown ['# References\\n', '* Page 268 of Automate the Boring Stuff with Python\\n', '* https://medium.com/aubergine-solutions/working-with-excel-sheets-in-python-using-openpyxl-4f9fd32de87f\\n', '* https://yagisanatode.com/2017/11/18/copy-and-paste-ranges-in-excel-with-openpyxl-and-python-3/\\n', '* http://zetcode.com/python/openpyxl/\\n', '* https://openpyxl.readthedocs.io/en/stable/tutorial.html\\n', '* https://openpyxl.readthedocs.io/en/stable/usage.html\\n', '* https://stackoverflow.com/questions/40385689/add-a-new-sheet-to-a-existing-workbook-in-python\\n', '* https://www.pythonexcel.com/openpyxl-load-workbook-function.php']\n",
      "markdown ['# install library']\n",
      "raw ['!conda install -c anaconda openpyxl']\n",
      "code ['!pip install openpyxl']\n",
      "markdown ['# load library']\n",
      "code ['import openpyxl']\n",
      "markdown ['# load data']\n",
      "code [\"wb = openpyxl.load_workbook('week_05_in-class_activity.xlsx')\\n\", '\\n', '# an Excel file is composed of one or more sheets\\n', 'wb.sheetnames']\n",
      "code ['# select the one available sheet; assign to a variable\\n', \"sheet = wb['Sheet1']\\n\", '\\n', 'type(sheet)']\n",
      "markdown ['# explore the data']\n",
      "code ['# what is at the first row and first column in the worksheet?\\n', '\\n', 'sheet.cell(row=1, column=1)']\n",
      "code ['sheet.cell(row=1, column=1).value']\n",
      "code ['for row_indx in range(1,5):\\n', '    print(row_indx, sheet.cell(row=row_indx, column=1).value)']\n",
      "markdown ['# create a new worksheet']\n",
      "code ['# https://openpyxl.readthedocs.io/en/stable/tutorial.html\\n', 'new_ws = wb.create_sheet(\"example\") ']\n",
      "code ['print(wb.sheetnames)']\n",
      "code ['# https://stackoverflow.com/questions/31395058/how-to-write-to-a-new-cell-in-python-using-openpyxl\\n', \"new_ws.cell(row=2, column=2).value = 'a string'\"]\n",
      "code ['new_ws[\"A1\"] = \"=SUM(1, 1)\"']\n",
      "markdown ['the following works sometimes and not others']\n",
      "raw ['new_ws.cell(coordinate=\"C3\").value = 3']\n",
      "markdown ['# play with ranges of cells']\n",
      "code ['# https://openpyxl.readthedocs.io/en/stable/_modules/openpyxl/worksheet/cell_range.html\\n', \"new_ws['A1':'A5']\"]\n",
      "code [\"type(new_ws['A1':'A5'])\"]\n",
      "markdown ['A tuple is a collection which is ordered and immutable -- https://www.w3schools.com/python/python_tuples.asp\\n', '\\n', 'https://www.geeksforgeeks.org/tuples-in-python/']\n",
      "code [\"new_ws['A1':'A5'][0]\"]\n",
      "code [\"type(new_ws['A1':'A5'][0])\"]\n",
      "code [\"new_ws['A1':'A5'][0][0]\"]\n",
      "code [\"type(new_ws['A1':'A5'][0][0])\"]\n",
      "code [\"new_ws['A1':'A5'][0][0].value\"]\n",
      "code [\"for each_cell in new_ws['A1':'A5']:\\n\", \"    each_cell[0].value='bob'\"]\n",
      "markdown ['# save our work to a new file']\n",
      "code [\"wb.save('new_file.xlsx')\"]\n",
      "code []\n",
      "code ['import random']\n",
      "markdown ['see https://jakevdp.github.io/PythonDataScienceHandbook/01.01-help-and-documentation.html']\n",
      "markdown ['# dir\\n', 'Without arguments, return the list of names in the current local scope. With an argument, attempt to return a list of valid attributes for that object.\\n', '\\n', 'https://docs.python.org/3/library/functions.html#dir']\n",
      "code ['dir(random)']\n",
      "markdown ['# help\\n', '\\n', 'If no argument is given, the interactive help system starts on the interpreter console. If the argument is a string, then the string is looked up as the name of a module, function, class, method, keyword, or documentation topic, and a help page is printed on the console. If the argument is any other kind of object, a help page on the object is generated.\\n', '\\n', 'https://docs.python.org/3/library/functions.html#help']\n",
      "code ['help(random)']\n",
      "code ['random?']\n",
      "markdown ['# Where these come from\\n', '\\n', 'https://en.wikipedia.org/wiki/Docstring#Python']\n",
      "code ['\"\"\" main explanation \"\"\"\\n', '\\n', 'def my_func(a_str):\\n', '    \"\"\"\\n', '    This is the help\\n', '    \"\"\"\\n', \"    return 'hello'\"]\n",
      "code ['help(my_func)']\n",
      "code ['my_func?']\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code ['import pandas']\n",
      "code ['pandas.DataFrame()']\n",
      "markdown ['# week 8 - assignment - date conversion function']\n",
      "markdown ['Complete the function that converts date to day of week\\n', '\\n', 'The function should pass the doctest\\n', '\\n', 'A template .ipynb file is provided']\n",
      "code ['# https://kolesnikov.ga/Testing_and_Debugging_Jupyter_Notebooks/\\n', 'import doctest']\n",
      "code ['def convert_datetime_to_dayofweek(datetime_string):\\n', '    \"\"\"\\n', '    This function takes date in the format MON DAY YEAR  HH:MM(PM/AM)\\n', '    and returns the day of the week\\n', '    Assume input string is UTC\\n', '    \\n', \"    >>> convert_datetime_to_dayofweek('Jun 1 2005  1:33PM')\\n\", \"    'Wednesday'\\n\", \"    >>> convert_datetime_to_dayofweek('Oct 25 2012  2:17AM')\\n\", \"    'Thursday'\\n\", '    \"\"\"\\n', '    date_to_day=pd.Timestamp(datetime_string)\\n', '    day=date_to_day.weekday_name\\n', '    return day\\n', '\\n', '#citation:https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.dt.weekday_name.html\\n', '#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.dayofweek.html\\n', '#citation:https://docs.python.org/3/library/datetime.html']\n",
      "code [\"convert_datetime_to_dayofweek('Jun 1 2005  1:33PM')\"]\n",
      "code [\"convert_datetime_to_dayofweek('Oct 25 2012  2:17AM')\"]\n",
      "code []\n",
      "code []\n",
      "markdown ['Write a Python notebook that generates a file containing the following data:\\u200b \\u200b\\n', '\\n', 'Email addresses\\u200b. Must have an \"@\"\\n', 'Phone numbers\\u200b\\n', 'Home Address\\u200b\\n', \"Person's name\\u200b\\n\", 'Year born. Use realistic values.\\u200b\\n', 'Number of kids. Use realistic values.\\u200b\\n', 'Categorical variable: rent or own?\\u200b\\n', 'Annual income. Optional challenge: Use a non-uniform distribution\\u200b\\n', 'Number of speeding tickets in past year. Optional challenge: Use a non-uniform distribution\\u200b\\n', 'The user of your notebook should be able to specify how many entities are to be generated.\\u200b \\u200b\\n', '\\n', '\\u200bDo not include the .csv output file in your submission -- the file should be generated dynamically.\\n', '\\n', 'Order of columns in CSV is not relevant. \\u200b']\n",
      "markdown ['# Fake Data']\n",
      "code ['#Started by importing faker libraries and pandas\\n', 'from faker import Faker\\n', '\\n', 'import pandas as pd\\n', '\\n', '#Citation: https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python']\n",
      "code ['import random\\n', 'import csv\\n', '\\n', '#Citation : https://www.programiz.com/python-programming/working-csv-files']\n",
      "code ['pip install csvfaker\\n', '\\n', '#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python']\n",
      "code ['pip install Faker\\n', '\\n', '#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python']\n",
      "code ['#initialise Faker generator \\n', '\\n', 'fk1 = Faker()\\n', '\\n', '\\n', '#create an example of generating a fake data for a random name:\\n', 'fk1.name()\\n', '\\n', '#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python']\n",
      "code ['#Attempts to generate more than one fake data record at a time\\n', 'for n in range(10):\\n', '    print(fk1.name())\\n', '    \\n', '#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python']\n",
      "code ['#wanted to generate a custom list of fake data types and create a pandas data frame with these fake data points:\\n', '\\n', 'df = []\\n', \"List = ['own', 'rent']\\n\", '\\n', '#taking imput from the user as of how many entities of fake data has to be generated.\\n', '\\n', 'number=int(input(\"number of entities =\"))\\n', '\\n', 'for n in range(number):\\n', \"    df.append({'Name': fk1.name()\\n\", \"        , 'email': fk1.email()\\n\", \"        , 'Address' : fk1.address()\\n\", \"        ,'City' : fk1.city()\\n\", \"        , 'State': fk1.state()\\n\", \"        , 'Year of birth' : fk1.year()\\n\", \"        , 'no.of children' : random.randint(1, 2)\\n\", \"        , 'Annual Income' : random.randrange(10000,90000, 2)\\n\", \"        ,'Number of speeding tickets in past year' : random.randrange(1,10, 2)\\n\", \"        ,'Phone Number' : random.randrange(4000000000,4999999999)\\n\", \"        , 'House' : random.choice(List)\\n\", '               \\n', '              })\\n', '    \\n', '#Data arrangement\\n', 'df = pd.DataFrame(df)\\n', \"df = df[['Name', 'email', 'Address', 'City', 'State','Year of birth','no.of children','Annual Income','House','Number of speeding tickets in past year','Phone Number']]\\n\", 'df\\n', '\\n', '#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python\\n', '#Citation : https://stackoverflow.com/questions/45574191/using-python-faker-generate-different-data-for-5000-rows\\n', '#citation : https://faker.readthedocs.io/en/master/\\n', '#Citation : https://faker.readthedocs.io/en/master/providers/faker.providers.phone_number.html\\n', '#citation : https://www.tutorialspoint.com/python/number_randrange.htm']\n",
      "code ['df.to_csv(\"Assign_Week3.csv\",sep=\\' \\')\\n', '\\n', '#Citation :https://medium.com/@kasiarachuta/importing-and-exporting-csv-files-in-python-7fa6e4d9f408']\n",
      "code []\n",
      "code ['\\n']\n",
      "code []\n",
      "code ['print(\"hello\")']\n",
      "code []\n",
      "markdown ['These pages explain how to get your Chrome history on your computer:\\n', '    \\n', 'https://superuser.com/questions/602252/can-chrome-browser-history-be-exported-to-an-html-file\\n', '    \\n', 'https://yuji.wordpress.com/2014/03/10/export-chrome-history-as-csv-spreadsheet/\\n', '\\n', 'Notebook of Chrome history analysis:\\n', '\\n', 'http://www.isabeldalessandro.com/cs234/google/notebook1.html\\n', '\\n', 'https://gist.github.com/dropmeaword/9372cbeb29e8390521c2']\n",
      "code ['# see \"Profile Path\" on the page chrome://version/\\n', '\\n', '#!cp /Users/ben/Library/Application\\\\ Support/Google/Chrome/Default/History .']\n",
      "markdown ['A good tutorial on how to load SQL into Pandas\\n', '\\n', 'https://www.dataquest.io/blog/python-pandas-databases/\\n', '\\n', 'The official documentation is\\n', '\\n', 'https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_sql.html\\n', '\\n', 'https://pandas.pydata.org/pandas-docs/stable/io.html#sqlite-fallback']\n",
      "code ['import pandas\\n', 'import sqlite3\\n', \"print('pandas',pandas.__version__)\"]\n",
      "code ['conn = sqlite3.connect(\"History\") # connect to the file\\n', 'type(conn)']\n",
      "markdown ['We need to do some math to get the date-time formats to work out.\\n', '\\n', 'The reason is that the timestamp in Google Chrome, named \"Webkit format\", defines time as the number of microseconds since January, 1601\\n', '\\n', 'We need to convert from the Webkit format\\n', '\\n', 'We can use <a href=\"http://strftime.org/\">strftime</a> in <a href=\"https://en.wikipedia.org/wiki/SQL\">SQL</a> to get that date:']\n",
      "code ['query_str = \"SELECT strftime(\\'%s\\', \\'1601-01-01 00:00\\');\" \\n', '\\n', 'curr_time = pandas.read_sql_query(query_str,conn)']\n",
      "markdown ['what do we get back?']\n",
      "code ['type(curr_time)']\n",
      "markdown ['not surprisingly, pandas returns a dataframe.\\n', '\\n', 'What is the content of the dataframe?']\n",
      "code ['curr_time']\n",
      "markdown ['What we care about is the returned value']\n",
      "code ['curr_time.iloc[0][0]']\n",
      "markdown ['now we do a bunch of fancy math to convert times to local']\n",
      "code ['query_str = \"SELECT \\n', \"               datetime(last_visit_time/1000000+strftime('%s', '1601-01-01 00:00:00')-(60*60*4),'unixepoch','localtime'), \\n\", '               url FROM urls ORDER BY last_visit_time DESC;\"']\n",
      "code ['conn = sqlite3.connect(\"History\")\\n', 'hist_df = pandas.read_sql_query(query_str, conn)']\n",
      "code [\"hist_df.columns=['date_time','url']\\n\", 'hist_df.head()']\n",
      "code ['hist_df.shape']\n",
      "markdown ['Just for a check, take a look at the bottom of the dataframe (the oldest entries)']\n",
      "code ['hist_df.tail()']\n",
      "markdown [\"clearly something went wrong, but I don't know what\\n\", '\\n', 'We will revisit later in this notebook how many invalid timestamps there are\\n', '\\n', 'Prior to plotting, we need to have datetime rather than strings']\n",
      "code ['hist_df.dtypes']\n",
      "markdown ['We need to convert the strings to datetime entries\\n', '\\n', 'For timestamp format, see \\n', 'http://strftime.org/']\n",
      "code [\"hist_df['date_time'] = pandas.to_datetime(hist_df['date_time'],\\n\", \"                                          format='%Y-%m-%d %H:%M:%S',\\n\", \"                                          errors='coerce')\"]\n",
      "markdown ['what did we get?']\n",
      "code ['hist_df.head()']\n",
      "code ['hist_df.tail()']\n",
      "code [\"hist_df['date_time'].max()\"]\n",
      "code [\"hist_df['date_time'].min()\"]\n",
      "markdown ['how many entries are missing a timestamp?']\n",
      "code [\"pandas.isnull(hist_df['date_time']).sum()\"]\n",
      "markdown [\"Let's keep the NaT and see if we can plot the data\"]\n",
      "code ['import matplotlib.pyplot as plt']\n",
      "markdown ['Source:\\n', 'https://stackoverflow.com/questions/27365467/can-pandas-plot-a-histogram-of-dates']\n",
      "code ['hist_df[\"date_time\"].groupby(hist_df[\"date_time\"].dt.month).count().plot(kind=\"bar\")\\n', \"plt.xlabel('month', fontsize=14)\\n\", \"plt.ylabel('URL count', fontsize=14)\\n\", 'plt.setp(plt.gca().get_xticklabels(), rotation=20, fontsize=12) # https://stackoverflow.com/questions/6390393/matplotlib-make-tick-labels-font-size-smaller\\n', 'plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n', 'plt.show()']\n",
      "code ['# sanity check:\\n', '# URLs per day?\\n', '8500/30']\n",
      "markdown ['In case 1 or more of the months shows up in multiple years, we need to apply multiple groupby operations']\n",
      "code ['hist_plot = hist_df[\"date_time\"].groupby([hist_df[\"date_time\"].dt.year, \\n', '                                          hist_df[\"date_time\"].dt.month]).count().plot(kind=\"bar\")\\n', \"plt.xlabel('year,month', fontsize=14)\\n\", \"plt.ylabel('URL count', fontsize=14)\\n\", 'plt.setp(plt.gca().get_xticklabels(), rotation=45, fontsize=12)\\n', 'plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n', 'plt.show()']\n",
      "markdown ['What was previously attributed to \"Oct\" we can see is actually \"Oct 2019\" and \"Oct 2018\"']\n",
      "code ['fig = plt.figure(figsize=(10,6))\\n', '#plt.figure(figsize=(20,20))\\n', '\\n', 'hist_df[\"date_time\"].groupby(hist_df[\"date_time\"].dt.day).count().plot(kind=\"bar\")\\n', '\\n', \"plt.xlabel('day of month', fontsize=14)\\n\", \"plt.ylabel('URL count', fontsize=14)\\n\", 'plt.setp(plt.gca().get_xticklabels(), rotation=45, fontsize=12)\\n', 'plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n', 'plt.show()']\n",
      "code ['fig = plt.figure(figsize=(10,6))\\n', '\\n', 'hist_plot = hist_df[\"date_time\"].groupby([hist_df[\"date_time\"].dt.month, hist_df[\"date_time\"].dt.day]).count().plot(kind=\"bar\")\\n', '# https://stackoverflow.com/questions/20337664/cleanest-way-to-hide-every-nth-tick-label-in-matplotlib-colorbar\\n', 'for label in hist_plot.xaxis.get_ticklabels()[::2]:\\n', '    label.set_visible(False)\\n', \"plt.xlabel('day', fontsize=14)\\n\", \"plt.ylabel('URL count', fontsize=14)\\n\", '\\n', 'plt.setp(plt.gca().get_xticklabels(), rotation=80, fontsize=12)\\n', 'plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n', '\\n', 'plt.show()']\n",
      "code ['fig = plt.figure(figsize=(10,6))\\n', '\\n', 'hist_df[\"date_time\"].groupby(hist_df[\"date_time\"].dt.hour).count().plot(kind=\"bar\")\\n', \"plt.xlabel('hour', fontsize=14)\\n\", \"plt.ylabel('URL count', fontsize=14)\\n\", '\\n', 'plt.setp(plt.gca().get_xticklabels(), rotation=40, fontsize=12)\\n', 'plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n', '\\n', '\\n', 'plt.show()']\n",
      "code []\n",
      "code []\n",
      "markdown ['# week 8 - assignment - imputation']\n",
      "markdown ['Load each XLS or XLSX file in your Jupyter notebook. \\u200b\\n', '\\n', 'For each dataframe, address missing values by taking the following steps:\\u200b\\n', '\\n', 'Count the number of missing values in the dataframe; count the number of missing values per column\\u200b\\n', 'Plot the distribution of data points using a histogram\\u200b\\n', 'Create a lag plot (a lag plot shows t versus t+1)\\u200b\\n', 'Based on the lag plot, state in a markdown cell whether the order of this data matters. \\u200b\\n', 'Do one of the following (not both):\\u200b\\n', 'If the order of the data matters, then interpolate the missing values\\u200b\\n', 'If the order of the data does not matter, fill in the missing data by sampling from the distribution\\u200b\\n', 'Create a scatter plot using the columns in dataframe; no Nan entries should be present\\u200b']\n",
      "code ['#Importing Pandas, numpy, random, matplot, excelwiter from pandas, excelfile from pandass, lag_plot from pandas.plotting\\n', '\\n', 'import pandas as pd\\n', 'import numpy as np\\n', 'import random\\n', 'import matplotlib.pyplot as plt\\n', 'from pandas import ExcelWriter\\n', 'from pandas import ExcelFile\\n', 'from pandas.plotting import lag_plot\\n', '\\n', '#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n', '#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html']\n",
      "code ['#reading the excel using pandas\\n', \"#the excel taken 'imputation_homework_29.xlsx'\\n\", \"df1 = pd.read_excel('imputation_homework_29.xlsx', sheetname='Sheet1')\\n\", 'df1\\n', '\\n', '#citation: https://pythonspot.com/read-excel-with-pandas/']\n",
      "code ['#reading the excel using pandas\\n', \"#the excel taken 'imputation_homework_04.xlsx'\\n\", '\\n', \"df2 = pd.read_excel('imputation_homework_04.xls', sheetname='Sheet1')\\n\", 'df2\\n', '\\n', '#citation: https://pythonspot.com/read-excel-with-pandas/']\n",
      "code ['#reading the excel using pandas\\n', \"#the excel taken 'imputation_homework_91.xlsx'\\n\", '\\n', \"df3 = pd.read_excel('imputation_homework_91.xlsx', sheetname='Sheet1')\\n\", 'df3\\n', '\\n', '#citation: https://pythonspot.com/read-excel-with-pandas/']\n",
      "code ['#checking for null values\\n', '\\n', 'df1.isnull()\\n', '\\n', '#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe']\n",
      "code ['#checking for number of null values present in the data set\\n', '\\n', 'df1.isnull().sum()\\n', '\\n', '#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe']\n",
      "code ['#checking for any null values present in the data set\\n', '\\n', 'df2.isnull()\\n', '\\n', '#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe']\n",
      "code ['#checking for number of null values present in the data set\\n', '\\n', 'df2.isnull().sum()\\n', '\\n', '#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe']\n",
      "code ['#checking for any null values present in the data set\\n', '\\n', 'df3.isnull()\\n', '\\n', '#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe']\n",
      "code ['#checking for number of null values present in the data set\\n', '\\n', 'df3.isnull().sum()\\n', '\\n', '#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe']\n",
      "code [\"#Histogram for the first dataset using the column 'x'\\n\", '\\n', \"plt.hist(df1['x'])\\n\", 'plt.show()']\n",
      "code [\"#Histogram for the second dataset using the column 'value'\\n\", '\\n', \"plt.hist(df2['value'])\\n\", 'plt.show()']\n",
      "code [\"#Histogram for the third dataset using the column 'value'\\n\", \"plt.hist(df3['value'])\\n\", 'plt.show()']\n",
      "code ['plt.figure(figsize=(7,7))\\n', \"lag_plot(df1['x'])\\n\", '\\n', '#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-lag']\n",
      "markdown ['The order of the data matters, the points in the lag plot are linear, so by looking at the pattern we can say that the order of the data matters.']\n",
      "code ['plt.figure(figsize=(7,7))\\n', \"lag_plot(df2['value'])\\n\", '\\n', '#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-lag']\n",
      "markdown ['The order of the data matters, the points in the lag plot are linear, so by looking at the pattern we can say that the order of the data matters.']\n",
      "code ['plt.figure(figsize=(7,7))\\n', \"lag_plot(df3['value'])\\n\", '\\n', '#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-lag']\n",
      "markdown [\"The order of the data doesn't matter, the points in the lag plot are non-linear, so by looking at the pattern we can say that the order of the data doen't matter.\"]\n",
      "code [\"#inserting values for the missing values, we are interpolating the values using the method 'linear'.\\n\", '\\n', \"df1['x'] = df1['x'].interpolate(method='linear')\\n\", '\\n', '#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html']\n",
      "code [\"df1.plot.scatter(x = 'x', y ='y', figsize=(7,7), c='red')\\n\", \"plt.title('imputation_homework_29.xlsx')\\n\", \"#plotting a scatter plot for the fiirst dataframe 'df1'\"]\n",
      "markdown ['The df1 data follows an order.']\n",
      "code [\"#inserting values for the missing values, we are interpolating the values using the method 'linear'.\\n\", '\\n', \"df2['value'] = df2['value'].interpolate(method='linear')\\n\", '\\n', '#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html']\n",
      "code [\"df2.plot.scatter(x = 'time', y ='value', figsize=(7,7), c='red')\\n\", \"plt.title('imputation_homework_04.xls')\\n\", \"#plotting a scatter plot for the second dataframe 'df2'\"]\n",
      "markdown ['The df2 data follows an order.']\n",
      "code ['#inserting values in the non linear set \\n', '\\n', \"df3['value'] = df3['value'].apply(lambda x: np.random.choice(df3['value'].dropna().values) if np.isnan(x) else x)\\n\", '\\n', '#citation:https://stackoverflow.com/questions/36413314/filling-missing-data-by-random-choosing-from-non-missing-values-in-pandas-datafr']\n",
      "code ['\\n', 'plt.figure(figsize=(17,7))\\n', \"plt.plot_date(x = df3['date'], y =df3['value'], xdate=True, c='red')\\n\", \"plt.xlabel('date')\\n\", \"plt.ylabel('value')\\n\", \"plt.title('imputation_homework_91.xlsx')\\n\", '#citation:https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.plot_date.html']\n",
      "markdown ['this data in the df3 doesnt follow an order.\\n']\n",
      "code []\n",
      "markdown ['https://docs.python.org/3/library/re.html\\n', '\\n', 'https://docs.python.org/3/howto/regex.html']\n",
      "code ['import re']\n",
      "markdown ['source of text: https://www.bradezone.com/2008/09/13/boring/']\n",
      "code ['# %load text.py\\n', 'this_text=\"I go to the store. A car is parked. \\\\\\n', 'Many cars are parked or moving. Some are blue. \\\\\\n', 'Some are tan. They have windows. In the store, \\\\\\n', 'there are items for sale. These include such \\\\\\n', 'things as soap, detergent, magazines, and lettuce. \\\\\\n', 'You can enhance your life with these products. \\\\\\n', 'Soap can be used for bathing, be it in a bathtub \\\\\\n', 'or in a shower. My email address is myname@sc.edu. \\\\\\n', 'Apply the soap to your body and rinse. My phone \\\\\\n', 'number is 452-953-2942. Detergent is used to \\\\\\n', 'wash clothes. Place your dirty clothes \\\\\\n', 'into a washing machine and add some detergent \\\\\\n', 'as directed on the box. Your email is \\\\\\n', 'aperson@farm.com and your cell is 595-942-2424. \\\\\\n', 'Select the appropriate settings on your \\\\\\n', 'Alexs question 953-242 \\\\\\n', 'washing machine and you should be ready to \\\\\\n', 'begin. Magazines are stapled reading material \\\\\\n', 'made with glossy paper, and they cover a wide \\\\\\n', 'variety of topics, ranging from news and \\\\\\n', 'politics to business and stock market information.\"']\n",
      "markdown ['Look for a number that exists in the text']\n",
      "code ['re.findall(r\"953\",this_text)']\n",
      "markdown ['Look for a set of digits in a specific pattern']\n",
      "code ['re.findall(r\"\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d\",this_text)']\n",
      "markdown ['<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '\\n', 'Same goal, but more concisely\\n', '\\n', 'https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285']\n",
      "code ['re.findall(r\"\\\\d{3}-\\\\d{3}-\\\\d{4}\",this_text)']\n",
      "markdown ['For more patterns, see http://regexlib.com/Search.aspx?k=phone\\n', '\\n', '<BR>\\n', '<BR>\\n', '\\n', 'Look for an email address']\n",
      "code ['re.findall(r\"@[a-z]\",this_text)']\n",
      "markdown ['To get all the letters in the domain, use +']\n",
      "code ['re.findall(r\"@[a-z]+\",this_text)']\n",
      "markdown ['To get the letters proceeding the @, ']\n",
      "code ['re.findall(r\"[a-z]+@[a-z]+\",this_text)']\n",
      "markdown ['Including the domain requires we escape the . since the period is a special character for regex']\n",
      "code ['re.findall(r\"[a-z]+@[a-z]+\\\\.[a-z]+\",this_text)']\n",
      "markdown ['Once a Regular Expression gets complicated, use comments to convey your intention']\n",
      "code ['re.findall(r\"\"\"[a-z]+  # user name\\n', '               @\\n', '               [a-z]+  # domain\\n', '               \\\\.\\n', '               [a-z]+  # TLD, https://en.wikipedia.org/wiki/Top-level_domain\\n', '               \"\"\",this_text, re.VERBOSE)']\n",
      "markdown ['That pattern for emails is not robust. For better versions, see http://regexlib.com/Search.aspx?k=email']\n",
      "markdown ['# Use of regex for string replacement\\n', '\\n', \"In this example, we'll anonymize the phone numbers\"]\n",
      "code ['re.sub(r\"\\\\d{4}\", \"XXXX\", this_text)']\n",
      "markdown ['we can compile a Regular Expression for repeated use']\n",
      "code ['myregex = re.compile(r\"\\\\d{4}\")\\n', 'for line in list_of_lines:\\n', '    print(myregex.sub(\"XXXX\", line))']\n",
      "markdown ['# Documentation \\n', '\\n', 'As an example of why documentation matters, try to figure out what this regex does']\n",
      "code ['a = re.compile(r\"\\\\d\\\\.\\\\d*\")']\n",
      "markdown ['compared to']\n",
      "code ['a = re.compile(r\"\"\"\\\\d +  # the integer part\\n', '                   \\\\.    # the decimal point, aka radix: https://en.wikipedia.org/wiki/Radix\\n', '                   \\\\d *  # some fractional digits\"\"\", re.VERBOSE)']\n",
      "code []\n",
      "code []\n",
      "markdown ['https://jakevdp.github.io/PythonDataScienceHandbook/06.00-figure-code.html#Expectation-Maximization']\n",
      "code ['import matplotlib.pylab as plt\\n', 'import numpy as np\\n', 'from sklearn.datasets.samples_generator import make_blobs\\n', '\\n', '# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html\\n', '# Compute minimum distances between one point and a set of points.\\n', 'from sklearn.metrics import pairwise_distances_argmin']\n",
      "code ['number_of_clusters=4\\n', 'X, y_true = make_blobs(n_samples=300, centers=number_of_clusters,\\n', '                       cluster_std=0.60, random_state=0)\\n', '\\n', 'rng = np.random.RandomState(42)\\n', 'centers = [0, 4] + rng.randn(number_of_clusters, 2)']\n",
      "code ['def draw_points(ax, c, factor=1):\\n', \"    ax.scatter(X[:, 0], X[:, 1], c=c, cmap='viridis',\\n\", '               s=50 * factor, alpha=0.3)\\n', '    \\n', 'def draw_centers(ax, centers, factor=1, alpha=1.0):\\n', '    ax.scatter(centers[:, 0], centers[:, 1],\\n', \"               c=np.arange(number_of_clusters), cmap='viridis', s=200 * factor,\\n\", '               alpha=alpha)\\n', '    ax.scatter(centers[:, 0], centers[:, 1],\\n', \"               c='black', s=50 * factor, alpha=alpha)\\n\", '\\n', 'def make_ax(fig, gs):\\n', '    ax = fig.add_subplot(gs)\\n', '    ax.xaxis.set_major_formatter(plt.NullFormatter())\\n', '    ax.yaxis.set_major_formatter(plt.NullFormatter())\\n', '    return ax']\n",
      "code ['fig = plt.figure()\\n', 'ax = plt.gca()\\n', \"draw_points(ax, 'gray', factor=2)\\n\", 'draw_centers(ax, centers, factor=2)\\n', 'print(\"initial centers\")\\n', 'print(centers)\\n', \"plt.title('Random Initialization')\\n\", '\\n', 'for indx in range(3):    \\n', '    # E-step\\n', '    fig = plt.figure()\\n', '    ax = plt.gca()\\n', '    y_pred = pairwise_distances_argmin(X, centers)\\n', '    draw_points(ax, y_pred)\\n', '    draw_centers(ax, centers)\\n', \"    plt.title('Expectation step '+str(indx))\\n\", '    \\n', '    # M-step\\n', '    fig = plt.figure()\\n', '    ax = plt.gca()\\n', '    new_centers = np.array([X[y_pred == i].mean(0) for i in range(number_of_clusters)])\\n', '    # where the centers are:\\n', '    print(\"centers \"+str(indx))\\n', '    print(new_centers)\\n', '    draw_points(ax, y_pred)\\n', '    draw_centers(ax, centers, alpha=0.3)\\n', '    draw_centers(ax, new_centers)\\n', '    for loop_index in range(number_of_clusters):\\n', \"        ax.annotate('', new_centers[loop_index], centers[loop_index],\\n\", \"                     arrowprops=dict(arrowstyle='->', linewidth=1))\\n\", \"    plt.title('Maximization step '+str(indx))\\n\", '    \\n', '    # Finish iteration\\n', '    centers = new_centers\\n', '\\n', '# Final E-step    \\n', 'fig = plt.figure()\\n', 'ax = plt.gca()\\n', 'y_pred = pairwise_distances_argmin(X, centers)\\n', 'draw_points(ax, y_pred, factor=2)\\n', 'draw_centers(ax, centers, factor=2)\\n', '_=plt.title(\"Final Clustering\")']\n",
      "code []\n",
      "markdown ['https://jakevdp.github.io/PythonDataScienceHandbook/06.00-figure-code.html#Expectation-Maximization']\n",
      "code ['import matplotlib.pylab as plt\\n', 'import numpy as np\\n', 'from sklearn.datasets.samples_generator import make_blobs\\n', '\\n', '# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html\\n', '# Compute minimum distances between one point and a set of points.\\n', 'from sklearn.metrics import pairwise_distances_argmin']\n",
      "code ['def draw_points(X,ax, c, factor=1):\\n', \"    ax.scatter(X[:, 0], X[:, 1], c=c, cmap='viridis',\\n\", '               s=50 * factor, alpha=0.3)\\n', '    \\n', 'def draw_centers(number_of_clusters,ax, centers, factor=1, alpha=1.0):\\n', '    ax.scatter(centers[:, 0], centers[:, 1],\\n', \"               c=np.arange(number_of_clusters), cmap='viridis', s=200 * factor,\\n\", '               alpha=alpha)\\n', '    ax.scatter(centers[:, 0], centers[:, 1],\\n', \"               c='black', s=50 * factor, alpha=alpha)\\n\", '\\n', 'def make_ax(fig, gs):\\n', '    ax = fig.add_subplot(gs)\\n', '    ax.xaxis.set_major_formatter(plt.NullFormatter())\\n', '    ax.yaxis.set_major_formatter(plt.NullFormatter())\\n', '    return ax\\n']\n",
      "code ['def kmeans(number_of_clusters,num_iterations,plot_figs):\\n', '    # generate blobs of data points\\n', '    X, y_true = make_blobs(n_samples=300, centers=4,\\n', '                           cluster_std=0.60, random_state=0)\\n', '\\n', '    # initialize random centroid locations\\n', '    rng = np.random.RandomState(42)\\n', '    centers = [0, number_of_clusters] + rng.randn(number_of_clusters, 2)\\n', '\\n', '    if plot_figs:\\n', '        fig = plt.figure()\\n', '        ax = plt.gca()\\n', \"        draw_points(X,ax, 'gray', factor=1)\\n\", '        draw_centers(number_of_clusters,ax, centers, factor=1)\\n', '        print(\"initial centers\")\\n', '        print(centers)\\n', \"        plt.title('Random Initialization')\\n\", '\\n', '    for indx in range(num_iterations):    \\n', '        # E-step\\n', '        y_pred = pairwise_distances_argmin(X, centers) # given a set of point and a set of centroids, determine which group each point is most likely in\\n', '        if plot_figs:\\n', '            fig = plt.figure()\\n', '            ax = plt.gca()\\n', '            draw_points(X,ax, y_pred)\\n', '            draw_centers(number_of_clusters,ax, centers)\\n', \"            plt.title('Expectation step '+str(indx))\\n\", '    \\n', '        # M-step\\n', '        new_centers = np.array([X[y_pred == i].mean(0) for i in range(number_of_clusters)])\\n', '        if plot_figs:\\n', '            fig = plt.figure()\\n', '            ax = plt.gca()\\n', '            # where the centers are:\\n', '            print(\"centers \"+str(indx))\\n', '            print(new_centers)\\n', '            draw_points(X,ax, y_pred)\\n', '            draw_centers(number_of_clusters,ax, centers, alpha=0.3)\\n', '            draw_centers(number_of_clusters,ax, new_centers)\\n', '            for loop_index in range(number_of_clusters):\\n', \"                ax.annotate('', new_centers[loop_index], centers[loop_index],\\n\", \"                             arrowprops=dict(arrowstyle='->', linewidth=1))\\n\", \"            plt.title('Maximization step '+str(indx))\\n\", '    \\n', '        # Finish iteration\\n', '        centers = new_centers\\n', '\\n', '    # Final E-step    \\n', '    y_pred = pairwise_distances_argmin(X, centers)\\n', '    if plot_figs:\\n', '        fig = plt.figure()\\n', '        ax = plt.gca()\\n', '        draw_points(X, ax, y_pred, factor=1)\\n', '        draw_centers(number_of_clusters,ax, centers, factor=1)\\n', '        _=plt.title(\"Final Clustering\")\\n', '    return X,y_pred,centers']\n",
      "markdown ['within cluster sum of square ']\n",
      "code ['X,y_pred,centers = kmeans(number_of_clusters=3,num_iterations=4,plot_figs=False)']\n",
      "code []\n",
      "code ['list_of_sumofsq=[]\\n', 'num_clstr_range=range(1,7)\\n', 'for number_of_clusters in num_clstr_range:\\n', '    X,y_pred,centers = kmeans(number_of_clusters,num_iterations=4,plot_figs=False)\\n', '    sumofsq=0\\n', '    centroid_indx=-1\\n', '    for this_centroid_coords in centers:\\n', '        centroid_indx+=1\\n', '        for this_dot in X[y_pred==centroid_indx]:\\n', '            # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\\n', '            sumofsq+=np.linalg.norm(this_dot - this_centroid_coords)**2\\n', '    list_of_sumofsq.append(sumofsq)\\n', '    print(number_of_clusters,sumofsq)']\n",
      "code ['_=plt.scatter(num_clstr_range,y=list_of_sumofsq)\\n', \"_=plt.xlabel('k',fontsize=12)\\n\", \"_=plt.ylabel('sum of squared errors',fontsize=12)\"]\n",
      "markdown ['<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '\\n', 'For an sklearn-based example, see\\n', '\\n', 'https://stackoverflow.com/questions/19197715/scikit-learn-k-means-elbow-criterion']\n",
      "code []\n",
      "markdown ['#### Hypothesis\\n', '1. What industry are entrepreneurs interested in the most?  \\n', \"2. Is the trend of starting a business increased between the 90s’ and 00's? \\n\"]\n",
      "markdown ['#### Project 2: Proposal\\n', 'Data source location: NYC Open Data( https://data.cityofnewyork.us/Business/Legally-Operating-Businesses/w7w3-xahh) \\n', 'How data source was discovered: I was searching on data.world on other dataset, and I came across this source. \\n', 'Data is free to download: Yes, it is free. \\n', 'Data is legal to access: Yes, it is legal to access. \\n', 'Documented EDA is not available for the data: I searched with the keywords of “data analysis Legally Operating Business”, “Legally Operating Business data analysis”, “Legally Operating Business GitHub”. I don’t see any documented EDA is available.   \\n', 'Project proposal includes size of data (rows, columns, megabytes): There are 199,325rows, 27columns and 43.7MB.   \\n', 'Project proposal relevant to Data 601 objectives: This dataset will be used for data exploration, data characterization, data visualization, identifying data issues and potentially resolving problems in the data. Also, hypothesis/question will be set before perform data analysis. By applying visualization to the data, we can investigate the problem to answer the questions or tell a story about the dataset.   \\n', 'Does student understand the stories in the data? This dataset includes businesses/individuals holding a DCA license so that they may legally operate in New York City.   \\n', 'Data size- at least 1000 rows; at least 10 columns; at least 2 different types of entries (e.g., text, int, float, phone number, city, state, zip, etc). \\n', 'Data should be no larger than 100MB unless negotiated in advance with instructor: The dataset has 199,325rows, 27columns and 43.7MB. There are 3 data types in the dataset: Plain Text, Date & Time and numeric values.   \\n']\n",
      "markdown ['#### About Dataset\\n', 'This data set features businesses/individuals holding a DCA license so that they may legally operate in New York City, and the dataset last updated was on Oct 18,2019. This dataset will perform EDA, then we will invested 2 questions:  \\n', '1. What industry are entrepreneurs interested in the most? I expect the answer to be the food industry will be the most. \\n', \"2. Is the trend of starting a business increased between the 90s’ and 00's? I expect the answer to be Yes, and the increase might be triple.\"]\n",
      "code ['url = \"https://data.cityofnewyork.us/api/views/w7w3-xahh/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\\n', 'response = urllib.request.urlopen(url)\\n', 'df = pd.read_csv(response,low_memory=False)']\n",
      "code ['\\n', 'import pandas as pd\\n', 'import numpy as np\\n', 'import chardet\\n', '#import ascii\\n', 'import string\\n', 'import sys\\n', 'import re\\n', 'import csv\\n', 'import os\\n', 'import matplotlib.pyplot as plt\\n', 'from pandas.plotting import lag_plot\\n', 'import matplotlib.patches as mpatches\\n', 'from matplotlib import rc\\n', 'from urllib.request import urlopen\\n', 'import networkx as net']\n",
      "code ['url = \"https://data.cityofnewyork.us/api/views/w7w3-xahh/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\\n', '#response = urllib.request.urlopen(url)\\n', 'df = pd.read_csv(url,low_memory=False)']\n",
      "code [\"df['Industry'].unique().sum()\"]\n",
      "code ['g=df.groupby(\"Industry\")']\n",
      "code [\"df['Industry'].value_counts().plot(kind='barh',figsize=(20,20))\"]\n",
      "code ['df.shape']\n",
      "code []\n",
      "code []\n",
      "markdown ['https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\\n', '\\n', 'https://matplotlib.org/tutorials/introductory/pyplot.html']\n",
      "code ['import time\\n', 'import matplotlib.pyplot as plt\\n', 'import pandas\\n', \"print('Pandas',pandas.__version__)\"]\n",
      "markdown ['# load the data into a dataframe']\n",
      "code ['start_time = time.time()\\n', \"loans_2007 = pandas.read_csv('https://github.com/yhat/demo-lending-club/blob/master/model/LoanStats3a.csv?raw=true', \\n\", '                             skiprows=1, low_memory=False)\\n', \"print(time.time() - start_time,'seconds')\"]\n",
      "markdown ['how big is the 41MB table?']\n",
      "code ['loans_2007.shape']\n",
      "markdown ['What are all the columns in the dataframe? \\n', '\\n', '_Tip_: Get a list of strings (rather than displaying the head of the dataframe)']\n",
      "code ['list(loans_2007.columns)']\n",
      "markdown ['what are the data types of each column?']\n",
      "code ['print(loans_2007.dtypes)']\n",
      "markdown ['create a histogram of one of the numeric columns']\n",
      "code [\"loans_2007['installment'].hist()\\n\", \"plt.xlabel('installment')\\n\", \"plt.ylabel('count')\"]\n",
      "code [\"plt.xlabel('installment')\"]\n",
      "code []\n",
      "code ['import random']\n",
      "markdown ['# sequence of flips I found experimentally']\n",
      "code ['#sequence_of_interest = (1, 2, 1, 1)\\n', '#sequence_of_interest = (1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2) #0.5 seconds\\n', 'sequence_of_interest = (1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2)\\n', 'print(\"length of sequence: \",len(sequence_of_interest))']\n",
      "code ['def create_random_sequence(len_of_seq,number_of_coin_flips):\\n', '    \"\"\"\\n', '    create a sequence of given length containing integer values, either 1 or 2\\n', '\\n', '    this is intended to represent the outcome of a sequence of coin flips\\n', '    \"\"\"\\n', '    this_run=[]\\n', '    for indx in range(len_of_seq):\\n', '        coin = random.randint(1, 2) # random coin flip results in 1 or 2\\n', '        number_of_coin_flips+=1\\n', '        this_run.append(coin)       # add coin flip result to list\\n', '    return this_run,number_of_coin_flips']\n",
      "code ['experiment_result, running_count = create_random_sequence(5,0)\\n', \"print('experiment result:',experiment_result)\\n\", \"print('running count:',running_count)\"]\n",
      "code ['import timeit\\n', 'start_time = timeit.default_timer()\\n', '\\n', 'number_of_coin_flips=0\\n', 'this_run=[]  # store the results of the random coin flips in a list\\n', 'while True:\\n', '    this_run,number_of_coin_flips = create_random_sequence(len(sequence_of_interest),\\n', '                                                           number_of_coin_flips)\\n', '    found_match=False\\n', '    for flip_indx in range(len(sequence_of_interest)):\\n', '#            print(\"flip index is \",flip_indx)\\n', '#            print(sequence_of_interest)\\n', '        if (sequence_of_interest[flip_indx] == this_run[flip_indx]):\\n', '            found_match=True\\n', \"        else: # patterns don't match\\n\", '            found_match=False\\n', '#                print(\"patterns don\\'t match\",this_run)\\n', '            this_run=[]\\n', '            break\\n', '    if (found_match==True):\\n', '        print(this_run)\\n', '        break\\n', '\\n', 'print(\"number of coin flips:\",number_of_coin_flips)\\n', 'elapsed = timeit.default_timer() - start_time\\n', 'print(str(elapsed) + \" seconds\")']\n",
      "markdown ['<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', '<BR>\\n', \"Here's the full code to run in http://pythontutor.com/visualize.html\\n\", '\\n', '(also pasted to https://pastebin.com/bYmkxqZn)']\n",
      "raw ['import random\\n', 'sequence_of_interest = (1, 2, 1, 1)\\n', 'print(\"length of sequence: \",len(sequence_of_interest))\\n', '\\n', 'def create_random_sequence(len_of_seq,number_of_coin_flips):\\n', '    this_run=[]\\n', '    for indx in range(len_of_seq):\\n', '        coin = random.randint(1, 2) # random coin flip results in 1 or 2\\n', '        number_of_coin_flips+=1\\n', '        this_run.append(coin)       # add coin flip result to list\\n', '    return this_run,number_of_coin_flips\\n', '    \\n', 'number_of_coin_flips=0\\n', 'this_run=[]  # store the results of the random coin flips in a list\\n', 'while True:\\n', '    this_run,number_of_coin_flips = create_random_sequence(len(sequence_of_interest),number_of_coin_flips)\\n', '    found_match=False\\n', '    for flip_indx in range(len(sequence_of_interest)):\\n', '        if (sequence_of_interest[flip_indx] == this_run[flip_indx]):\\n', '            found_match=True\\n', \"        else: # patterns don't match\\n\", '            found_match=False\\n', '            this_run=[]\\n', '            break\\n', '    if (found_match==True):\\n', '        print(this_run)\\n', '        break']\n",
      "markdown ['# scaling of search']\n",
      "code ['import random\\n', 'import timeit\\n', 'import time']\n",
      "code ['def create_sequence_of_interest(length_of_seq):\\n', '    \"\"\"\\n', '    create a sequence of given length containing integer values, either 1 or 2\\n', '\\n', '    this is intended to represent the outcome of a sequence of coin flips\\n', '    \"\"\"\\n', '    list_of_flips=[]\\n', '    for this_flip in range(length_of_seq):\\n', '        list_of_flips.append(random.randint(1, 2))\\n', '    return tuple(list_of_flips)']\n",
      "code ['create_sequence_of_interest(5)']\n",
      "code ['def find_match(sequence_of_interest):\\n', '    \"\"\"\\n', '    given a sequence of integers (outcome of either 1 or 2), \\n', '    flip a coin until that sequence is found\\n', '    \"\"\"\\n', '    start_time = timeit.default_timer()\\n', '    number_of_coin_flips=0\\n', '    this_run=[]  # store the results of the random coin flips in a list\\n', '    while True:\\n', '        coin = random.randint(1, 2) # random coin flip results in 1 or 2\\n', '        number_of_coin_flips+=1\\n', '        this_run.append(coin)       # add coin flip result to list\\n', '        if (len(this_run)==len(sequence_of_interest)):\\n', '            found_match=False\\n', '            for flip_indx in range(len(sequence_of_interest)):\\n', '                if (sequence_of_interest[flip_indx] == this_run[flip_indx]):\\n', '                    found_match=True\\n', \"                else: # patterns don't match\\n\", '                    found_match=False\\n', '                    this_run=[]\\n', '                    break\\n', '            if (found_match==True):\\n', '                #print(this_run)\\n', '                break\\n', '\\n', '    #print(\"number of coin flips:\",number_of_coin_flips)\\n', '    elapsed = timeit.default_timer() - start_time\\n', '    #print(str(elapsed) + \" seconds\")\\n', '    return elapsed, number_of_coin_flips']\n",
      "code ['seq_of_interest = create_sequence_of_interest(5)\\n', 'print(seq_of_interest)\\n', 'find_match(seq_of_interest)']\n",
      "markdown ['parameters for my investigation']\n",
      "code ['number_of_experiments_per_loop=1000\\n', 'start_length=3\\n', 'end_length=10']\n",
      "markdown ['\\n', '<pre>\\n', '1000 sequences of length 3\\n', '1000 sequences of length 4\\n', '1000 sequences of length 5\\n', '...\\n', '1000 sequences of length 10\\n', '</pre>\\n', '\\n', \"To make that clearer, I'll break out what each experiment is\\n\", '<pre>\\n', '1000 sequences of length 3:\\n', '* for a sequence (1,2,1), how many coin flips to find a match?\\n', '* for a sequence (1,1,1), how many coin flips to find a match?\\n', '* for a sequence (2,2,1), how many coin flips to find a match?\\n', '...\\n', '1000 sequences of length 4\\n', '* for a sequence (2,1,2,1), how many coin flips to find a match?\\n', '* for a sequence (1,2,1,1), how many coin flips to find a match?\\n', '* for a sequence (2,2,1,1), how many coin flips to find a match?\\n', '...\\n', '1000 sequences of length 5\\n', '* for a sequence (1,1,1,2,1), how many coin flips to find a match?\\n', '* for a sequence (1,1,1,2,1), how many coin flips to find a match?\\n', '* for a sequence (2,2,1,1,1), how many coin flips to find a match?\\n', '...\\n', '1000 sequences of length 10\\n', '* for a sequence (1,1,1,2,1,2,2,1,2,2), how many coin flips to find a match?\\n', '* for a sequence (1,1,1,2,1,2,1,2,1,1), how many coin flips to find a match?\\n', '* for a sequence (2,2,1,1,1,2,1,2,1,1), how many coin flips to find a match?\\n', '</pre>']\n",
      "code ['list_of_result_dicts=[]\\n', 'start_time=time.time()\\n', 'for length_of_seq in range(3,10):\\n', '    for this_try in range(number_of_experiments_per_loop):\\n', '        sequence_of_interest = create_sequence_of_interest(length_of_seq)\\n', '#        print(sequence_of_interest)\\n', '\\n', '        elapsed, number_of_coin_flips = find_match(sequence_of_interest)\\n', '#        print(elapsed,\"seconds\")\\n', '#        print(number_of_coin_flips)\\n', '        this_result={}\\n', \"        this_result['elapsed time in seconds'] = elapsed\\n\", \"        this_result['number of flips'] = number_of_coin_flips\\n\", \"        this_result['sequence length'] = length_of_seq\\n\", '        list_of_result_dicts.append(this_result)\\n', '        \\n', \"print('elapsed:',time.time()-start_time,'seconds')\"]\n",
      "code ['import pandas\\n', 'pandas.__version__']\n",
      "code ['results_df = pandas.DataFrame(list_of_result_dicts)\\n', 'results_df.head(10)']\n",
      "code ['results_df.shape']\n",
      "code ['import matplotlib\\n', 'import matplotlib.pyplot as plt\\n', 'matplotlib.__version__']\n",
      "code [\"results_df.plot.scatter(x='sequence length',y='elapsed time in seconds')\\n\", 'plt.show()']\n",
      "code [\"results_df.plot.scatter(x='sequence length',y='number of flips')\\n\", 'plt.show()']\n",
      "code [\"results_df.groupby('sequence length').describe()\"]\n",
      "markdown ['We can add error bars to the plot for each sequence length\\n', '\\n', 'https://matplotlib.org/api/_as_gen/matplotlib.pyplot.errorbar.html\\n', '\\n', 'https://matplotlib.org/1.2.1/examples/pylab_examples/errorbar_demo.html\\n', '\\n', 'https://matplotlib.org/gallery/statistics/errorbar_features.html']\n",
      "code ['import seaborn\\n', \"print('seaborn',seaborn.__version__)\"]\n",
      "code ['# https://seaborn.pydata.org/generated/seaborn.stripplot.html\\n', '_=seaborn.stripplot(x=\"sequence length\", \\n', '                        y=\"number of flips\", \\n', '                        data=results_df)']\n",
      "markdown [' The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range. <a href=\"https://seaborn.pydata.org/generated/seaborn.boxplot.html\">source</a>\\n', ' \\n', 'The first quartile (Q1) is defined as the middle number between the smallest number and the median of the data set. The second quartile (Q2) is the median of the data. The third quartile (Q3) is the middle value between the median and the highest value of the data set. <a href=\"https://en.wikipedia.org/wiki/Quartile\">source</a>']\n",
      "code ['# https://seaborn.pydata.org/generated/seaborn.boxplot.html\\n', '_=seaborn.boxplot(x=\"sequence length\", \\n', '                        y=\"number of flips\", \\n', '                        data=results_df)']\n",
      "raw ['# https://seaborn.pydata.org/generated/seaborn.swarmplot.html#seaborn.swarmplot\\n', '_=seaborn.swarmplot(x=\"sequence length\", \\n', '                        y=\"number of flips\", \\n', '                        data=results_df)']\n",
      "markdown ['the violin plot features a kernel density estimation of the underlying distribution\\n', '\\n', 'violins for relatively small samples might look misleadingly smooth.']\n",
      "code ['# https://seaborn.pydata.org/generated/seaborn.violinplot.html\\n', '_ = seaborn.violinplot(x=\"sequence length\", \\n', '                        y=\"number of flips\", \\n', '                        data=results_df,\\n', '                        showmeans=True,\\n', '                        showextrema=True,\\n', '                        showmedians=True)']\n",
      "code []\n",
      "markdown ['# visualization of fair outcomes']\n",
      "code ['import random\\n', 'import pandas\\n', \"print('pandas',pandas.__version__)\\n\", 'import matplotlib.pyplot as plt\\n', 'import numpy\\n', \"print('numpy',numpy.__version__)\"]\n",
      "code ['count_heads=0\\n', 'num_flips=50\\n', 'for indx in range(num_flips):\\n', '    coin = random.randint(1, 2) # random coin flip results in 1 (tail) or 2 (heads)\\n', '    count_heads += coin-1']\n",
      "code ['count_heads']\n",
      "code [\"df = pandas.DataFrame([{'head count':count_heads,'tail count':num_flips-count_heads}])\\n\", 'df']\n",
      "code ['_=df.plot.bar()\\n', \"#_=plt.xticks(numpy.arange(2), ('Heads','Tails'))\\n\", '_=plt.xticks([])\\n', \"plt.savefig('coin_flips')\"]\n",
      "markdown ['# increase the number of flips']\n",
      "code ['count_heads=0\\n', 'num_flips=500\\n', 'for indx in range(num_flips):\\n', '    coin = random.randint(1, 2) # random coin flip results in 1 or 2\\n', '    count_heads += coin-1']\n",
      "code ['count_heads']\n",
      "code ['import pandas\\n', \"df = pandas.DataFrame([{'head count':count_heads,'tail count':num_flips-count_heads}])\\n\", 'df']\n",
      "code ['import matplotlib.pyplot as plt\\n', 'import numpy\\n', '_=df.plot.bar()\\n', \"#_=plt.xticks(numpy.arange(2), ('Heads','Tails'))\\n\", '_=plt.xticks([])']\n",
      "markdown ['--> more flips for a fair coin yield closer distribution to \"fair\"']\n",
      "code []\n",
      "code ['import pandas as pd\\n', 'import numpy as np\\n', 'import chardet\\n', '#import ascii\\n', 'import string\\n', 'import sys\\n', 'import re\\n', 'import csv\\n', 'import os\\n', 'import matplotlib.pyplot as plt\\n', 'from pandas.plotting import lag_plot']\n",
      "code ['path_to_file = \"DOHMH_HIV_AIDS_Annual_Report.csv\"\\n', \"data = pd.read_csv(path_to_file, encoding='utf-8')\"]\n",
      "code ['data']\n",
      "code ['data.dtypes']\n",
      "code [\"data.Year = pd.to_datetime(data.Year, format='%Y')\"]\n",
      "code ['data.dtypes']\n",
      "code ['data.shape']\n",
      "code ['data.isnull()']\n",
      "code ['data.isnull().sum()']\n",
      "code ['data.info()']\n",
      "code ['data.head()']\n",
      "code ['data.tail()']\n",
      "code ['data1=data.replace(to_replace =99999, \\n', '                 value =0) \\n', 'data1']\n",
      "code ['data1.describe()']\n",
      "code ['g1 = data1.groupby( [ \"Year\", \"HIV-related death rate\"] ).sum()\\n', 'g1']\n",
      "code ['sum1=data1.groupby(\\'Year\\', as_index=False).agg({\"HIV-related death rate\": \"sum\"})\\n', 'sum1']\n",
      "code ['sum2=data1.groupby(\\'Year\\', as_index=False).agg({\"Non-HIV-related death rate\": \"sum\"})\\n', 'sum2']\n",
      "code ['sum3=data1.groupby(\\'Year\\', as_index=False).agg({\"Death rate\": \"sum\"})\\n', 'sum3']\n",
      "code ['df=[sum1,sum2,sum3]\\n', '#df\\n', 'result = pd.concat(df)\\n', 'result']\n",
      "code []\n",
      "code [\"df1 = pd.merge(sum1, sum2, on='Year',how='inner')\\n\", 'df1']\n",
      "code [\"data2 = pd.merge(df1, sum3, on='Year',how='inner')\\n\", 'data2']\n",
      "code []\n",
      "code [\"lag_plot(data1['Non-HIV-related death rate'])\\n\", '#Death rate\\n', \"#s2=lag_plot(data1['Death rate'])\"]\n",
      "code [\"lag_plot(data1['Death rate'])\"]\n",
      "code [\"lag_plot(data1['HIV-related death rate'])\"]\n",
      "code ['n_groups = 5\\n', 'fig, ax = plt.subplots(figsize=(13,5))\\n', 'bar_width = 0.20\\n', 'opacity = 0.8\\n', 'index = np.arange(n_groups)\\n', '\\n', \"rects1 = plt.bar(index - bar_width, data2['HIV-related death rate'], bar_width,alpha=opacity,color='b',label='HIV-related death rate')\\n\", \"rects2 = plt.bar(index, data2['Non-HIV-related death rate'], bar_width,alpha=opacity,color='g',label='Non-HIV-related death rate')\\n\", \"rects3 = plt.bar(index + bar_width, data2['Death rate'], bar_width,alpha=opacity,color='r',label='Death rate')\\n\", \"plt.xlabel('years')\\n\", \"plt.ylabel('death count')\\n\", \"plt.title('Deaths')\\n\", \"plt.xticks(index + bar_width, ('2011', '2012', '2013', '2014', '2015'))\\n\", 'plt.legend()\\n', 'plt.tight_layout()\\n', 'plt.show()']\n",
      "code ['fig, ax = plt.subplots(figsize=(10,5))\\n', \"plt.plot('Year' ,'HIV-related death rate', data=data2, marker='', markerfacecolor='blue', color='skyblue', linewidth=4)\\n\", \"plt.plot( 'Year','Non-HIV-related death rate',  data=data2, marker='', color='red', linewidth=3)\\n\", 'plt.plot(\\'Year\\', \\'Death rate\\', data=data2, marker=\\'\\', color=\\'olive\\', linewidth=3, label=\"Death rate\")\\n', \"plt.xlabel('years')\\n\", \"plt.ylabel('death count')\\n\", \"plt.title('Deaths')\\n\", 'plt.legend()\\n']\n",
      "code [\"#if data1['year'] = 2011 \\n\", '\\n', \"data_f = data1[data1.Year == '2011']\\n\", 'data_f.head(5)']\n",
      "code ['data_f\\n', \"#data1['Year'] = pd.to_year(data1['Year'])\"]\n",
      "code [\"data_f['Year'].head()\"]\n",
      "code []\n",
      "code ['!pip install nltk']\n",
      "code ['import nltk']\n",
      "markdown ['The following is a one-time action to get a list of stopwords for NLTK']\n",
      "code [\"nltk.download('stopwords')\"]\n",
      "code [\"nltk.download('punkt')\"]\n",
      "markdown ['https://www.bradezone.com/2008/09/13/boring/']\n",
      "code ['this_text=\"I go to the store. A car is parked. \\\\\\n', 'Many cars are parked or moving. Some are blue. \\\\\\n', 'Some are tan. They have windows. In the store, \\\\\\n', 'there are items for sale. These include such \\\\\\n', 'things as soap, detergent, magazines, and lettuce. \\\\\\n', 'You can enhance your life with these products. \\\\\\n', 'Soap can be used for bathing, be it in a bathtub \\\\\\n', 'or in a shower. My email address is myname@sc.edu. \\\\\\n', 'Apply the soap to your body and rinse. My phone \\\\\\n', 'number is 452-953-2942. Detergent is used to \\\\\\n', 'wash clothes. Place your dirty clothes \\\\\\n', 'into a washing machine and add some detergent \\\\\\n', 'as directed on the box. Your email is \\\\\\n', 'aperson@farm.com and your cell is 595-942-2424. \\\\\\n', 'Select the appropriate settings on your \\\\\\n', 'washing machine and you should be ready to \\\\\\n', 'begin. Magazines are stapled reading material \\\\\\n', 'made with glossy paper, and they cover a wide \\\\\\n', 'variety of topics, ranging from news and \\\\\\n', 'politics to business and stock market information.\"']\n",
      "code ['from nltk.tokenize import sent_tokenize,word_tokenize ']\n",
      "code ['sent_tokenize_list = sent_tokenize(this_text)\\n', 'sent_tokenize_list']\n",
      "code ['for this_sent in sent_tokenize_list:\\n', '    word_tokens = word_tokenize(this_sent) \\n', '    print(word_tokens)']\n",
      "code ['from nltk.corpus import stopwords\\n', \"en_stops = set(stopwords.words('english'))\"]\n",
      "code ['en_stops']\n",
      "code ['len(en_stops)']\n",
      "code ['for this_sent in sent_tokenize_list:\\n', '    filtered_sentence = [] \\n', '    word_tokens = word_tokenize(this_sent) \\n', '    for w in word_tokens: \\n', '        if w not in en_stops: \\n', '            filtered_sentence.append(w) \\n', '    print(filtered_sentence)']\n",
      "code []\n",
      "markdown ['# PROJECT 1 (Movie Budgets and Box office earnings)']\n",
      "markdown ['__PROPOSAL__\\n', '\\n', 'The data on which I am interested to do my project is about the budget and profit of Hollywood movies which got released over a decade.\\n', '\\n', 'Data source discovered from : https://umbc.app.box.com/notes/375877568053?s=inwbinsctcmedcknp7h8r3tm1das5vs9\\n', '\\n', 'Data source location : https://www.statcrunch.com/app/index.php?dataid=2188684\\n', '\\n', 'I was going through few data and data sources and I found the data about the movie budget and the gross profit these movies have earned in this country and all over the world,everytime i search for a movie on a search engine i always find the cast of the movie, the budget and how much profit the movie has earned. So, i have found this data set is appropriate for my interests on knowing and learning on the data about the budget and the profits made, and also i would be able to know the marginal changes in the budget and profit ratio over this period.\\n', 'The data which i have extrated is free of cost and is legal to use.\\n', 'Documented EDA isnt available for the data which im using.\\n', '\\n', \"The data contains hollywood movie's data since 1915\\n\", '\\n']\n",
      "markdown ['Primary Observation: More investment(Budget) leads to higher profits(domestic and worldwide gross) for a movie, as per the dataset. The Domestic gross is less than worldwide gross from the past 100 years.']\n",
      "markdown ['conclusion : Whatever may be the budget of the movie, the worldwide gross is always greater than the domestic gross of a movie since the year 1915. For higher profits the budget need no be higher all the time. Budget numbers and the profits for movies can be both difficult to find and unreliable.']\n",
      "code ['#Importing Pandas, numpy, chardet, ascii, string, sys, re, csv,pyplot\\n', '\\n', 'import pandas as pd\\n', 'import numpy as np\\n', 'import chardet\\n', 'import ascii\\n', 'import string\\n', 'import sys\\n', 'import re\\n', 'import csv\\n', 'import os\\n', 'import matplotlib.pyplot as plt\\n', '\\n', '#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n', '#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html']\n",
      "markdown ['pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. \\n', 'NumPy is the most basic yet a powerful package for scientific computing and data manipulation in Python. It is an open source library available in Python. It helps as to do the mathematical and scientific operation and used extensively in data science.\\n', 'It provides high-performance, easy to use structures and data analysis tools. Unlike NumPy library which provides objects for multi-dimensional arrays, Pandas provides in-memory 2d table object called Dataframe\\n', 'Chardet is a Python port, universal character encoding detector.\\n', 'The os module is a part of the standard library, or stdlib, within Python 3. This means that it comes with your Python installation, but you still must import it.\\n', '\\n', 'Citation:https://docs.python.org/3/library/']\n",
      "code ['#Loading the data into pandas data frame\\n', '\\n', '#https://www.statcrunch.com/app/index.php?dataid=2188684 ---> csv data used\\n', '\\n', 'path_to_file = \"Project_Movies_Data.csv\"\\n', \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\", \"#loading the csv file to a variable 'data'\\n\", '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\\n']\n",
      "markdown ['Using Python dictionaries and lists to create DataFrames only works for small datasets that you can type out manually.']\n",
      "code ['#to view the data type of each column\\n', '\\n', 'data.dtypes\\n', '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/']\n",
      "code ['data.shape  #Finding the rows and columns of the dataset']\n",
      "code ['#viewing the complete data present in the data frame\\n', '\\n', 'data\\n', '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\\n']\n",
      "code ['#checking for null values\\n', '\\n', 'data.isnull()']\n",
      "code ['#checking for any null values present in the data set\\n', 'data.isnull().sum()']\n",
      "code ['#checking the total number of null values present in the data set\\n', 'data.isnull().sum().sum()']\n",
      "markdown ['Pandas data.info() function is used to get a concise summary of the dataframe.']\n",
      "code ['#summary of the dataframe\\n', '\\n', 'data.info()\\n', '\\n', '#citation:https://www.google.com/search?q=data.info()+in+python&rlz=1C1SQJL_enUS862US862&oq=data.info()&aqs=chrome.1.69i57j0l5.2068j0j4&sourceid=chrome&ie=UTF-8']\n",
      "code ['#checking for any missing values in the data\\n', '\\n', 'missing_values = [\"n/a\", \"na\", \"--\"]\\n', 'data = pd.read_csv(\"Project_Movies_Data.csv\", na_values = missing_values)\\n', 'missing_values\\n', 'data.isnull().values.any()']\n",
      "markdown ['There are no Null, n/a, NaN, special characters in the dataset. ']\n",
      "code ['data.head()\\n', '\\n', '#consistent data for data headings']\n",
      "markdown ['checking if there are any inconsistant movie names and listing them']\n",
      "code ['#checking for any special characters in the data\\n', '\\n', 'alphabet = string.ascii_letters+string.punctuation\\n', '\\n', 'data.Movie.str.strip(alphabet).astype(bool).any()\\n', '\\n', '#citation:https://stackoverflow.com/questions/53375623/pandas-keep-default-na-false-does-not-work\\n', '#citation:https://www.geeksforgeeks.org/python-program-check-string-contains-special-character/']\n",
      "code ['#listing out the special characters in the data\\n', '\\n', 'data.Movie.str.strip(alphabet)\\n', '\\n', '#citation:https://stackoverflow.com/questions/53375623/pandas-keep-default-na-false-does-not-work\\n', '#citation:https://www.geeksforgeeks.org/python-program-check-string-contains-special-character/']\n",
      "markdown ['checking for any data inconsistencies in the movie names, all the names mentioned above are also consistent.']\n",
      "code ['#no null values \\n', 'null_values=data.columns[data.isnull().any()]\\n', 'data[null_values].isnull().sum()\\n', 'null_values\\n']\n",
      "code ['#no null values\\n', 'data.Movie.isnull().sum()']\n",
      "markdown ['Pandas data.describe() is used to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values.']\n",
      "code ['#to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values\\n', 'data.describe()\\n', '\\n', '#citation: https://minimaxir.com/2018/07/imdb-data-analysis/']\n",
      "code ['data.Movie.head()\\n']\n",
      "code ['data.Day.head()']\n",
      "code ['data.Year.head()']\n",
      "code [\"data['Movie'].is_unique\"]\n",
      "code [\"movies = data['Movie'].unique().tolist()\\n\", 'movies\\n']\n",
      "markdown ['1)Checked for null values in the data set\\n', '\\n', '2)check for any special characters in the data set, if yes listed all the values.\\n', '\\n', '3)checked for missing values\\n', '\\n', '4)No null values, missing values found, special characters were found in the movie names which can be ignored.\\n', '\\n', '5)All the movie names were unique\\n', '\\n', 'No data inconsistencies found in the data set']\n",
      "markdown ['Scatter Plots: The coordinates of each point are defined by two dataframe columns and filled circles are used to represent each point. This kind of plot is useful to see complex correlations between two variables. Points could be for instance natural 2D coordinates like longitude and latitude in a map or, in general, any pair of metrics that can be plotted against each other.']\n",
      "code ['#plotting a scatter plot against Budget invested in the movie and the Domestic gross earned by a particular film.\\n', 'fig, ax = plt.subplots()\\n', \"ax.scatter(data['Budget($M)'], data['Domestic Gross($M)'])\\n\", \"ax.set_title('Movies Dataset for budget and domestic gross(without plot values)')\\n\", \"ax.set_xlabel('Budget($M)')\\n\", \"ax.set_ylabel('Domestic Gross($M)')\\n\", '\\n', '\\n', '\\n', '#citation:https://stackoverflow.com/questions/32319166/highlight-last-data-point-in-scatter-plot-with-pandas\\n', '#citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e\\n', '#citation:https://plot.ly/python/line-and-scatter/']\n",
      "markdown ['positive correlation : Variables that are positively correlated move in the same direction']\n",
      "code ['#plotting a scatter plot against Budget invested in the movie and the Domestic gross earned by a particular film.\\n', '#scatter plots with the values\\n', 'plt.clf()\\n', '\\n', '# using Budget and Domestic gross\\n', \"xs = (data['Budget($M)'])\\n\", \"ys = (data['Domestic Gross($M)'])\\n\", 'plt.figure(figsize=(10,10))\\n', \"plt.scatter(xs,ys,s=70, c='darkred')\\n\", '\\n', '# zip joins x and y coordinates in pairs\\n', 'for x,y in zip(xs,ys):\\n', '\\n', '    label = \"{:.0f}\".format(y)\\n', '\\n', '    plt.annotate(label,# this is the text\\n', '                 (x,y), # this is the point to label\\n', '                 textcoords=\"offset points\", # how to position the text\\n', '                 xytext=(0,10), # distance from text to points (x,y)\\n', \"                 ha='center') # horizontal alignment can be left, right or center\\n\", '\\n', 'plt.xticks(np.arange(0,500,100))\\n', 'plt.yticks(np.arange(0,1000,100))\\n', \"plt.title('Movies Dataset(A scatter plot for budget and the domestic gross)(scatterplot with plot values)')\\n\", \"plt.xlabel('Budget of the movie in millions')\\n\", \"plt.ylabel('Domestic Gross of the movie in millions')\\n\", '\\n', 'plt.show()\\n', '\\n', '#Citation: https://pythonprogramming.net/legends-titles-labels-matplotlib-tutorial/\\n', '#Citation: https://stackoverflow.com/questions/29188757/matplotlib-specify-format-of-floats-for-tick-lables\\n', '#Citation: http://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples\\n', '#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html']\n",
      "markdown ['The above scatter plot hows us the Budget invested in the movie and the Domestic gross earned. It is observed that few movies despite of investing less made a large amount of profits, whereas few who invested more havent, on the plot we can observe that the movie which invested for about 300 million dollars has earned a profit of 937 million dollars.\\n', 'The above scatterplot gives the complete description of the invested Budget and the earned profits.\\n', 'The above scatterplot gives the complete description of the invested Budget and the earned domestic profits.\\n', 'The budget and the profits are sometimes undetermined, but most of the times the gross is high.']\n",
      "code ['#plotting a scatter plot against Budget invested in the movie and the Worldwide Gross earned by a particular film.\\n', 'fig, ax = plt.subplots()\\n', \"ax.scatter(data['Budget($M)'], data['Worldwide Gross($M)'])\\n\", \"ax.set_title('Movies Dataset for budget and worldwide gross(scatter plot without plot values)')\\n\", \"ax.set_xlabel('Budget($M)')\\n\", \"ax.set_ylabel('Worldwide Gross($M)')\\n\", '\\n', '\\n', '#Citation:https://stackoverflow.com/questions/32319166/highlight-last-data-point-in-scatter-plot-with-pandas\\n', '#Citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e\\n', '#Citation:https://plot.ly/python/line-and-scatter/']\n",
      "markdown ['Positive correlation']\n",
      "code ['#plotting a scatter plot against Budget invested in the movie and the Worldwide Gross earned by a particular film.\\n', '#scatter plots with the values\\n', 'plt.clf()\\n', '#using budget and worldwide gross\\n', \"xs = (data['Budget($M)'])\\n\", \"ys = (data['Worldwide Gross($M)'])\\n\", 'plt.figure(figsize=(10,10))\\n', \"plt.scatter(xs,ys,s=70, c='darkblue')\\n\", '\\n', '# zip joins x and y coordinates in pairs\\n', 'for x,y in zip(xs,ys):\\n', '\\n', '    label = \"{:.2f}\".format(y)\\n', '\\n', '    plt.annotate(label, # this is the text\\n', '                 (x,y), # this is the point to label\\n', '                 textcoords=\"offset points\", # how to position the text\\n', '                 xytext=(0,10), # distance from text to points (x,y)\\n', \"                 ha='center') # horizontal alignment can be left, right or center\\n\", '\\n', 'plt.xticks(np.arange(0,600,100))\\n', 'plt.yticks(np.arange(0,4000,1000))\\n', \"plt.title('Movies Dataset(A scatter plot for budget and the WorldWide gross)(scatter plot with plot values)')\\n\", \"plt.xlabel('Budget of the movie in millions')\\n\", \"plt.ylabel('Worldwide Gross of the movie in millions')\\n\", '\\n', 'plt.show()\\n', '\\n', '#Citation: https://pythonprogramming.net/legends-titles-labels-matplotlib-tutorial/\\n', '#Citation: https://stackoverflow.com/questions/29188757/matplotlib-specify-format-of-floats-for-tick-lables\\n', '#Citation: http://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples\\n', '#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html']\n",
      "markdown ['The above scatter plot hows us the Budget invested in the movie and the WorldWide gross earned.The above scatter plot hows us the Budget invested in the movie and the Wroldwide gross earned. It is observed that few movies despite of investing less made a large amount of profits, whereas few who invested more havent, on the plot we can observe that the movie which invested for about 400 million dollars has earned a profit of 2783 million dollars.\\n', 'The above scatterplot gives the complete description of the invested Budget and the earned worldwide profits.\\n', 'The budget and the profits are sometimes undetermined, but most of the times the gross is high']\n",
      "markdown ['A histogram is a representation of the distribution of data. This function calls matplotlib.pyplot.hist() , on each series in the DataFrame, resulting in one histogram per column.']\n",
      "markdown ['Histogram for the Worldwide Gross and domestic gross  in millions ']\n",
      "code ['#Histogram for the Worldwide Gross in millions\\n', '# create figure and axis\\n', 'fig, ax = plt.subplots()\\n', '# plot histogram\\n', \"ax.hist(data['Worldwide Gross($M)'])\\n\", '# set title and labels\\n', \"ax.set_title('Movies Dataset')\\n\", \"ax.set_ylabel('Movies Count')\\n\", \"ax.set_xlabel('Worldwide Gross($M)')\\n\", 'data.hist(column=\"Worldwide Gross($M)\", bins=5)\\n', \"plt.title('Movies Dataset(A scatter plot for Movie count and the Domestic profits earned)')\\n\", \"plt.ylabel('Number of movies')\\n\", \"plt.xlabel('Domestic Gross of the movie in millions')\\n\", '\\n', '#Citation:https://wellsr.com/python/python-create-histogram-from-pandas-dataframe/\\n', '#Citation:https://www.mathworks.com/matlabcentral/answers/436999-histogram-x-axis-range\\n', '#Citation:https://stackoverflow.com/questions/6986986/bin-size-in-matplotlib-histogram\\n', '#Citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e']\n",
      "markdown ['The above histogram shows the profit earned (Worldwide gross) for the number of movies in the data set, The maximum profit earned by the the number of movies(movies count). more than 5000 movies earned a profit ranging from (0 to 500 million dollars) and fewer than 500 movies has earned a profit ranging from (600 to 1200 million dollars), fewer than 100 movies has earned a profit of about 1500 million dollars']\n",
      "code ['#Histogram for the Domestic Gross in millions\\n', 'fig, ax = plt.subplots()\\n', '# plot histogram\\n', \"ax.hist(data['Domestic Gross($M)'])\\n\", '# set title and labels\\n', \"ax.set_title('Movies Dataset')\\n\", \"ax.set_ylabel('Movies count')\\n\", \"ax.set_xlabel('Domestic Gross($M)')\\n\", 'data.hist(column=\"Domestic Gross($M)\", bins=5)\\n', \"plt.title('Movies Dataset(A scatter plot for Movie count and the profits earned)')\\n\", \"plt.ylabel('Number of movies')\\n\", \"plt.xlabel('Worldwide Gross of the movie in millions')\\n\", '\\n', '#Citation:https://wellsr.com/python/python-create-histogram-from-pandas-dataframe/\\n', '#Citation:https://www.mathworks.com/matlabcentral/answers/436999-histogram-x-axis-range\\n', '#Citation:https://stackoverflow.com/questions/6986986/bin-size-in-matplotlib-histogram\\n', '#Citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e']\n",
      "markdown ['The above histogram shows the profit earned (Domestic gross) for the number of movies in the data set']\n",
      "markdown ['The above histogram shows the profit earned (Domestic gross) for the number of movies in the data set, The maximum profit earned by the the number of movies(movies count). About 5000 movies earned a profit ranging from (0 to less than 200 million dollars) and fewer than 500 movies has earned a profit ranging from (less than 200 to less than 400 million dollars), fewer than 100 movies has earned a profit of about 600 million dollars']\n",
      "markdown ['The number of movies which earned higher profits are very high than the number of movies earning lower profits.']\n",
      "markdown ['conclusion : Whatever may be the budget of the movie, the worldwide gross is always greater than the domestic gross of a movie since the year 1915. For higher profits the budget need no be higher all the time. Budget numbers and the profits for movies can be both difficult to find and unreliable.']\n",
      "markdown ['__*Learned:*__\\n', '  \\n', '  -- Data Visualization\\n', '  \\n', '  -- Working with different data frames\\n', '  \\n', '  -- Data analysis\\n', '  \\n', '  -- working with pandas and many more different libraries in python']\n",
      "code []\n",
      "markdown ['# Project - 2 : DOHMH_HIV_AIDS_Annual_Report']\n",
      "markdown ['The data which is under study is the HIV-AIDS-ANNUAL-REPORT.\\n', '\\n', 'Data source discovered from : https://catalog.data.gov/dataset\\n', '\\n', 'Data source location : https://catalog.data.gov/dataset/dohmh-hiv-aids-annual-report\\n', '\\n', 'I choose this data as I was concerned about the effects of HIV increasing on an everyday basis, with a rapid deterioration of health of those affected by it. Statistics show that more people die of HIV than either of alchohol or drug overdose.\\n', 'This data set is based on the death rates (includes both HIV and NON-HIV) in some of the boroughs in New York from 2011 to 2015.\\n', '\\n', 'Hypothesis:\\n', '-----------\\n', '**Hypothesis:1** There has been a decline in the number of HIV related deaths from 2011 to 2015 because of a four-part strategy began in New York post 2012 called Ending the Epidemic (EtE) , result of which is the aforementioned point.The Non-HIV related death-rate was constant but HIV-realted deaths declined thus resulting in a reduction in the overall death rates.\\n', '     \\n', '**Hypothesis:2** Bronx and brookyln are the two boroughs in New York city as these two boroughs have less awareness about the disease and they are populated mostly with the non-awareness communities and unrecorded death cases which is why these two boroughs have more HIV related death rates.\\n', '\\n', '**Hypothesis:3** One in every seven African-American with HIV are unaware they have it. They are the most affected race among the all the races in New York city. Some African-American communities have higher rates of a few sexually transmitted diseases (STDs) than alternative racial/ethnic communities.\\n', '\\n', '\\n', 'The data which i have extracted is free of cost and is legal to use.\\n', 'Documented EDA isnt available for the data which im using.\\n', 'This data has 18 columns and 6006 rows.\\n', '\\n', '**Primary Observation:** HIV related death rates are because of the people with less awareness about the disease and they are decreasing gradually from the year 2011.\\n']\n",
      "code ['#Loading the data into pandas data frame through URL\\n', '\\n', '#https://catalog.data.gov/dataset/dohmh-hiv-aids-annual-report --> CSV File used\\n', '\\n', 'path_to_file = \"https://data.cityofnewyork.us/api/views/fju2-rdad/rows.csv?accessType=DOWNLOAD\"\\n', \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\", '\\n', '#citation:https://www.geeksforgeeks.org/different-ways-to-import-csv-file-in-pandas/']\n",
      "code []\n",
      "markdown ['# week 9: removing punctuation and stop words from a corpus']\n",
      "markdown ['For each file, remove punctuation and stop words\\u200b\\n', '\\n', 'Produce a single .dat file containing the name of each file in quotes, a colon, then a list of words separated by commas\\u200b. The list of words per file should be unique for that file. Do not include URLs or phone numbers. Words should be made lowercase. ']\n",
      "code ['!pip install nltk\\n', '\\n', '#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n', '#citation:https://docs.python.org/3/howto/unicode.html\\n', '#citation: https://stackoverflow.com/questions/35715353/glob-iglob-to-find-all-txt-files-in-all-sub-directories-yields-error']\n",
      "code ['pip install --pre python-docx']\n",
      "code ['pip install glob2']\n",
      "code ['import math\\n', 'import string\\n', 'import os\\n', 'import re\\n', 'import os.path\\n', 'import glob\\n', 'import fnmatch\\n', 'import nltk\\n', 'import errno\\n', 'from nltk.corpus import stopwords \\n', 'from nltk.tokenize import word_tokenize \\n', 'from nltk.tokenize import RegexpTokenizer\\n', 'from docx import Document\\n', 'from nltk.tokenize import sent_tokenize,word_tokenize \\n', '\\n', '#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n', '#citation:https://docs.python.org/3/howto/unicode.html\\n', '#citation: https://stackoverflow.com/questions/35715353/glob-iglob-to-find-all-txt-files-in-all-sub-directories-yields-error']\n",
      "code ['#downloading stop words\\n', '\\n', \"nltk.download('stopwords')\\n\", '\\n', '#reference: http://localhost:8889/notebooks/tfidf.ipynb (lecture notebook)']\n",
      "code [\"nltk.download('punkt')\\n\", '\\n', '#reference: http://localhost:8889/notebooks/tfidf.ipynb (lecture notebook)']\n",
      "markdown ['### For single text file (how_rubber_goods_are_made.txt)']\n",
      "code ['#reading the text file and converting it into lower case\\n', '\\n', \"file=open('how_rubber_goods_are_made.txt','r',encoding='UTF-8')\\n\", 'file1= file.read()\\n', '#print(file1)\\n', 'string1= file1\\n', 'file_after=string1.lower()\\n', 'file_after\\n', '\\n', '#citation: http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html\\n', '#citation:https://realpython.com/working-with-files-in-python/#listing-all-files-in-a-directory\\n', '#citation:https://www.quora.com/How-do-I-read-mutiple-txt-files-from-folder-in-python']\n",
      "code ['#the file after converting to lowercase, removing the punctuation marks, tokenizing the string, removing digits, removing stop words\\n', '\\n', 'my_string= file_after\\n', 'table=str.maketrans(\"!,.?’”“\\':\", 9*\" \") \\n', 'file_1=my_string.translate(table)\\n', '#file_1\\n', 'sent_tokenize_list = sent_tokenize(file_1)\\n', 'sent_tokenize_list\\n', 'for this_file in sent_tokenize_list:\\n', '    word_tokens = word_tokenize(this_file) \\n', '   # print(word_tokens)\\n', 'for n in sent_tokenize_list:\\n', '    filtered_sentence = [] \\n', '    word_tokens = word_tokenize(n) \\n', '    for w in word_tokens: \\n', '        if w not in file_stop: \\n', '            filtered_sentence.append(w) \\n', '    print(filtered_sentence)\\n', \"    result = ''.join([i for i in filtered_sentence if not i.isdigit()])\\n\", '    result\\n', '    \\n', '#citation: https://www.youtube.com/watch?v=ru5jQA2Nre8\\n', '#citation: https://swcarpentry.github.io/python-novice-inflammation/06-func/index.html\\n', '#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n', '#reference: http://localhost:8889/notebooks/tfidf.ipynb (lecture notebook)\\n', '#citation: https://stackoverflow.com/questions/12851791/removing-numbers-from-string/12851835']\n",
      "code [\"file_stop = set(stopwords.words('english'))\"]\n",
      "code ['file_stop ']\n",
      "markdown ['### For multiple files ']\n",
      "code ['#taking the given files in a list\\n', '\\n', \"file_all=['52256-0.txt',\\n\", \" '53031-0.txt',\\n\", \" '58108-0.txt',\\n\", \" 'blind_text.txt',\\n\", \" 'dr_yawn.txt',\\n\", \" 'how_rubber_goods_are_made.txt',\\n\", \" 'most_boring_ever.txt',\\n\", \" 'most_boring_part2.txt',\\n\", \" 'pg12814.txt',\\n\", \" 'pg14895.txt',\\n\", \" 'pg43994.txt',\\n\", \" 'random_text.txt',\\n\", \" 'smiley_the_bunny.txt']\"]\n",
      "code ['file_all']\n",
      "code [\"#for all the files, converting them into lowercase, removing punctuatuon marks, removing links, replacing '_', removing phonenumbers\\n\", '#removing stop words\\n', '#printing in \"File 1.txt\" : word1, word2, word3, word7 format\\n', '#saving in outputfile2.dat\\n', '\\n', 'for n in file_all: \\n', \"    file= open (n , 'rb')\\n\", \"    lines= file.read().decode('UTF-8')\\n\", '    sentence= lines.lower()\\n', '    url_remove = sentence.replace(\"www.\",\"\")\\n', \"    tokenizer = RegexpTokenizer(r'\\\\w+')\\n\", '    tokens= tokenizer.tokenize(url_remove)\\n', '    words = list(filter(lambda n: n not in string.punctuation, tokens))\\n', '    stop_words = set(stopwords.words(\"english\"))\\n', '    cleaned=list(filter(lambda n:n not in stop_words, words))\\n', \"    result = ' '.join([i for i in cleaned if not i.isdigit()])\\n\", \"    final_text = ''.join(result)\\n\", '    finals_text = final_text.replace(\"_\", \" \")\\n', '    print(\\'\"\\'+ n + \\'\"\\', \\':\\', \\'\\\\n\\',\\'\\\\n\\', finals_text, \\'\\\\n\\',\\'\\\\n\\')\\n', \"    saveFile = open('outputFile2.dat','w')\\n\", '    saveFile.write(finals_text)\\n', '    #saveFile.close()\\n', '    \\n', '#citation: https://stackoverflow.com/questions/7794208/how-can-i-remove-duplicate-words-in-a-string-with-python\\n', '#citation: https://www.youtube.com/watch?v=ru5jQA2Nre8\\n', '#citation: https://stackoverflow.com/questions/12851791/removing-numbers-from-string/12851835\\n', '#citation: https://swcarpentry.github.io/python-novice-inflammation/06-func/index.html\\n', '#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n', '#citation: https://www.youtube.com/watch?v=O2onA4r5UaY\\n', '#citation: https://www.youtube.com/watch?v=EVhx6kjJ85U\\n', '#citation: https://stackoverflow.com/questions/12851791/removing-numbers-from-string/12851835\\n', '#citation: https://stackoverflow.com/questions/3411771/best-way-to-replace-multiple-characters-in-a-string\\n', '\\n']\n",
      "code ['#reading the content in the document file\\n', '\\n', \"document = Document('week_10_document1.docx')\\n\", 'for para in document.paragraphs:\\n', '    print(para.text)']\n",
      "code ['#removing stop words, punctuations, phonenumbers for the first document\\n', \"document = Document('week_10_document1.docx')\\n\", 'for para in document.paragraphs:\\n', '    #print(para.text)\\n', '    sentence= para.text.lower()\\n', '    finals1 = sentence.replace(\".\", \" \")\\n', '    finals2 = finals1.replace(\",\", \" \")\\n', '    words = list(filter(lambda n: n not in string.punctuation, tokens))\\n', \"    #stop_words = set(stopwords.words('english')) \\n\", '    #words = list(filter(lambda n: n not in string.punctuation, tokens))\\n', '    #stop_words = set(stopwords.words(\"english\"))\\n', '    #cleaned=list(filter(lambda n:n not in stop_words, words))\\n', '    word_tokens = word_tokenize(finals2) \\n', '    filtered_sentence1 = [w for w in word_tokens if not w in stop_words] \\n', '    #filtered_sentence10 = [] \\n', '    for w in word_tokens: \\n', '        if w not in stop_words: \\n', '            filtered_sentence.append(w) \\n', '#print(word_tokens) \\n', 'print(filtered_sentence1)  \\n', '\\n', '#citation :https://stackoverflow.com/questions/25228106/how-to-extract-text-from-an-existing-docx-file-using-python-docx\\n', '#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n', '#citation : https://python-docx.readthedocs.io/en/latest/user/documents.html']\n",
      "code ['#removing stop words, punctuations, phonenumbers for the second document\\n', \"document2 = Document('week_10_document2.docx')\\n\", 'for p in document2.paragraphs:\\n', '    #print(p.text)\\n', '    sentence2= p.text.lower()\\n', '    finals3 = sentence.replace(\".\", \" \")\\n', '    finals4 = finals3.replace(\",\", \" \")\\n', \"    stop= set(stopwords.words('english')) \\n\", '    word_tokens = word_tokenize(finals4) \\n', '    filtered_sentence2 = [wt for wt in word_tokens if not wt in stop] \\n', '    #filtered_sentence2 = [] \\n', '    for wt in word_tokens: \\n', '        if wt not in stop: \\n', '            filtered_sentence.append(wt) \\n', '#print(word_tokens) \\n', 'print(filtered_sentence2) \\n', '\\n', '#citation :https://stackoverflow.com/questions/25228106/how-to-extract-text-from-an-existing-docx-file-using-python-docx\\n', '#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n', '#citation : https://python-docx.readthedocs.io/en/latest/user/documents.html']\n",
      "code ['#list of unique values \\n', '\\n', 'list1 = filtered_sentence1\\n', 'print(\"the unique values from 1st document is\") \\n', 'unique(filtered_sentence1) \\n', '\\n', 'list2 =filtered_sentence2\\n', 'print(\"\\\\nthe unique values from 2nd document is\") \\n', 'unique(filtered_sentence2)\\n', '  \\n', '#citation : https://www.geeksforgeeks.org/python-get-unique-values-list/\\n']\n",
      "code ['#saving the content in the file\\n', \"saveFile = open('outputFile.dat','w')\\n\", 'saveFile.write(finals)\\n', 'saveFile.close()\\n', '\\n', '#citation : https://www.youtube.com/watch?v=f6zeuk5UjuE\\n']\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "markdown ['# install library']\n",
      "raw ['!conda install -c anaconda openpyxl']\n",
      "raw ['!pip install openpyxl']\n",
      "code ['import openpyxl']\n",
      "markdown ['# load data']\n",
      "code [\"wb = openpyxl.load_workbook('week_05_in-class_activity.xlsx')\"]\n",
      "code ['# select the one available sheet; assign to a variable\\n', \"sheet = wb['Sheet1']\"]\n",
      "markdown ['&nbsp;\\n', '# objective 1: create a new column that is the sum of the second (B) and third (C) columns']\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "markdown ['&nbsp;\\n', '# objective 2: create a new row when the value in column 1 is less than 5. In the new row, double the entries of the original row\\n', '\\n', 'The new row can go at the bottom of the existing sheet']\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "markdown ['https://en.wikipedia.org/wiki/Tf%E2%80%93idf']\n",
      "code ['import math\\n', 'import os\\n', 'import fnmatch # https://docs.python.org/3/library/fnmatch.html']\n",
      "markdown ['Source https://stevenloria.com/tf-idf/ <BR>\\n', '\\n', 'Caveat: this post now uses TextBlob for breaking up the text into words and getting the word counts.']\n",
      "code ['def term_freq(term, list_of_words_in_document):\\n', '    \"\"\"\\n', '    computes \"term frequency\" which is the number of times a word appears in a document, \\n', '    normalized by dividing by the total number of words in document. \\n', '    \"\"\"\\n', '    return list_of_words_in_document.count(term)/(len(list_of_words_in_document)*1.0)\\n', '\\n', 'def number_of_documents_containing(term,all_documents):\\n', '    \"\"\"\\n', '    Returns the number of documents containing word. \\n', '    \"\"\"\\n', '    countr=0\\n', '    for this_doc in all_documents:\\n', '        if (term in this_doc):\\n', '            countr+=1\\n', '    return countr\\n', '\\n', 'def inverse_doc_freq(term, all_documents):\\n', '    \"\"\"\\n', '    computes \"inverse document frequency\" which measures \\n', '    how common a word is among all documents in corpus. \\n', '    The more common a word is, the lower its idf. \\n', '    Take the ratio of the total number of documents to the number of documents containing word, \\n', '    then take the log of that. Add 1 to the divisor to prevent division by zero.\\n', '    \"\"\"\\n', '    return math.log(len(all_documents) / ( 1.0 + number_of_documents_containing(term, all_documents)))\\n', '\\n', 'def tfidf(term, list_of_words_in_document, all_documents):\\n', '    \"\"\"\\n', \"    computes the TF-IDF score. It's the product of tf and idf.\\n\", '    \"\"\"\\n', '    return term_freq(term, list_of_words_in_document) * inverse_doc_freq(term, all_documents)']\n",
      "markdown ['The \\\\*.dat files in the directory have only key words from each file\\n', '\\n', 'Convert the .dat contents to lists per document']\n",
      "code ['all_documents={}\\n', 'all_words_from_all_docs=[]\\n', 'all_terms=[]\\n', \"foldr='../week_03_getting_data/essays/'\\n\", \"fname='*.dat'\\n\", 'for file_name in os.listdir(foldr):\\n', '    #print(file_name)\\n', '    if fnmatch.fnmatch(file_name, fname): # Unix shell-style wildcards\\n', '        print(file_name)\\n', \"        with open(foldr+file_name,'r') as fil:\\n\", '            words_in_file=fil.read().split(\"\\\\n\")\\n', '        # remove empty strings from list of words\\n', '        while \"\" in words_in_file:\\n', '            words_in_file.remove(\"\")\\n', '        # save the words per file as value in a dictionary\\n', '        all_documents[file_name]=words_in_file\\n', \"        print('has',len(words_in_file),'words\\\\n')\\n\", '        # also save all the words to a list\\n', '        for this_word in words_in_file:\\n', '            all_words_from_all_docs.append(this_word)\\n', '            ']\n",
      "code ['len(all_documents)']\n",
      "code ['all_words_from_all_docs = list(set(all_words_from_all_docs))\\n', 'len(all_words_from_all_docs)']\n",
      "markdown ['Sample sizes are small, so results are not reliable representations of the document']\n",
      "code ['for doc_name, word_list_in_this_doc in all_documents.items():\\n', '    if (len(word_list_in_this_doc)==0):\\n', '        print(\"error: empty input file\"+doc_name)\\n', '    else:\\n', '        dic_of_terms={}\\n', '        for this_term in word_list_in_this_doc:\\n', '            dic_of_terms[this_term] = tfidf(this_term, word_list_in_this_doc, all_words_from_all_docs)\\n', '        #print(dic_of_terms)\\n', \"        print('\\\\n'+doc_name)\\n\", '        terms_in_doc_sorted_by_score=sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\\n', '        # first 40 words by importance\\n', '        for this_tup in terms_in_doc_sorted_by_score[0:40]:\\n', '            print(this_tup)\\n', '#        for indx in range(10):\\n', '#            print(terms_in_doc_sorted_by_score[indx][0] + \":\"+str(terms_in_doc_sorted_by_score[indx][1]))']\n",
      "code []\n",
      "markdown ['# Project - 2 : DOHMH_HIV_AIDS_Annual_Report']\n",
      "markdown ['The data which is under study is the HIV-AIDS-ANNUAL-REPORT.\\n', '\\n', 'Data source discovered from : https://catalog.data.gov/dataset\\n', '\\n', 'Data source location : https://catalog.data.gov/dataset/dohmh-hiv-aids-annual-report\\n', '\\n', 'I choose this data as I was concerned about the effects of HIV increasing on an everyday basis, with a rapid deterioration of health of those affected by it. Statistics show that more people die of HIV than either of alchohol or drug overdose.\\n', 'This data set is based on the death rates (includes both HIV and NON-HIV) in some of the boroughs in New York from 2011 to 2015.\\n', '\\n', 'Hypothesis:\\n', '-----------\\n', '**Hypothesis:1** There has been a decline in the number of HIV related deaths from 2011 to 2015 because of a four-part strategy began in New York post 2012 called Ending the Epidemic (EtE) , result of which is the aforementioned point.The Non-HIV related death-rate was constant but HIV-realted deaths declined thus resulting in a reduction in the overall death rates.\\n', '     \\n', '**Hypothesis:2** Bronx and brookyln are the two boroughs in New York city as these two boroughs have less awareness about the disease and they are populated mostly with the non-awareness communities and unrecorded death cases which is why these two boroughs have more HIV related death rates.\\n', '\\n', '**Hypothesis:3** One in every seven African-American with HIV are unaware they have it. They are the most affected race among the all the races in New York city. Some African-American communities have higher rates of a few sexually transmitted diseases (STDs) than alternative racial/ethnic communities.\\n', '\\n', '\\n', 'The data which i have extracted is free of cost and is legal to use.\\n', 'Documented EDA isnt available for the data which im using.\\n', 'This data has 18 columns and 6006 rows.\\n', '\\n', '**Primary Observation:** HIV related death rates are because of the people with less awareness about the disease and they are decreasing gradually from the year 2011.\\n', '\\n', '   **Conclusion for Hypothesis 1** From the analysis the results supports the Hypothesis 1     \\n', '   **Conclusion for Hypothesis 2** From the analysis the results supports the Hypothesis 2  \\n', '   **Conclusion for Hypothesis 3** From the analysis the results contradicts the Hypothesis 3']\n",
      "code ['#Importing Pandas, numpy, chardet, ascii, string, sys, re, csv,pyplot\\n', '\\n', 'import pandas as pd\\n', 'import numpy as np\\n', 'import chardet\\n', '#import ascii\\n', 'import string\\n', 'import sys\\n', 'import re\\n', 'import csv\\n', 'import os\\n', 'import matplotlib.pyplot as plt\\n', 'from pandas.plotting import lag_plot\\n', 'import matplotlib.patches as mpatches\\n', 'from matplotlib import rc\\n', '\\n', '#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n', '#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html']\n",
      "markdown ['pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. \\n', 'NumPy is the most basic yet a powerful package for scientific computing and data manipulation in Python. It is an open source library available in Python. It helps as to do the mathematical and scientific operation and used extensively in data science.\\n', 'It provides high-performance, easy to use structures and data analysis tools. Unlike NumPy library which provides objects for multi-dimensional arrays, Pandas provides in-memory 2d table object called Dataframe\\n', 'Chardet is a Python port, universal character encoding detector.\\n', 'The os module is a part of the standard library, or stdlib, within Python 3. This means that it comes with your Python installation, but you still must import it.\\n', '\\n', 'Citation:https://docs.python.org/3/library/']\n",
      "code ['#Loading the data into pandas data frame through URL\\n', '\\n', '#https://catalog.data.gov/dataset/dohmh-hiv-aids-annual-report --> CSV File used\\n', '\\n', 'path_to_file = \"https://data.cityofnewyork.us/api/views/fju2-rdad/rows.csv?accessType=DOWNLOAD\"\\n', \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\", '\\n', '#citation:https://www.geeksforgeeks.org/different-ways-to-import-csv-file-in-pandas/']\n",
      "code ['#viewing the complete data present in the data frame\\n', '\\n', 'data']\n",
      "code ['#to view the data type of each column\\n', 'data.dtypes']\n",
      "code ['#converting the Year column to date time format\\n', '\\n', \"data.Year = pd.to_datetime(data.Year, format='%Y')\"]\n",
      "code ['##to view the data type of each column after conversion\\n', '\\n', 'data.dtypes']\n",
      "code ['#Finding the rows and columns of the dataset\\n', 'data.shape']\n",
      "code ['#checking for null values\\n', '\\n', 'data.isnull()']\n",
      "code ['#checking for any null values present in the data set\\n', '\\n', 'data.isnull().sum()']\n",
      "code ['#summary of the dataframe\\n', 'data.info()']\n",
      "code ['#consistent data for data headings\\n', '\\n', 'data.head()']\n",
      "code ['data.tail()']\n",
      "code [\"#Data cleaning was required for HIV-related death rates and Non-HIV realted death rates, many values were entered as '99999.0'.\\n\", \"#hence replaced '99999.0' with '0'\\n\", 'data1=data.replace(to_replace =99999, \\n', '                 value =0) \\n', 'data1\\n', '\\n', '#citation: https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\\n', '#citation :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-replace/']\n",
      "code ['#to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values\\n', '\\n', 'data1.describe()']\n",
      "markdown ['**Analysis for Hypothesis 1:**\\n', '\\n', 'There has been a decline in the number of HIV related deaths from 2011 to 2015 because of a four-part strategy began in New York post 2012 called Ending the Epidemic (EtE) , result of which is the aforementioned point.The Non-HIV related death-rate was constant but HIV-realted deaths declined thus resulting in a reduction in the overall death rates.']\n",
      "code ['##total count of the Years\\n', '\\n', \"dfYear= pd.value_counts(data1['Year'].values)\\n\", 'dfYear']\n",
      "markdown ['*segregation of data:*\\n', '\\n', \"sum1 represents the sum of 'HIV related death rates' for the respective years.\\n\", '\\n', \"sum2 represents the sum of 'Non-HIV related death rates' for the respective years.\\n\", '\\n', \"sum3 represents the sum of 'death rates' for the respective years\"]\n",
      "code ['sum1=data1.groupby(\\'Year\\', as_index=False).agg({\"HIV-related death rate\": \"sum\"})\\n', 'sum1\\n', '\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['sum2=data1.groupby(\\'Year\\', as_index=False).agg({\"Non-HIV-related death rate\": \"sum\"})\\n', 'sum2\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['\\n', 'sum3=data1.groupby(\\'Year\\', as_index=False).agg({\"Death rate\": \"sum\"})\\n', 'sum3\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['#checking if the data got aligned properly.\\n', '\\n', 'df=[sum1,sum2,sum3]\\n', '#df\\n', 'result = pd.concat(df)\\n', 'result\\n', '\\n', '#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\\n', '#citation : https://datacarpentry.org/python-ecology-lesson/05-merging-data/']\n",
      "code ['#merging two data frames using inner join and assigning it to df1\\n', '\\n', \"df1 = pd.merge(sum1, sum2, on='Year',how='inner')\\n\", 'df1\\n', '\\n', '#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns']\n",
      "code ['#merging two data frames (df1 and sum3) using inner join and assigning it to data2\\n', '\\n', \"data2 = pd.merge(df1, sum3, on='Year',how='inner')\\n\", 'data2\\n', '\\n', '#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns']\n",
      "markdown ['Contacted the data source owner for the null values for hiv and non-hiv related deaths for the year 2015']\n",
      "markdown ['**DATA VISUALIZATIONS** for Hypothesis 1']\n",
      "code ['#plotting a grouped bar graph to check the death rates and comparing the HIV and Non-Hiv related death rates.\\n', '\\n', 'n_groups = 5\\n', 'fig, ax = plt.subplots(figsize=(13,5))\\n', 'bar_width = 0.20\\n', 'opacity = 0.8\\n', 'index = np.arange(n_groups)\\n', '\\n', \"rects1 = plt.bar(index - bar_width, data2['HIV-related death rate'], bar_width,alpha=opacity,color='b',label='HIV-related death rate')\\n\", \"rects2 = plt.bar(index, data2['Non-HIV-related death rate'], bar_width,alpha=opacity,color='g',label='Non-HIV-related death rate')\\n\", \"rects3 = plt.bar(index + bar_width, data2['Death rate'], bar_width,alpha=opacity,color='r',label='Death rate')\\n\", \"plt.xlabel('Years', fontsize=18)\\n\", \"plt.ylabel('sum of death rates', fontsize=18)\\n\", \"plt.title('Deaths over the years (2011 - 2015)',fontsize=23)\\n\", \"plt.xticks(index + bar_width, ('2011', '2012', '2013', '2014', '2015'))\\n\", 'plt.legend()\\n', 'plt.tight_layout()\\n', 'plt.show()\\n', '\\n', '#citation: https://pythonspot.com/matplotlib-bar-chart/\\n', '#citation: https://stackoverflow.com/questions/47838680/matplotlib-xticks-values-in-bar-chart\\n', '#citation: https://stackoverflow.com/questions/51080491/matplotlib-how-to-change-figsize-for-double-bar-plot/51081443\\n', '#citation:https://stackoverflow.com/questions/14270391/python-matplotlib-multiple-bars']\n",
      "markdown ['From the plot we can observe that the HIV related death rates are slightly reducing over the period of these listed years with HIV related death rates for the years 2011 having 6440.7 , 2012 having 5322.9, 2013 having 4670.5, 2014 having 3893.3, this clearly shows us the decrease of HIV related death rates which resulted in over all death rates. The gradual change in the HIV related deaths are clearly seen and the Non-HIV related deaths are almost constant. So this decrease in the HIV-related death rates resulted in decrease of over all death rates.\\n', '\\n', '**Contacted the data source owner for the null values for hiv and non-hiv related deaths for the year 2015**']\n",
      "code ['#checking if the HIV death rates are decreasing with respect to the Non-HIV death rates\\n', '#and checking if the total death rates are getting affected.\\n', '\\n', 'fig, ax = plt.subplots(figsize=(10,7))\\n', \"plt.plot('Year' ,'HIV-related death rate', data=data2, marker='', markerfacecolor='blue', color='skyblue', linewidth=4)\\n\", \"plt.plot( 'Year','Non-HIV-related death rate',  data=data2, marker='', color='red', linewidth=3)\\n\", 'plt.plot(\\'Year\\', \\'Death rate\\', data=data2, marker=\\'\\', color=\\'olive\\', linewidth=3, label=\"Death rate\")\\n', \"plt.xlabel('years', fontsize=18)\\n\", \"plt.ylabel('sum of death rates ', fontsize=18)\\n\", \"plt.title('Increase or Decrease of deaths ', fontsize=23)\\n\", 'plt.legend()\\n', '\\n', '#citation: https://python-graph-gallery.com/line-chart/\\n', '#citation: http://www.learningaboutelectronics.com/Articles/How-to-change-the-line-width-of-a-graph-plot-in-matplotlib-with-Python.php']\n",
      "markdown ['From the line graph we can observe that the HIV related death rates are slightly reducing which also directly reduced the overall death rates, and the Non-HIV related death rates are almost constant. So this decrease in the HIV-related death rates resulted in decrease of over all death rates. are slightly reducing over the period of these listed years with HIV related death rates for the years 2011 having 6440.7 , 2012 having 5322.9, 2013 having 4670.5, 2014 having 3893.3 and the over all deaths for the years 2011 having 15465.2, 2012 having 13345.5, 2013 having 12430.2, 2014 having 11462.3 and 2015 having 9398.3 of over all death rates.\\n', '\\n', '**Hypothesis 1 holds true, there is a gradual decrease in the HIV realted death rates over the years 2011 to 2015.**']\n",
      "markdown ['**Analysis for Hypothesis 2:**\\n', '\\n', 'Bronx and brookyln are the two boroughs in New York city as these two boroughs have less awareness about the disease and they are populated mostly with the non-awareness communities and unrecorded death cases which is why these two boroughs have more HIV related death rates.']\n",
      "code [\"#flitering the data1 dataframe with Borough == 'Bronx' and assigning to data_fBB\\n\", '\\n', \"data_fBB = data1[data1.Borough == 'Bronx']\\n\", 'data_fBB\\n']\n",
      "code [\"#flitering the data1 dataframe with Borough == 'Brooklyn' and assigning to data_fBR\\n\", '\\n', \"data_fBR = data1[data1.Borough == 'Brooklyn']\\n\", 'data_fBR']\n",
      "code [\"#flitering the data1 dataframe with Borough == 'Manhattan' and assigning to data_fBM\\n\", '\\n', \"data_fBM = data1[data1.Borough == 'Manhattan']\\n\", 'data_fBM']\n",
      "code [\"#flitering the data1 dataframe with Borough == 'Queens' and assigning to data_fBQ\\n\", '\\n', \"data_fBQ = data1[data1.Borough == 'Queens']\\n\", 'data_fBQ']\n",
      "code [\"#flitering the data1 dataframe with Borough == 'Staten Island' and assigning to data_fBS\\n\", \"data_fBS = data1[data1.Borough == 'Staten Island']\\n\", 'data_fBS\\n']\n",
      "code [\"#flitering the data1 dataframe with Borough == 'All' and assigning to data_fBA\\n\", \"# All Boroughs are the Boroughs for which the names aren't mentioned in the list\\n\", '\\n', \"data_fBA = data1[data1.Borough == 'All']\\n\", 'data_fBA']\n",
      "code ['#total count of the Boroughs\\n', \"dfBorough= pd.value_counts(data1['Borough'].values)\\n\", 'dfBorough']\n",
      "markdown ['*segregation of data:*\\n', '\\n', \"sumB represents the sum of 'death rates' for the respective Boroughs.\\n\", '\\n', \"sumNH represents the sum of 'Non-HIV related death rates' for the respective Boroughs.\\n\", '\\n', \"sumH represents the sum of 'HIV related death rates' for the respective Boroughs.\"]\n",
      "code ['\\n', 'sumB=data1.groupby(\\'Borough\\', as_index=False).agg({\"Death rate\": \"sum\"})\\n', 'sumB\\n', '\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['sumNH=data1.groupby(\\'Borough\\', as_index=False).agg({\"Non-HIV-related death rate\": \"sum\"})\\n', 'sumNH\\n', '\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['sumH=data1.groupby(\\'Borough\\', as_index=False).agg({\"HIV-related death rate\": \"sum\"})\\n', 'sumH\\n', '\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['#merging two data frames (sumH and sumNH) using inner join and assigning it to df2\\n', '#merging two data frames (df2 and sumB) using inner join and assigning it to data3\\n', '\\n', \"df2 = pd.merge(sumH, sumNH, on='Borough',how='inner')\\n\", \"data3 = pd.merge(df2, sumB, on='Borough',how='inner')\\n\", 'data3\\n', '\\n', '\\n', '#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns']\n",
      "markdown ['**Data Visualisations for Hypothesis 2**']\n",
      "code ['#plotting a stacked graph with the dataframe(data3)\\n', '#checking which all Boroughs have more HIV death rates when compared with the non HIV death rates and the effect of actual death rates.\\n', 'colors = [\"blue\", \"yellow\",\"red\"]\\n', \"x=['All','Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\", \"#names = ['All','Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\", \"#xticks(np.arange(5), ('All','Bronx','Brookyln','Manhattan','Queens','Staten Island'))\\n\", \"rect=data3.loc[:,['HIV-related death rate','Non-HIV-related death rate','Death rate']].plot.bar(stacked=True, color=colors ,figsize=(10,7), fontsize=13)\\n\", \"plt.xticks(range(6),['All','Bronx','Brookyln','Manhattan','Queens','Staten Island'])\\n\", \"plt.xlabel('Boroughs',fontsize=18) \\n\", \"plt.ylabel('sum of death rates', fontsize=18) \\n\", \"plt.title('Deaths with respect to boroughs', fontsize=23)\\n\", '\\n', '#citation: https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/bar_stacked.html\\n', '#citation: https://www.smartsheet.com/stacked-bar-chart-graph\\n', '#citation: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xticks.html']\n",
      "markdown [\"By observing the plot we can say that the people in Brookyln Borough with HIV-death rate of 7626.6 in New York are more prone towards HIV death Rates than the other Broughs'All','Bronx','Manhattan','Queens','Staten Island' with 466.1, 3877.9, 3885.0, 3701.2, 3416.1 respectively, the Non-HIV related deaths are almost the same for 'Bronx','Brookyln','Manhattan','Queens','Staten Island' except for All(which aren't revealed) , so people in Brookyln are more prone towards HIV than the reamining listed boroughs.Which eventually resulted in the increase of the death rates as well.\\n\", '\\n', '\\n', '**Hypothesis 2 holds true, but as per the hypothesis Bronx and Brookyln bothe are having more HIV-Related death rates, but when compared Brookyln is having higher HIV realted death rates than Bronx.**']\n",
      "markdown ['**Data Analysis for Hypothesis 3:**\\n', '\\n', 'One in every seven African-American with HIV are unaware they have it. They are the most affected race among the all the races in New York city. Some African-American communities have higher rates of a few sexually transmitted diseases (STDs) than alternative racial/ethnic communities.']\n",
      "code ['#total count of the Races\\n', '\\n', \"dfRace= pd.value_counts(data1['Race'].values)\\n\", 'dfRace']\n",
      "markdown ['*segregation of data:*\\n', '\\n', \"sumAll represents the sum of 'death rates' for the respective Race.\\n\", '\\n', \"sumHiv represents the sum of 'HIV related death rates' for the respective Race.\\n\", '\\n', \"sumN_Hiv represents the sum of 'Non-HIV related death rates' for the respective Race.\"]\n",
      "code ['sumAll=data1.groupby(\\'Race\\', as_index=False).agg({\"Death rate\": \"sum\"})\\n', 'sumAll\\n', '\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['sumHiv=data1.groupby(\\'Race\\', as_index=False).agg({\"HIV-related death rate\": \"sum\"})\\n', 'sumHiv\\n', '\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['sumN_Hiv=data1.groupby(\\'Race\\', as_index=False).agg({\"Non-HIV-related death rate\": \"sum\"})\\n', 'sumN_Hiv\\n', '\\n', '#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n', '#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n', '#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n', '#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/']\n",
      "code ['#merging two data frames (sumAll and sumHiv) using inner join and assigning it to df_Race\\n', '#merging two data frames (df_Race and sumN_Hiv) using inner join and assigning it to data_Race\\n', '\\n', \"df_Race = pd.merge(sumAll, sumHiv, on='Race',how='inner')\\n\", \"data_Race = pd.merge(df_Race, sumN_Hiv, on='Race',how='inner')\\n\", 'data_Race\\n', '\\n', '#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns']\n",
      "code [\"#dropping the row 'All' because it is the sum of the death rates of all the races.\\n\", '\\n', 'data_race_1=data_Race.drop([data_Race.index[0]])\\n', 'data_race_1\\n', '\\n', '#citation:https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/']\n",
      "code ['#plotting a stacked graph with the dataframe(data_race_1)\\n', '#checking which all Races have more HIV death rates when compared with the non HIV death rates and the effect of actual death rates.\\n', '\\n', 'colors = [\"black\",\"grey\", \"#b2beb5\"]\\n', \"#x=['Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\", \"#plt.xlabel('x') \\n\", \"#names = ['All','Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\", \"#xticks(np.arange(5), ('All','Bronx','Brookyln','Manhattan','Queens','Staten Island'))\\n\", \"rect=data_race_1.loc[:,['HIV-related death rate','Non-HIV-related death rate','Death rate']].plot.bar(stacked=True, color=colors, label = x ,figsize=(10,10),fontsize=18)\\n\", \"plt.xticks(range(5),['Asian/Pacific Islander','Black','Latino/Hispanic','Other/Unknown','White'],fontsize=13)\\n\", \"plt.xlabel('Races', fontsize=18) \\n\", \"plt.ylabel('sum of death rates', fontsize=18) \\n\", \"plt.title('Deaths with respect to race', fontsize=23)\\n\", '\\n', '#citation: https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/bar_stacked.html\\n', '#citation: https://www.smartsheet.com/stacked-bar-chart-graph\\n', '#citation: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xticks.html']\n",
      "markdown [\"By observing the plot we can say that Latin/Hispanic Race communities are more prone towards HIV death Rates for about 1924.8 as the over all HIV-death rate and the Non-HIV related deaths are almost the same for Asian/Pacific Islander,Black,Latino/Hispanic, white which are of death rates which are 1952.9, 2652.5, 2944.8, 2850.5 respectively except for other/unknown(which aren't revealed) which was 762.9 , so Latin/Hispanic Race communities with HIV-death rates as 1924.8 are slightly more prone towards HIV than Blacks with HIV-death rates as 1812.4 and also other communities of races. There is a slight difference between blacks and Latin/Hispanic Race communities.\\n\", '\\n', '**Hypothesis 3 holds false, according to my hypothesis African-Americans (Blacks) have more chances of getting HIV and having more HIV-related death rates due to unawareness, but after analysing the data Latino/Hispanic is having slightly more HIV death rates when compared to African-Americans (Blacks).**']\n",
      "markdown ['***Conclusion:***\\n', '\\n', '**After the analysis of this dataset, we can conclude that the hypothesis holds true for two hypothesis and contradicts for one out of three hypothesis, as the HIV related death rates are getting reduced which is directly effecting the over all death rates which holds true as per the hypothesis, HIV related deaths are more frequent in Brookyln also holds true as per the hypothesis and the community of people which are more HIV prone are Latino/Hispanic but the third hypothesis holds false as it was stated as African-Americans(Blacks) are more prone to HIV .**']\n",
      "markdown ['__*Learned:*__\\n', '  \\n', '  -- Working with different Visualizations.\\n', '  \\n', '  -- Working with different data frames and merging the dataframes.\\n', '  \\n', '  -- Data analysis.\\n', '  \\n', '  -- working with pandas and many more different libraries in python.']\n",
      "code []\n",
      "code ['import pandas\\n', \"print('pandas',pandas.__version__)\"]\n",
      "code ['df = pandas.read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/mlb_players.csv\", \\n', '                     skiprows=[1035],\\n', '                     skipinitialspace=True)\\n', 'df.head()']\n",
      "markdown [\"Here's a function that we may use often to explore a dataframe:\"]\n",
      "code ['for this_column in df.columns:\\n', '    print(\"==== \",this_column,\"has\",df[this_column].nunique(),\"unique entries ====\")\\n', '    print(df[this_column].value_counts().head(10))']\n",
      "markdown ['Rather than copy-pasting it from notebook to notebook, place the code inside a function in a .py file.\\n', '\\n', 'To show you the contents of the .py file, I\\'ll use \"cat\" command:']\n",
      "code ['!cat myfunctions.py']\n",
      "markdown ['load this function using `%run` cell magic\\n', '\\n', 'https://ipython.readthedocs.io/en/stable/interactive/magics.html']\n",
      "code ['%run myfunctions.py']\n",
      "markdown ['To use the function, I need to know the name of the function and the arguments']\n",
      "code ['unique_entries_in_frame(df,5)']\n",
      "markdown ['Alternatively, we can use `%load` to show the file content\\n', '\\n', 'https://stackoverflow.com/questions/21034373/how-to-load-edit-run-save-text-files-py-into-an-ipython-notebook-cell']\n",
      "code ['%load myfunctions.py']\n",
      "markdown [\"The advantage of `%load` is that the source code is part of the notebook, so we don't need to store the .py source with the .ipynb notebook.\\n\", '\\n', 'That is also a disadvantage: if we update the .py source, the change does not impact the code used by the notebook.']\n",
      "code []\n",
      "markdown ['# week 7: visualizing time series data']\n",
      "markdown ['\\n', 'Acquire power data from this source for at least 10 days and not more than 40 days. The website limits the window size, so you will need to download multiple files. Load the data into a Jupyter Notebook. Combine the multiple input files to create a single dataframe.\\n', '\\n', 'Create two bar graphs of the power consumption per hour.\\n', '\\n', 'One bar graph has 24 bars; each bar is the average across all days for that hour\\u200b\\n', '\\n', '\\n', 'one bar graph has 24*(number of days) bars. Your choice of average or sum for each hour. Label the y-axis appropriately.']\n",
      "code ['#Importing Pandas, numpy, chardet, string, sys, re, csv,pyplot\\n', 'import pandas as pd\\n', 'import numpy as np\\n', 'import chardet\\n', 'import string\\n', 'import sys\\n', 'import re\\n', 'import csv\\n', 'import os\\n', 'import matplotlib.pyplot as plt\\n', 'import pandas\\n', 'import sqlite3\\n', \"print('pandas',pandas.__version__)\\n\", '#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n', '#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html']\n",
      "code ['#Loading the data into pandas data frame\\n', '#First 7 days of data extracted and saved in RollingSystemDemand_20191015_1556.csv \\n', '\\n', 'path_to_file = \"RollingSystemDemand_20191015_1556.csv\"\\n', \"data1 = pd.read_csv(path_to_file, encoding='utf-8')\\n\", \"#loading the csv file to a variable 'data1'\\n\", '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/']\n",
      "code ['#viewing the complete data present in the data frame\\n', '\\n', 'data1\\n', '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\\n']\n",
      "code ['#dropping the last row with (FTR\\t2000\\tNaN) and storing the newly created dataset into df1\\n', '\\n', 'df1=data1.drop(data1.index[2000])\\n', '\\n', '#citation:https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/']\n",
      "code ['df1']\n",
      "code ['#Loading the data into pandas data frame\\n', '#next 7 days of data extracted and saved in RollingSystemDemand_20191015_1556 (1).csv\\n', '\\n', '\\n', 'path_to_file = \"RollingSystemDemand_20191015_1556 (1).csv\"\\n', \"data2 = pd.read_csv(path_to_file, encoding='utf-8')\\n\", '\\n', \"#loading the csv file to a variable 'data2'\\n\", '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/']\n",
      "code ['data2']\n",
      "code ['#dropping the last row with (FTR\\t2000\\tNaN) and storing the newly created dataset into df2\\n', '\\n', '\\n', 'df2=data2.drop(data2.index[2000])\\n', '\\n', '#citation:https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/']\n",
      "code ['df2']\n",
      "code ['#concatinating two dataframes\\n', '\\n', 'result = [df1, df2]\\n', '\\n', 'data = pd.concat(result)\\n', '\\n', '#citation:https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html']\n",
      "code ['data']\n",
      "code ['#checking for the data types\\n', 'data.dtypes\\n', '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/']\n",
      "code ['#changing the column names\\n', '\\n', \"data.columns=['time_and_date','demand_level']\\n\", '\\n', '#https://cmdlinetips.com/2018/03/how-to-change-column-names-and-row-indexes-in-pandas/']\n",
      "code ['data.head()']\n",
      "code ['#importing the datetime libraries\\n', '\\n', 'from datetime import datetime\\n', '\\n', '#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime']\n",
      "code ['#converting the time of measurement to datetime format\\n', '\\n', 'data[\\'time_and_date\\']=pd.to_datetime(data[\\'time_and_date\\'],format=\\'%Y%m%d%H%M%S\\',errors=\"coerce\")\\n', '\\n', '#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime']\n",
      "code ['data.shape']\n",
      "code ['data']\n",
      "code ['#maximum of the datetime\\n', '\\n', \"data['time_and_date'].max()\\n\", '\\n', '#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime']\n",
      "code ['#minimum of the datetime\\n', '\\n', \"data['time_and_date'].min()\\n\", '\\n', '#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime']\n",
      "code ['#checking if the datatype got changed to datetime for \"time of measurement\".\\n', '\\n', 'data.dtypes\\n', '\\n', '#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/']\n",
      "code ['#plotting bar graph has 24 bars, each bar is the average across all days for that hour\\n', '\\n', 'data.groupby(data[\"time_and_date\"].dt.hour)[\\'demand_level\\'].mean().plot(kind=\"bar\")\\n', '\\n', 'plt.setp(plt.gca().get_xticklabels(), rotation=40, fontsize=12)\\n', 'plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n', \"plt.xlabel('hours', fontsize=10)\\n\", \"plt.ylabel('average for that hour for all taken days', fontsize=10)\\n\", \"plt.title('power consumption per that particular hour for all days')\\n\", '\\n', 'plt.show()\\n', '\\n', '#citation:https://python-graph-gallery.com/10-barplot-with-number-of-observation/\\n', '#citation:https://chrisalbon.com/python/data_wrangling/group_pandas_data_by_hour_of_the_day/']\n",
      "code ['#plotting bar graph has 24*(number of days) bars. i.e., 24*(14 days) = 336 bars\\n', '\\n', 'data.groupby([data[\"time_and_date\"].dt.day,data[\"time_and_date\"].dt.hour])[\\'demand_level\\'].mean().plot(kind=\"bar\",figsize=(100,20))\\n', '#fig = plt.figure(figsize=(30,70))\\n', \"#plt.xlabel('hour', fontsize=14)\\n\", \"#plt.ylabel('count', fontsize=14)\\n\", '\\n', \"plt.xlabel('days', fontsize=100)\\n\", \"plt.ylabel('hour average', fontsize=100)\\n\", \"plt.title('electricity consumption per that particular hour for each day',fontsize=100)\\n\", '\\n', 'plt.show()\\n', '\\n', '#citation:https://python-graph-gallery.com/10-barplot-with-number-of-observation/\\n', '#citation:https://chrisalbon.com/python/data_wrangling/group_pandas_data_by_hour_of_the_day/']\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code []\n",
      "code ['import pandas\\n', \"print('pandas',pandas.__version__)\"]\n",
      "markdown ['Query interface on the web: \\n', '\\n', 'https://www.bmreports.com/bmrs/?q=demand/rollingsystemdemand\\n', '\\n', 'Preview the CSV']\n",
      "code ['!head RollingSystemDemand_20190314_0043.csv']\n",
      "markdown ['load CSV into Pandas using the default options']\n",
      "code ['dframe = pandas.read_csv(\"RollingSystemDemand_20190314_0043.csv\")\\n', 'dframe.head()']\n",
      "markdown ['I don\\'t like that Pandas is treating \"VD\" as the index, so I\\'m going to force the index to be numeric']\n",
      "code [\"#dataframe.reset_index() # not sure why this didn't work\\n\", 'dframe.index=range(len(dframe))\\n', 'dframe.head()']\n",
      "markdown [\"I don't like the column names, so I'm going to change those\"]\n",
      "code [\"dframe.columns=['time of measurement','demand level']\"]\n",
      "code ['dframe.head()']\n",
      "markdown [\"I'm happy. Let's see what the plot looks like\"]\n",
      "code ['import matplotlib\\n', \"print('matplotlib',matplotlib.__version__)\\n\", 'import matplotlib.pyplot as plt']\n",
      "code [\"dframe.plot(x='time of measurement', y='demand level')\\n\", 'plt.show()']\n",
      "markdown ['Hmm, those flat lines look unlikely. Also, the time axis needs some work. Switch to scatter plot']\n",
      "code [\"plt.scatter(dframe['time of measurement'], dframe['demand level'])\\n\", 'plt.show()']\n",
      "markdown ['Switching to scatter plot confirms the lines are not actual data points. \\n', '\\n', \"Also, need to convert 'time of measurement' column to datetime\"]\n",
      "code ['from datetime import datetime']\n",
      "markdown [\"Before trying to force the column to a datetime, let's experiment with a single entry to confirm the method.\\n\", '\\n', 'strptime = \"string parse time\"<BR>\\n', 'strftime = \"string format time\"<BR>\\n', '    see <a href=\"http://strftime.org/\">http://strftime.org/</a>']\n",
      "code ['!head -n 3 RollingSystemDemand_20190314_0043.csv']\n",
      "code ['# see https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior for details\\n', '\\n', '# using the first entry from the output of !head above \\n', \"datetime_object = datetime.strptime('20190301000000', '%Y%m%d%H%M%S')\\n\", \"datetime_object.strftime('%Y-%m-%d %H:%M:%S') # print the result in a human readable format\"]\n",
      "markdown ['Conclusion: looks like the method works\\n', '\\n', 'Next, convert the entire column using that format']\n",
      "code [\"dframe['time of measurement'] = pandas.to_datetime(dframe['time of measurement'],\\n\", \"                                                   format='%Y%m%d%H%M%S')\"]\n",
      "markdown ['I was making an assumption that the data type was string\\n', '\\n', 'Look at the type of the columns:']\n",
      "code ['dframe.dtypes']\n",
      "markdown ['Before converting the time column to datetime, first convert to string']\n",
      "code [\"dframe['time of measurement']=dframe['time of measurement'].to_string()\"]\n",
      "code [\"dframe['time of measurement'] = pandas.to_datetime(dframe['time of measurement'],\\n\", \"                                                   format='%Y%m%d%H%M%S')\"]\n",
      "markdown ['Hmm, something went wrong...']\n",
      "code ['dframe.head()']\n",
      "code []\n",
      "code ['import pandas\\n', \"print('pandas',pandas.__version__)\"]\n",
      "code ['!head RollingSystemDemand_20180901_0129.csv']\n",
      "code ['dframe = pandas.read_csv(\"RollingSystemDemand_20180901_0129.csv\")']\n",
      "code ['dframe.head()']\n",
      "code ['dframe.dtypes']\n",
      "markdown ['I don\\'t like that Pandas is treating \"VD\" as the index, so I\\'m going to force the index to be numeric']\n",
      "code ['dframe.index=range(len(dframe))\\n', 'dframe.head()']\n",
      "markdown [\"I don't like the column names, so I'm going to change those\"]\n",
      "code ['dframe.dtypes']\n",
      "code [\"dframe.columns=['time of measurement','demand level']\"]\n",
      "code ['dframe.head()']\n",
      "markdown ['Up to this point, we have merely reconstructed the previous notebook.\\n', '\\n', \"Now let's inspect the time column again\"]\n",
      "code [\"dframe['time of measurement'][0:5]\"]\n",
      "markdown [\"Let's see what happens when we try to convert just those 5 rows to integers.\"]\n",
      "code [\"dframe['time of measurement'][0:5].to_string()\"]\n",
      "code []\n",
      "code ['import pandas\\n', \"print('pandas',pandas.__version__)\"]\n",
      "code ['!head RollingSystemDemand_20180901_0129.csv']\n",
      "markdown ['The \"VD\" entry is being used as an index because the header has two columns\\n', '\\n', 'Tell Pandas to not use the first column as the index']\n",
      "code ['dframe = pandas.read_csv(\"RollingSystemDemand_20180901_0129.csv\",\\n', '                         index_col=False)\\n', 'dframe.head()']\n",
      "markdown [\"Hmm, that's not quite what I intended.\\n\", '\\n', \"Rather than Pandas trying to figure out what's going on, tell Pandas to skip the first row\\n\", '\\n', '`skiprows` : Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.']\n",
      "code ['dframe = pandas.read_csv(\"RollingSystemDemand_20180901_0129.csv\",\\n', '                         index_col=False,\\n', '                         skiprows=1)\\n', 'dframe.head()']\n",
      "markdown [\"And now Pandas assumes the first row is the header. Let's disable that\"]\n",
      "code ['dframe = pandas.read_csv(\"RollingSystemDemand_20180901_0129.csv\",\\n', '                         index_col=False,\\n', '                         skiprows=1, \\n', '                         header=None)\\n', 'dframe.head()']\n",
      "markdown ['Now we can set the column labels']\n",
      "code [\"dframe.columns=['VD','time of measurement','value']\\n\", 'dframe.head()']\n",
      "markdown [\"Let's check on the status of the data types in each column\"]\n",
      "code ['dframe.dtypes']\n",
      "markdown ['_Lesson_: abstraction frameworks are convenient when the assumptions they make are correct. \\n', '\\n', '(Different confusion might have arisen if we had chosen to simply read in the file manually. Here we are using read_csv.)\\n', '\\n', 'Change the type from \"int\" to string so that we can then convert to datetime']\n",
      "code ['# https://stackoverflow.com/questions/17950374/converting-a-column-within-pandas-dataframe-from-int-to-string\\n', \"dframe['time of measurement']=dframe['time of measurement'].apply(str)\"]\n",
      "markdown ['Check the data type of the columns']\n",
      "code ['dframe.dtypes']\n",
      "code ['dframe.head()']\n",
      "markdown ['Now we can apply the conversion of the time column from string to datetime']\n",
      "code [\"pandas.to_datetime(dframe['time of measurement'],format='%Y%m%d%H%M%S')\"]\n",
      "markdown ['Still getting errors!\\n', '\\n', 'A similar issue was solved here:\\n', 'https://www.kaggle.com/najagumbi/data-cleaning-challenge-parsing-dates-v2']\n",
      "code ['pandas.to_datetime(dframe[\\'time of measurement\\'],format=\\'%Y%m%d%H%M%S\\',errors=\"coerce\")']\n",
      "markdown ['To confirm, inspect the bottom of the CSV']\n",
      "code ['!tail RollingSystemDemand_20180901_0129.csv']\n",
      "code []\n",
      "code []\n",
      "code ['import pandas\\n', \"print('pandas',pandas.__version__)\"]\n",
      "code ['!head RollingSystemDemand_20180901_0129.csv']\n",
      "markdown ['The \"VD\" entry is being used as an index because the header has two columns\\n', '\\n', 'Tell Pandas to not use the first column as the index\\n', '\\n', '`skipfooter`: Number of lines at bottom of file to skip (Unsupported with engine=’c’)']\n",
      "code ['dframe = pandas.read_csv(\"RollingSystemDemand_20180901_0129.csv\",\\n', '                         index_col=False,\\n', '                         skiprows=1,\\n', \"                         skipfooter=1, engine='python',\\n\", '                         header=None)\\n', 'dframe.head()']\n",
      "markdown ['Now we can set the column labels']\n",
      "code [\"dframe.columns=['VD','time of measurement','value']\\n\", 'dframe.head()']\n",
      "code ['dframe.tail()']\n",
      "markdown [\"Let's check on the status of the data types in each column\"]\n",
      "code ['dframe.dtypes']\n",
      "code [\"dframe['time of measurement']=pandas.to_datetime(dframe['time of measurement'].astype(str),format='%Y%m%d%H%M%S')\\n\", 'dframe.head()']\n",
      "code ['import matplotlib\\n', 'import matplotlib.pyplot as plt\\n', 'matplotlib.__version__']\n",
      "code [\"plt.plot_date(dframe['time of measurement'], dframe['value'])\\n\", 'plt.show()']\n",
      "markdown ['Almost good, but the labels overlap']\n",
      "code [\"plt.plot_date(dframe['time of measurement'], dframe['value'])\\n\", \"plt.xticks(rotation='vertical')\\n\", 'plt.tick_params(labelsize=14)\\n', \"plt.xlabel('date',fontsize=14)\\n\", \"plt.ylabel('demand [MegaWatts]',fontsize=14)\\n\", 'plt.show()']\n",
      "markdown ['Can we explain these peaks?']\n",
      "code []\n",
      "code ['import pandas\\n', \"print('pandas',pandas.__version__)\"]\n",
      "markdown ['Load the CSV we downloaded from \\n', '\\n', 'https://www.bmreports.com/bmrs/?q=demand/rollingsystemdemand\\n', '\\n', 'Include relevant modifiers on loading CSV']\n",
      "code ['dframe = pandas.read_csv(\"RollingSystemDemand_20190314_0043.csv\",\\n', '                         index_col=False,\\n', '                         skiprows=1,\\n', '                         skipfooter=1,\\n', \"                         engine='python',\\n\", '                         header=None)\\n', '\\n', \"dframe.columns=['VD','time of measurement','value']\\n\", '\\n', 'dframe.head()']\n",
      "markdown ['Convert time to useful format']\n",
      "code [\"dframe['time of measurement']=pandas.to_datetime(\\n\", \"                                  dframe['time of measurement'].astype(str),\\n\", \"                                  format='%Y%m%d%H%M%S')\"]\n",
      "markdown ['Plot the two columns as a scatter plot']\n",
      "code ['import matplotlib\\n', \"print('maplotlib',matplotlib.__version__)\\n\", 'import matplotlib.pyplot as plt']\n",
      "code [\"plt.plot_date(dframe['time of measurement'], dframe['value'])\\n\", \"plt.xticks(rotation='vertical')\\n\", 'plt.tick_params(labelsize=14)\\n', \"plt.xlabel('date',fontsize=14)\\n\", \"plt.ylabel('demand [MegaWatts]',fontsize=14)\\n\", 'plt.show()']\n",
      "code []\n",
      "markdown ['# week 10 - assignment - parse ipynb files\\n', '\\n', 'Submit a notebook that finds all other notebooks used for Data 601 on your computer and generates a list of the modules used\\u200b.\\n', '\\n', 'Look for .ipynb files using the glob module in Python\\n', 'Your analysis should include at least eight .ipynb files. \\n', 'Files used for code other than assignments (eg projects, work from other courses) is acceptable to include\\n', 'Create functions that parse the file and find modules.\\n', 'Use a loop to call the functions on each file.\\n', 'Look for code cells (ignore Markdown and raw cells)\\n', 'Look for lines of code that start with either \"from\" or \"import\"\\u200b\\n', 'Look for the keywords only in code input (rather than including cell output content)']\n",
      "code ['#Importing the required libraries\\n', '\\n', 'import glob\\n', 'import io, os, sys, types\\n', 'from IPython import get_ipython\\n', 'from nbformat import read\\n', 'from IPython.core.interactiveshell import InteractiveShell\\n', 'import json\\n', 'import re\\n', 'import pandas as pd']\n",
      "code ['#Reading all the .ipynb files and printing the cell type and the source present in the JSON file\\n', '#dict1={}\\n', \"for name in glob.glob('*.ipynb'):\\n\", '    with open (name, encoding=\"utf8\", errors=\\'coerce\\') as f:\\n', '        data = json.load(f)\\n', \"        for cell in data ['cells']:\\n\", \"                print(cell['cell_type'],cell['source'])\\n\", '#citation: https://stackoverflow.com/questions/2186525/how-to-use-glob-to-find-files-recursively\\n', '#citation: https://stackoverflow.com/questions/20186344/ipynb-import-another-ipynb-file\\n', '#citation: https://www.youtube.com/watch?v=7GFXm8HUD7s\\n', '#citation: https://pymotw.com/2/glob/\\n']\n",
      "code ['#printing the complete data\\n', '\\n', 'print(data)']\n",
      "code ['\\n', \"for cell in data ['cells']:\\n\", \"            dict1=cell['cell_type']=='code',cell['source']\\n\", '           ']\n",
      "code ['dict1']\n",
      "code ['type(data)']\n",
      "code ['#the cell_type with \"code\" are getting displayed\\n', 'lis1=[]\\n', \"for name in glob.glob('*.ipynb'):\\n\", '    with open (name, encoding=\"utf8\", errors=\\'coerce\\') as f: \\n', '        data = json.load(f)\\n', \"        for cell in data ['cells']:\\n\", \"            if cell['cell_type'] == 'code':\\n\", \"                lis1=json.dumps(cell['source'], indent=1)\\n\", '                print(lis1)\\n', '                \\n', '#citation : https://www.geeksforgeeks.org/working-with-json-data-in-python/\\n', '#citation: https://www.youtube.com/watch?v=Uh2ebFW8OYM']\n",
      "code ['type(lis1)']\n",
      "code ['#assigning import and from to imp and frm respectively\\n', 'imp = \"import\"\\n', 'frm = \"from\"\\n']\n",
      "code ['#printing the values with \"import and from\" and printing the remaining string values\\n', 'list1=[]\\n', \"for name in glob.glob('*.ipynb'):\\n\", '    with open (name, mode=\\'r\\',encoding=\"utf\", errors=\\'coerce\\') as f: \\n', '        data = json.load(f)\\n', \"        for cell in data ['cells']:\\n\", \"            if cell['cell_type'] == 'code':\\n\", \"                for i in cell['source']:\\n\", '                    #print(i)\\n', '                    if (imp in i):\\n', \"                        i = i.replace(imp, '')\\n\", \"                        i = i.replace(frm, '')\\n\", '                        list1.append(i)\\n', 'print(list1)\\n', '#citation : https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\\n', '#citation : https://stackoverflow.com/questions/5884517/python-assign-print-output-to-a-variable\\n', '#citation : https://stackoverflow.com/questions/12572362/get-a-string-after-a-specific-substring\\n', '#citation : https://www.programiz.com/python-programming/methods/list/append\\n', '#citation: https://towardsdatascience.com/python-basics-6-lists-and-list-manipulation-a56be62b1f95']\n",
      "code ['type(list1)']\n",
      "code [\"lis2= [x.replace('\\\\n', '') for x in list1] \\n\", '\\n', '#citation: https://stackoverflow.com/questions/3136689/find-and-replace-string-values-in-list']\n",
      "code ['lis2']\n",
      "code ['#converting lis2 into dataframe using pandas\\n', '\\n', 'data2 = pd.DataFrame(lis2)  \\n', '\\n', '#citation :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html']\n",
      "code ['#remaming the column\\n', '\\n', \"data2.columns = ['module']\"]\n",
      "code ['data2']\n",
      "code ['#using strip()\\n', '\\n', \"data2['module']=data2['module'].str.strip()\\n\", '\\n', '#citation : https://github.com/getpelican/pelican/issues/1317\\n', '#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.strip.html']\n",
      "code [\"#dropping the rows which contains '#' and assigning to 'ind'\\n\", \"ind = data2[data2['module'].str.startswith('#')]\\n\", '\\n', '#citation: https://howchoo.com/g/zdvmogrlngz/python-regexes-findall-search-and-match\\n', '#citation: https://stackoverflow.com/questions/54485186/remove-all-rows-with-a-special-character-in-pandas\\n', '#citation: https://stackoverflow.com/questions/52229998/remove-all-rows-that-meet-regex-condition/52230096']\n",
      "code [\"#displaying the index of 'ind'\\n\", 'ind.index']\n",
      "code [\"#the indexes with '#' are dropped.\\n\", '\\n', 'd=data2.drop(ind.index)\\n', 'd\\n', '#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\\n']\n",
      "code ['#unique values in the data frame are displayed.\\n', '\\n', 'd1= d.module.unique()\\n', 'd1\\n', '#citation: https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/']\n",
      "code []\n",
      "code []\n",
      "markdown ['Write a function that returns the count of characters and words in a string. Do not use the print() function to display the results. The string should provided to the function as an argument.']\n",
      "markdown ['1) function that returns the count of words']\n",
      "code ['# word count \\n', '\\n', 'OUT = 0\\n', 'IN = 1\\n', '\\n', '#function definition \\n', 'def countwords(string):\\n', '    state = OUT\\n', '    wc = 0\\n', '\\n', '    # Scan all characters one by one\\n', '    for i in range(len(string)):\\n', '        # If next character is a separator,set the state as OUT\\n', '        # set the state as OUT\\n', \"        if (string[i] == ' ' or string[i] == '\\\\n' or\\n\", \"                string[i] == '\\\\t'):\\n\", '            state = OUT\\n', '\\n', '        # If next character is not a word\\n', '        # separator and state is OUT, then\\n', '        # set the state as IN and increment\\n', '        # word count\\n', '        elif state == OUT:\\n', '            state = IN\\n', '            wc += 1\\n', '\\n', '    # Return the number of words\\n', '    return wc\\n', '\\n', 'string = input(\"enter a sentence --->  \")\\n', 'countwords(string)\\n', '\\n', '#citation: https://stackoverflow.com/questions/2932511/word-count-on-a-string']\n",
      "markdown ['2) function that returns the count of characters']\n",
      "code ['#Character count\\n', '\\n', '#accepting input from the user\\n', 'my_string=input(\"enter a word or a string ----> \");\\n', '\\n', '#function definition\\n', 'def char_count(my_string):\\n', '    #counting the letters and not ounting spaces if any\\n', '    count = len(my_string) - my_string.count(\" \")\\n', '    return(count)\\n', 'char_count(my_string)\\n', '\\n', '#citation: https://stackoverflow.com/questions/2932511/letter-count-on-a-string']\n",
      "code []\n",
      "markdown ['# install xlsxwriter module']\n",
      "markdown ['https://xlsxwriter.readthedocs.io/example_pandas_multiple.html\\n', '\\n', 'https://pbpython.com/improve-pandas-excel-output.html']\n",
      "code ['!pip install xlsxwriter']\n",
      "code ['import pandas as pd\\n', \"print('pandas',pd.__version__)\"]\n",
      "code ['# Create some Pandas dataframes from some data.\\n', \"df1 = pd.DataFrame({'Data': [11, 12, 13, 14]})\\n\", \"df2 = pd.DataFrame({'Data': [21, 22, 23, 24]})\\n\", \"df3 = pd.DataFrame({'Data': [31, 32, 33, 34]})\"]\n",
      "code ['# Create a Pandas Excel writer using XlsxWriter as the engine.\\n', \"excel_file='tmp_multiple.xlsx'\\n\", \"writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\"]\n",
      "code ['# Write each dataframe to a different worksheet.\\n', \"df1.to_excel(writer, sheet_name='Sheet1')\\n\", \"df2.to_excel(writer, sheet_name='Sheet2')\\n\", \"df3.to_excel(writer, sheet_name='Sheet3')\"]\n",
      "code ['# Close the Pandas Excel writer and output the Excel file.\\n', 'writer.save()']\n",
      "markdown [\"# Now that we've saved the file, let's open it\"]\n",
      "markdown [\"by default Pandas only loads the first sheet (and doesn't indicate there are other sheets)\"]\n",
      "code ['df=pd.read_excel(excel_file)\\n', 'df']\n",
      "code ['df1=pd.read_excel(excel_file,sheet_name=1)\\n', 'df1']\n",
      "markdown ['https://xlsxwriter.readthedocs.io/working_with_pandas.html\\n', '\\n', 'Alternatively,']\n",
      "code ['xls = pd.ExcelFile(excel_file)\\n', \"df1 = pd.read_excel(xls, 'Sheet1')\\n\", \"df2 = pd.read_excel(xls, 'Sheet2')\"]\n",
      "code ['df1']\n",
      "code ['df2']\n",
      "code []\n"
     ]
    }
   ],
   "source": [
    "#Reading all the .ipynb files and printing the cell type and the source present in the JSON file\n",
    "#dict1={}\n",
    "for name in glob.glob('*.ipynb'):\n",
    "    with open (name, encoding=\"utf8\", errors='coerce') as f:\n",
    "        data = json.load(f)\n",
    "        for cell in data ['cells']:\n",
    "                print(cell['cell_type'],cell['source'])\n",
    "#citation: https://stackoverflow.com/questions/2186525/how-to-use-glob-to-find-files-recursively\n",
    "#citation: https://stackoverflow.com/questions/20186344/ipynb-import-another-ipynb-file\n",
    "#citation: https://www.youtube.com/watch?v=7GFXm8HUD7s\n",
    "#citation: https://pymotw.com/2/glob/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cells': [{'cell_type': 'markdown', 'metadata': {}, 'source': ['# install xlsxwriter module']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['https://xlsxwriter.readthedocs.io/example_pandas_multiple.html\\n', '\\n', 'https://pbpython.com/improve-pandas-excel-output.html']}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['Requirement already satisfied: xlsxwriter in /opt/conda/lib/python3.6/site-packages (1.2.1)\\n']}], 'source': ['!pip install xlsxwriter']}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['pandas 0.23.4\\n']}], 'source': ['import pandas as pd\\n', \"print('pandas',pd.__version__)\"]}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {}, 'outputs': [], 'source': ['# Create some Pandas dataframes from some data.\\n', \"df1 = pd.DataFrame({'Data': [11, 12, 13, 14]})\\n\", \"df2 = pd.DataFrame({'Data': [21, 22, 23, 24]})\\n\", \"df3 = pd.DataFrame({'Data': [31, 32, 33, 34]})\"]}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {}, 'outputs': [], 'source': ['# Create a Pandas Excel writer using XlsxWriter as the engine.\\n', \"excel_file='tmp_multiple.xlsx'\\n\", \"writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\"]}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {}, 'outputs': [], 'source': ['# Write each dataframe to a different worksheet.\\n', \"df1.to_excel(writer, sheet_name='Sheet1')\\n\", \"df2.to_excel(writer, sheet_name='Sheet2')\\n\", \"df3.to_excel(writer, sheet_name='Sheet3')\"]}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {}, 'outputs': [], 'source': ['# Close the Pandas Excel writer and output the Excel file.\\n', 'writer.save()']}, {'cell_type': 'markdown', 'metadata': {}, 'source': [\"# Now that we've saved the file, let's open it\"]}, {'cell_type': 'markdown', 'metadata': {}, 'source': [\"by default Pandas only loads the first sheet (and doesn't indicate there are other sheets)\"]}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {}, 'outputs': [{'data': {'text/html': ['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Data</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>11</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>12</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>13</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>14</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>'], 'text/plain': ['   Data\\n', '0    11\\n', '1    12\\n', '2    13\\n', '3    14']}, 'execution_count': 7, 'metadata': {}, 'output_type': 'execute_result'}], 'source': ['df=pd.read_excel(excel_file)\\n', 'df']}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {}, 'outputs': [{'data': {'text/html': ['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Data</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>21</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>22</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>23</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>24</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>'], 'text/plain': ['   Data\\n', '0    21\\n', '1    22\\n', '2    23\\n', '3    24']}, 'execution_count': 8, 'metadata': {}, 'output_type': 'execute_result'}], 'source': ['df1=pd.read_excel(excel_file,sheet_name=1)\\n', 'df1']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['https://xlsxwriter.readthedocs.io/working_with_pandas.html\\n', '\\n', 'Alternatively,']}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {}, 'outputs': [], 'source': ['xls = pd.ExcelFile(excel_file)\\n', \"df1 = pd.read_excel(xls, 'Sheet1')\\n\", \"df2 = pd.read_excel(xls, 'Sheet2')\"]}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {}, 'outputs': [{'data': {'text/html': ['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Data</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>11</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>12</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>13</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>14</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>'], 'text/plain': ['   Data\\n', '0    11\\n', '1    12\\n', '2    13\\n', '3    14']}, 'execution_count': 10, 'metadata': {}, 'output_type': 'execute_result'}], 'source': ['df1']}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {}, 'outputs': [{'data': {'text/html': ['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Data</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>21</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>22</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>23</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>24</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>'], 'text/plain': ['   Data\\n', '0    21\\n', '1    22\\n', '2    23\\n', '3    24']}, 'execution_count': 11, 'metadata': {}, 'output_type': 'execute_result'}], 'source': ['df2']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': []}], 'metadata': {'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3}, 'file_extension': '.py', 'mimetype': 'text/x-python', 'name': 'python', 'nbconvert_exporter': 'python', 'pygments_lexer': 'ipython3', 'version': '3.7.3'}}, 'nbformat': 4, 'nbformat_minor': 2}\n"
     ]
    }
   ],
   "source": [
    "#printing the complete data\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cell in data ['cells']:\n",
    "            dict1=cell['cell_type']=='code',cell['source']\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " \"#installing openpyxl\\n\",\n",
      " \"\\n\",\n",
      " \"!pip install openpyxl\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/\"\n",
      "]\n",
      "[\n",
      " \"#importing openpyxl\\n\",\n",
      " \"\\n\",\n",
      " \"import openpyxl\\n\",\n",
      " \"import pandas as pd\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/\"\n",
      "]\n",
      "[\n",
      " \"#loading the wrokbook using openpyxl, by uploading the xlsx file to our jupyternote book and then loading it to 'wb'\\n\",\n",
      " \"\\n\",\n",
      " \"wb = openpyxl.load_workbook('week_05_homework_XLSX_openpyxl.xlsx')\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/\"\n",
      "]\n",
      "[\n",
      " \"#assigning sheet 'patient info' to sheet1\\n\",\n",
      " \"sheet1 = wb['patient info']\\n\",\n",
      " \"sheet1\\n\",\n",
      " \"#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/\"\n",
      "]\n",
      "[\n",
      " \"#assigning sheet 'records' to sheet2\\n\",\n",
      " \"sheet2 = wb['records']\\n\",\n",
      " \"sheet2\\n\",\n",
      " \"#citation: https://realpython.com/openpyxl-excel-spreadsheets-python/\"\n",
      "]\n",
      "[\n",
      " \"#reading the excel sheet for data present in the excel\\n\",\n",
      " \"\\n\",\n",
      " \"df = pd.read_excel (r'week_05_homework_XLSX_openpyxl.xlsx') \\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/20225110/comparing-two-dataframes-and-getting-the-differences/42652112#42652112\"\n",
      "]\n",
      "[\n",
      " \"#assigning the workbook and sheets to the respective variables\\n\",\n",
      " \"\\n\",\n",
      " \"xls = pd.ExcelFile('week_05_homework_XLSX_openpyxl.xlsx')\\n\",\n",
      " \"df1 = pd.read_excel(xls, 'patient info')\\n\",\n",
      " \"df2 = pd.read_excel(xls, 'records')\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.freecodecamp.org/news/how-to-create-read-update-and-search-through-excel-files-using-python-c70680d811d4/\"\n",
      "]\n",
      "[\n",
      " \"df2\"\n",
      "]\n",
      "[\n",
      " \"#renaming the column p_id to patient id, so as to merge both the columns and find the common id's in both the columns\\n\",\n",
      " \"\\n\",\n",
      " \"df3=df2.rename(columns={\\\"p_id\\\": \\\"patient id\\\"})\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://www.datacamp.com/community/tutorials/joining-dataframes-pandas\"\n",
      "]\n",
      "[\n",
      " \"#column heading of all the columns present in df1, i.e., patient info\\n\",\n",
      " \"\\n\",\n",
      " \"df1.columns\"\n",
      "]\n",
      "[\n",
      " \"#column heading of all the columns present in df3, i.e., records, after renaming the column\\n\",\n",
      " \"\\n\",\n",
      " \"df3.columns\"\n",
      "]\n",
      "[\n",
      " \"#the renamed data set\\n\",\n",
      " \"\\n\",\n",
      " \"df3\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"#merging df1 and renamed dataframe (df3) using outer join\\n\",\n",
      " \"\\n\",\n",
      " \"df3_col = pd.merge(df1, df3, on='patient id',how='outer')\\n\",\n",
      " \"\\n\",\n",
      " \"df3_col\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\"\n",
      "]\n",
      "[\n",
      " \"#importing the data to the new xlsx file into our local.\\n\",\n",
      " \"\\n\",\n",
      " \"df3_col.to_excel(\\\"patient_info_output.xlsx\\\")\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\"\n",
      "]\n",
      "[\n",
      " \"#(trial not the output)\\n\",\n",
      " \"# merging the common values using outer join \\n\",\n",
      " \"\\n\",\n",
      " \"df_op=pd.merge(df1,df2,left_on='patient id',right_on='p_id',how='outer')\\n\",\n",
      " \"df_op\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"#Importing ElementTree, beautifulSoup\\n\",\n",
      " \"\\n\",\n",
      " \"import xml.etree.ElementTree as ET\\n\",\n",
      " \"from bs4 import BeautifulSoup\\n\",\n",
      " \"\\n\",\n",
      " \"#The source document is a table of contents written in XML format, and we want to get it displayed in HTML. We will use XSLT module of the lxml library in Python\\n\",\n",
      " \"from lxml import etree\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation : https://docs.python.org/2/library/xml.etree.elementtree.html\\n\",\n",
      " \"#citation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"\n",
      "]\n",
      "[\n",
      " \"#import the data. Reading the file from disk:\\n\",\n",
      " \"\\n\",\n",
      " \"tree=ET.parse('xml_containing_html.xml')\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n\"\n",
      "]\n",
      "[\n",
      " \"#Using ElementTree to parse and get the root\\n\",\n",
      " \"\\n\",\n",
      " \"tree = ET.parse('xml_containing_html.xml')\\n\",\n",
      " \"root = tree.getroot()\\n\",\n",
      " \"root.tag\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n\"\n",
      "]\n",
      "[\n",
      " \"#displaying the tags and attributes\\n\",\n",
      " \"\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    print(child.tag,child.attrib)\\n\",\n",
      " \"    \\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n\",\n",
      " \"#citation: https://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/\"\n",
      "]\n",
      "[\n",
      " \"root[0][1].text\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\"\n",
      "]\n",
      "[\n",
      " \"#The HTML File\\n\",\n",
      " \"\\n\",\n",
      " \"root=x1.getroot()\\n\",\n",
      " \"list=[]\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        print(children.text)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: http://makble.com/convert-xml-to-html-with-lxml-xslt-in-python\\n\",\n",
      " \"#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementinclude-functions\\n\",\n",
      " \"#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementtree-objects\\n\",\n",
      " \"#citation :https://wiki.python.org/moin/ForLoop\\n\"\n",
      "]\n",
      "[\n",
      " \"#displaying the links and number of links for the page\\n\",\n",
      " \"count=0\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        data=children.text\\n\",\n",
      " \"        soup=BeautifulSoup(data,'html.parser')\\n\",\n",
      " \"        for link in soup.find_all('a'):\\n\",\n",
      " \"            ab=(link.get('href'))\\n\",\n",
      " \"            print(ab)\\n\",\n",
      " \"            count+=1\\n\",\n",
      " \"            b=count\\n\",\n",
      " \"    print('----number of links for the page = ',count)\\n\",\n",
      " \"    count=0\\n\",\n",
      " \"    \\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/3075550/how-can-i-get-href-links-from-html-using-python\\n\",\n",
      " \"#citation :https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n\",\n",
      " \"#citation :https://wiki.python.org/moin/ForLoop\"\n",
      "]\n",
      "[\n",
      " \"#Displaying only the links present in the HTML page\\n\",\n",
      " \"count=0\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        data=children.text\\n\",\n",
      " \"        soup=BeautifulSoup(data,'html.parser')\\n\",\n",
      " \"        for link in soup.find_all('a'):\\n\",\n",
      " \"            ab=(link.get('href'))\\n\",\n",
      " \"            print(ab)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n\",\n",
      " \"#citation :https://wiki.python.org/moin/ForLoop\"\n",
      "]\n",
      "[\n",
      " \"#displaying the number links for a single page\\n\",\n",
      " \"\\n\",\n",
      " \"count=0\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        data=children.text\\n\",\n",
      " \"        soup=BeautifulSoup(data,'html.parser')\\n\",\n",
      " \"        for link in soup.find_all('a'):\\n\",\n",
      " \"            count+=1\\n\",\n",
      " \"            b=count\\n\",\n",
      " \"    print('total links for single page=',count)\\n\",\n",
      " \"    count=0\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"#Importing ElementTree, beautifulSoup\\n\",\n",
      " \"\\n\",\n",
      " \"import xml.etree.ElementTree as ET\\n\",\n",
      " \"from bs4 import BeautifulSoup\\n\",\n",
      " \"\\n\",\n",
      " \"#The source document is a table of contents written in XML format, and we want to get it displayed in HTML. We will use XSLT module of the lxml library in Python\\n\",\n",
      " \"from lxml import etree\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation : https://docs.python.org/2/library/xml.etree.elementtree.html\\n\",\n",
      " \"#citation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"\n",
      "]\n",
      "[\n",
      " \"#import the data. Reading the file from disk:\\n\",\n",
      " \"\\n\",\n",
      " \"tree=ET.parse('xml_containing_html.xml')\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n\"\n",
      "]\n",
      "[\n",
      " \"#Using ElementTree to parse and get the root\\n\",\n",
      " \"\\n\",\n",
      " \"tree = ET.parse('xml_containing_html.xml')\\n\",\n",
      " \"root = tree.getroot()\\n\",\n",
      " \"root.tag\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n\"\n",
      "]\n",
      "[\n",
      " \"#displaying the tags and attributes\\n\",\n",
      " \"\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    print(child.tag,child.attrib)\\n\",\n",
      " \"    \\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\\n\",\n",
      " \"#citation: https://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/\"\n",
      "]\n",
      "[\n",
      " \"root[0][1].text\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://stackoverflow.com/questions/33814607/converting-a-python-xml-elementtree-to-a-string?rq=1\"\n",
      "]\n",
      "[\n",
      " \"#The HTML File\\n\",\n",
      " \"\\n\",\n",
      " \"root=x1.getroot()\\n\",\n",
      " \"list=[]\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        print(children.text)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: http://makble.com/convert-xml-to-html-with-lxml-xslt-in-python\\n\",\n",
      " \"#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementinclude-functions\\n\",\n",
      " \"#citation: https://docs.python.org/3/library/xml.etree.elementtree.html#elementtree-objects\\n\",\n",
      " \"#citation :https://wiki.python.org/moin/ForLoop\\n\"\n",
      "]\n",
      "[\n",
      " \"#displaying the links and number of links for the page\\n\",\n",
      " \"count=0\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        data=children.text\\n\",\n",
      " \"        soup=BeautifulSoup(data,'html.parser')\\n\",\n",
      " \"        for link in soup.find_all('a'):\\n\",\n",
      " \"            ab=(link.get('href'))\\n\",\n",
      " \"            print(ab)\\n\",\n",
      " \"            count+=1\\n\",\n",
      " \"            b=count\\n\",\n",
      " \"    print('----number of links for the page = ',count)\\n\",\n",
      " \"    count=0\\n\",\n",
      " \"    \\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/3075550/how-can-i-get-href-links-from-html-using-python\\n\",\n",
      " \"#citation :https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n\",\n",
      " \"#citation :https://wiki.python.org/moin/ForLoop\"\n",
      "]\n",
      "[\n",
      " \"#Displaying only the links present in the HTML page\\n\",\n",
      " \"count=0\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        data=children.text\\n\",\n",
      " \"        soup=BeautifulSoup(data,'html.parser')\\n\",\n",
      " \"        for link in soup.find_all('a'):\\n\",\n",
      " \"            ab=(link.get('href'))\\n\",\n",
      " \"            print(ab)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/33488518/display-the-contents-of-an-element-in-xml-using-python\\n\",\n",
      " \"#citation :https://wiki.python.org/moin/ForLoop\"\n",
      "]\n",
      "[\n",
      " \"#displaying the number links for a single page\\n\",\n",
      " \"\\n\",\n",
      " \"count=0\\n\",\n",
      " \"for child in root:\\n\",\n",
      " \"    for children in child:\\n\",\n",
      " \"        data=children.text\\n\",\n",
      " \"        soup=BeautifulSoup(data,'html.parser')\\n\",\n",
      " \"        for link in soup.find_all('a'):\\n\",\n",
      " \"            count+=1\\n\",\n",
      " \"            b=count\\n\",\n",
      " \"    print('total links for single page=',count)\\n\",\n",
      " \"    count=0\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import os \\n\",\n",
      " \"import nltk\\n\",\n",
      " \"from nltk.tokenize import RegexpTokenizer\\n\",\n",
      " \"from string import punctuation\\n\",\n",
      " \"from nltk.tokenize import sent_tokenize,word_tokenize \\n\",\n",
      " \"from nltk.corpus import stopwords\\n\",\n",
      " \"import string\\n\",\n",
      " \"import re\\n\",\n",
      " \"import os.path\\n\",\n",
      " \"import glob\"\n",
      "]\n",
      "[\n",
      " \"# path of the file location and listing the files \"\n",
      "]\n",
      "[\n",
      " \"#path=\\\"C:/Users/Shrimanth/DATA_601/Week_9/Assignment\\\"\\n\",\n",
      " \"#os.listdir(path)\"\n",
      "]\n",
      "[\n",
      " \"#filetxtList=filelist[:-2]\\n\",\n",
      " \"filetxtList=['52256-0.txt', '53031-0.txt', '58108-0.txt', 'blind_text.txt', 'dr_yawn.txt', 'how_rubber_goods_are_made.txt', 'most_boring_ever.txt', 'most_boring_part2.txt', 'pg12814.txt', 'pg14895.txt', 'pg43994.txt', 'random_text.txt', 'smiley_the_bunny.txt']\\n\",\n",
      " \"type(filetxtList)\"\n",
      "]\n",
      "[\n",
      " \"filetxtList\"\n",
      "]\n",
      "[\n",
      " \"len(filelist) # total number of files \"\n",
      "]\n",
      "[\n",
      " \"from nltk.corpus import stopwords\\n\",\n",
      " \"stop_word = set(stopwords.words('english'))\\n\",\n",
      " \"stop_word\"\n",
      "]\n",
      "[\n",
      " \"# function to get the words other than stopwords\\n\",\n",
      " \"def filewords(text):\\n\",\n",
      " \"    from nltk.corpus import stopwords\\n\",\n",
      " \"    stop_words = set(stopwords.words('english'))\\n\",\n",
      " \"    text=text.lower()\\n\",\n",
      " \"    #table=str.maketrans(\\\"!?.@*$\\u201c\\u2019^\\\",9*\\\" \\\")\\n\",\n",
      " \"    #text.translate(table)\\n\",\n",
      " \"    word_tokens = word_tokenize(text)\\n\",\n",
      " \"    removing_stopwords = [word for word in word_tokens if word not in stop_words]\\n\",\n",
      " \"    text=''.join(c for c in text if c not in c.isdigit())\\n\",\n",
      " \"    text=''.join(w for w in text if w not in punctuation)\\n\",\n",
      " \"    print('words',removing_stopwords)\\n\",\n",
      " \"    \"\n",
      "]\n",
      "[\n",
      " \"with open('52256-0.txt','r',encoding=\\\"utf8\\\") as file1:\\n\",\n",
      " \"    text1=(file1.read())\\n\",\n",
      " \"    #filewords(text1)\"\n",
      "]\n",
      "[\n",
      " \"# to convert to lowercase\\n\",\n",
      " \"text1=text1.lower()\"\n",
      "]\n",
      "[\n",
      " \"# to remove numbers\\n\",\n",
      " \"text1 = ''.join(c for c in text1 if not c.isdigit())\\n\",\n",
      " \"print(text1)\"\n",
      "]\n",
      "[\n",
      " \"# Reference- https://machinelearningmastery.com/clean-text-machine-learning-python/\\n\",\n",
      " \"# to remove punctuation\\n\",\n",
      " \"from string import punctuation\\n\",\n",
      " \"text1=''.join(c for c in text1 if c not in punctuation)\\n\",\n",
      " \"print(text1)\\n\"\n",
      "]\n",
      "[\n",
      " \"# to remove metacharcters by replacing \\n\",\n",
      " \"text1=text1.replace('\\u201d','')\\n\",\n",
      " \"#print(output)\\n\",\n",
      " \"text1=text1.replace('\\u201c','')\\n\",\n",
      " \"text1=text1.replace('\\u2019','')\\n\",\n",
      " \"text1=text1.replace('_','')\\n\",\n",
      " \"print(text1)\"\n",
      "]\n",
      "[\n",
      " \"# to convert to word tokens \\n\",\n",
      " \"from nltk.tokenize import sent_tokenize,word_tokenize \\n\",\n",
      " \"word_tokens = word_tokenize(text1) \\n\",\n",
      " \"print(word_tokens)\"\n",
      "]\n",
      "[\n",
      " \"stopword = stopwords.words('english')\\n\",\n",
      " \"removing_stopwords = [word for word in word_tokens if word not in stopword]\\n\",\n",
      " \"print (removing_stopwords)\"\n",
      "]\n",
      "[\n",
      " \"for file in filetxtList: \\n\",\n",
      " \"    file= open (file , 'rb')\\n\",\n",
      " \"    textcont= file.read().decode(\\\"utf-8\\\")\\n\",\n",
      " \"    textcont= textcont.lower()\\n\",\n",
      " \"    link_rem = textcont.replace(\\\"www.\\\",\\\"\\\")\\n\",\n",
      " \"    text = link_rem.replace(\\\"_\\\", \\\" \\\")\\n\",\n",
      " \"    tokenizer = RegexpTokenizer(r'\\\\w+')\\n\",\n",
      " \"    word_tokens= tokenizer.tokenize(text)\\n\",\n",
      " \"    words = list(filter(lambda w: w not in string.punctuation, word_tokens))\\n\",\n",
      " \"    stop_words = set(stopwords.words(\\\"english\\\"))\\n\",\n",
      " \"    words_aftstp=list(filter(lambda w:w not in stop_words, words))\\n\",\n",
      " \"    output_text = ''.join([a for a in words_aftstp if not a.isdigit()])\\n\",\n",
      " \"    final_text = ''.join(output_text)\\n\",\n",
      " \"    print(':', final_text, '\\\\n','\\\\n')\\n\",\n",
      " \"   \\n\",\n",
      " \"    #print(final_text)\\n\",\n",
      " \"    #print(file,final_text, '\\\\n')\\n\"\n",
      "]\n",
      "[\n",
      " \"filetxtList\\n\"\n",
      "]\n",
      "[\n",
      " \"for file in filetxtList: \\n\",\n",
      " \"    f= open (file , 'rb')\\n\",\n",
      " \"    lines= f.read().decode('UTF-8')\\n\",\n",
      " \"    s_to_lower= lines.lower()\\n\",\n",
      " \"    links = s_to_lower.replace(\\\"www.\\\",\\\"\\\")\\n\",\n",
      " \"    tokenizer = RegexpTokenizer(r'\\\\w+')\\n\",\n",
      " \"    tok= tokenizer.tokenize(links)\\n\",\n",
      " \"    w = list(filter(lambda n: n not in string.punctuation, tok))\\n\",\n",
      " \"    s_words = set(stopwords.words(\\\"english\\\"))\\n\",\n",
      " \"    after=list(filter(lambda n:n not in s_words, w))\\n\",\n",
      " \"    res = ' '.join([i for i in after if not i.isdigit()])\\n\",\n",
      " \"    f_text = ''.join(res)\\n\",\n",
      " \"    text = f_text.replace(\\\"_\\\", \\\" \\\")\\n\",\n",
      " \"    print('\\\"'+ file + '\\\"', ':', text, '\\\\n','\\\\n')\\n\",\n",
      " \"    saveFile = open('outputFile2.dat','w')\\n\",\n",
      " \"    saveFile.write(finals_text)\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import pandas as pd\\n\",\n",
      " \"import os\"\n",
      "]\n",
      "[\n",
      " \"#Loading the data into pandas data frame\\n\",\n",
      " \"\\n\",\n",
      " \"#https://people.sc.fsu.edu/~jburkardt/data/csv/oscar_age_male.csv ---> csv data used\\n\",\n",
      " \"\\n\",\n",
      " \"path_to_file = \\\"C:/Users/pushy/OneDrive/Desktop/UMBC Sem-1/601/assignment.csv\\\"\\n\",\n",
      " \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\",\n",
      " \"print(type(data))\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\"\n",
      "]\n",
      "[\n",
      " \"# count the number of rows of data \\n\",\n",
      " \"\\n\",\n",
      " \"len(df.index)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"#  count the number of columns. \\n\",\n",
      " \"\\n\",\n",
      " \"len(df.columns)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"df.shape\"\n",
      "]\n",
      "[\n",
      " \"#dimensions of the data frame \\n\",\n",
      " \"\\n\",\n",
      " \"df.shape\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html\"\n",
      "]\n",
      "[\n",
      " \"#transpose of the data frame\\n\",\n",
      " \"\\n\",\n",
      " \"result = df[:4].T\\n\",\n",
      " \"print(result)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-transpose/#targetText=Pandas%20DataFrame.transpose()%20function,as%20columns%20and%20vice%2Dversa.&targetText=Parameter%20%3A,the%20underlying%20data%20is%20copied.\"\n",
      "]\n",
      "[\n",
      " \"#Display the bottom 8 rows of the dataframe\\n\",\n",
      " \"\\n\",\n",
      " \"df.head(8)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html\"\n",
      "]\n",
      "[\n",
      " \"#Display the bottom 3 rows of the dataframe\\n\",\n",
      " \"\\n\",\n",
      " \"df.tail(3)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html\"\n",
      "]\n",
      "[\n",
      " \"#displaying the complete data\\n\",\n",
      " \"\\n\",\n",
      " \"data\"\n",
      "]\n",
      "[\n",
      " \"data\\n\"\n",
      "]\n",
      "[\n",
      " \"path_to_file = \\\"C:/Users/pushy/OneDrive/Desktop/UMBC Sem-1/601/assignment.csv\\\"\"\n",
      "]\n",
      "[\n",
      " \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\",\n",
      " \"print(type(data))\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"print(\\\"hello\\\")\\n\",\n",
      " \"print(a)\"\n",
      "]\n",
      "[\n",
      " \"def a_func(my_lst)\\n\",\n",
      " \"    return my_lst[5]\"\n",
      "]\n",
      "[\n",
      " \"def a_func(my_lst)\\n\",\n",
      " \"     print('hello')\\n\",\n",
      " \"    return my_lst[5]\"\n",
      "]\n",
      "[\n",
      " \"lst = ['a', 'b', 'c']\"\n",
      "]\n",
      "[\n",
      " \"len(lst)\"\n",
      "]\n",
      "[\n",
      " \"lst[5]\"\n",
      "]\n",
      "[\n",
      " \"try:\\n\",\n",
      " \"    lst[5]\\n\",\n",
      " \"except IndexError:\\n\",\n",
      " \"    print('index out of bounds for lst')\"\n",
      "]\n",
      "[\n",
      " \"def a_func(my_lst):\\n\",\n",
      " \"#   print('reached a_func')\\n\",\n",
      " \"    return my_lst[5]\\n\",\n",
      " \"\\n\",\n",
      " \"def speaker(phone_number,lst):\\n\",\n",
      " \"#    print(\\\"reached speaker\\\")\\n\",\n",
      " \"    val = a_func(lst)\\n\",\n",
      " \"    return val\"\n",
      "]\n",
      "[\n",
      " \"the_value = speaker('424-5241',lst)\"\n",
      "]\n",
      "[\n",
      " \"def a_func(my_lst):\\n\",\n",
      " \"    print('reached a_func')\\n\",\n",
      " \"    return my_lst[5]\\n\",\n",
      " \"\\n\",\n",
      " \"def speaker(phone_number,lst):\\n\",\n",
      " \"    print(\\\"reached speaker\\\")\\n\",\n",
      " \"    val = a_func(lst)\\n\",\n",
      " \"    return val\"\n",
      "]\n",
      "[\n",
      " \"the_value = speaker('424-5241',lst)\"\n",
      "]\n",
      "[\n",
      " \"def a_func(my_lst):\\n\",\n",
      " \"    print('reached a_func')\\n\",\n",
      " \"    try:\\n\",\n",
      " \"        return my_lst[5]\\n\",\n",
      " \"    except IndexError:\\n\",\n",
      " \"        print('index out of range, returning None')\\n\",\n",
      " \"        return None\\n\",\n",
      " \"\\n\",\n",
      " \"def speaker(phone_number,lst):\\n\",\n",
      " \"    print(\\\"reached speaker\\\")\\n\",\n",
      " \"    val = a_func(lst)\\n\",\n",
      " \"    return val\"\n",
      "]\n",
      "[\n",
      " \"the_value = speaker('424-5241',lst)\"\n",
      "]\n",
      "[\n",
      " \"type('hello')\"\n",
      "]\n",
      "[\n",
      " \"type('hello')=='str'\"\n",
      "]\n",
      "[\n",
      " \"isinstance('hello', str)\"\n",
      "]\n",
      "[\n",
      " \"isinstance(['a','b'],list)\"\n",
      "]\n",
      "[\n",
      " \"def a_func(my_lst):\\n\",\n",
      " \"    print('reached a_func')\\n\",\n",
      " \"    if not isinstance(my_lst,list):\\n\",\n",
      " \"        raise TypeError('a_func was provided the wrong type',type(my_lst))\\n\",\n",
      " \"    try:\\n\",\n",
      " \"        return my_lst[5]\\n\",\n",
      " \"    except IndexError:\\n\",\n",
      " \"        print('index out of range, returning None')\\n\",\n",
      " \"        return None\\n\",\n",
      " \"\\n\",\n",
      " \"def speaker(phone_number,lst):\\n\",\n",
      " \"    print(\\\"reached speaker\\\")\\n\",\n",
      " \"    val = a_func(lst)\\n\",\n",
      " \"    return val\"\n",
      "]\n",
      "[\n",
      " \"the_value = speaker('424-5241','bob')\"\n",
      "]\n",
      "[\n",
      " \"def a_func(my_lst):\\n\",\n",
      " \"    print('reached a_func')\\n\",\n",
      " \"    try:\\n\",\n",
      " \"        return my_lst[5]\\n\",\n",
      " \"    except IndexError:\\n\",\n",
      " \"        print('index out of range, returning None')\\n\",\n",
      " \"        return None\\n\",\n",
      " \"\\n\",\n",
      " \"def speaker(phone_number,lst):\\n\",\n",
      " \"    print(\\\"reached speaker\\\")\\n\",\n",
      " \"    val = a_func(lst)\\n\",\n",
      " \"    return val\"\n",
      "]\n",
      "[\n",
      " \"speaker('424-5241','asdfjbas')\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"!pip install xlsxwriter\"\n",
      "]\n",
      "[\n",
      " \"import xlsxwriter\\n\",\n",
      " \"import pandas\\n\",\n",
      " \"print('Pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"df=pandas.read_excel('week_05_in-class_activity.xlsx')\"\n",
      "]\n",
      "[\n",
      " \"df.head()\"\n",
      "]\n",
      "[\n",
      " \"df['fantastic']=df['cool']+2*df['rad']\"\n",
      "]\n",
      "[\n",
      " \"df.to_excel('tmp_overwrite.xlsx', engine='xlsxwriter')\"\n",
      "]\n",
      "[\n",
      " \"!pip install openpyxl\"\n",
      "]\n",
      "[\n",
      " \"import openpyxl\"\n",
      "]\n",
      "[\n",
      " \"wb = openpyxl.load_workbook('week_05_in-class_activity.xlsx')\\n\",\n",
      " \"\\n\",\n",
      " \"# an Excel file is composed of one or more sheets\\n\",\n",
      " \"wb.sheetnames\"\n",
      "]\n",
      "[\n",
      " \"# select the one available sheet; assign to a variable\\n\",\n",
      " \"sheet = wb['Sheet1']\\n\",\n",
      " \"\\n\",\n",
      " \"type(sheet)\"\n",
      "]\n",
      "[\n",
      " \"# what is at the first row and first column in the worksheet?\\n\",\n",
      " \"\\n\",\n",
      " \"sheet.cell(row=1, column=1)\"\n",
      "]\n",
      "[\n",
      " \"sheet.cell(row=1, column=1).value\"\n",
      "]\n",
      "[\n",
      " \"for row_indx in range(1,5):\\n\",\n",
      " \"    print(row_indx, sheet.cell(row=row_indx, column=1).value)\"\n",
      "]\n",
      "[\n",
      " \"# https://openpyxl.readthedocs.io/en/stable/tutorial.html\\n\",\n",
      " \"new_ws = wb.create_sheet(\\\"example\\\") \"\n",
      "]\n",
      "[\n",
      " \"print(wb.sheetnames)\"\n",
      "]\n",
      "[\n",
      " \"# https://stackoverflow.com/questions/31395058/how-to-write-to-a-new-cell-in-python-using-openpyxl\\n\",\n",
      " \"new_ws.cell(row=2, column=2).value = 'a string'\"\n",
      "]\n",
      "[\n",
      " \"new_ws[\\\"A1\\\"] = \\\"=SUM(1, 1)\\\"\"\n",
      "]\n",
      "[\n",
      " \"# https://openpyxl.readthedocs.io/en/stable/_modules/openpyxl/worksheet/cell_range.html\\n\",\n",
      " \"new_ws['A1':'A5']\"\n",
      "]\n",
      "[\n",
      " \"type(new_ws['A1':'A5'])\"\n",
      "]\n",
      "[\n",
      " \"new_ws['A1':'A5'][0]\"\n",
      "]\n",
      "[\n",
      " \"type(new_ws['A1':'A5'][0])\"\n",
      "]\n",
      "[\n",
      " \"new_ws['A1':'A5'][0][0]\"\n",
      "]\n",
      "[\n",
      " \"type(new_ws['A1':'A5'][0][0])\"\n",
      "]\n",
      "[\n",
      " \"new_ws['A1':'A5'][0][0].value\"\n",
      "]\n",
      "[\n",
      " \"for each_cell in new_ws['A1':'A5']:\\n\",\n",
      " \"    each_cell[0].value='bob'\"\n",
      "]\n",
      "[\n",
      " \"wb.save('new_file.xlsx')\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import random\"\n",
      "]\n",
      "[\n",
      " \"dir(random)\"\n",
      "]\n",
      "[\n",
      " \"help(random)\"\n",
      "]\n",
      "[\n",
      " \"random?\"\n",
      "]\n",
      "[\n",
      " \"\\\"\\\"\\\" main explanation \\\"\\\"\\\"\\n\",\n",
      " \"\\n\",\n",
      " \"def my_func(a_str):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    This is the help\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    return 'hello'\"\n",
      "]\n",
      "[\n",
      " \"help(my_func)\"\n",
      "]\n",
      "[\n",
      " \"my_func?\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import pandas\"\n",
      "]\n",
      "[\n",
      " \"pandas.DataFrame()\"\n",
      "]\n",
      "[\n",
      " \"# https://kolesnikov.ga/Testing_and_Debugging_Jupyter_Notebooks/\\n\",\n",
      " \"import doctest\"\n",
      "]\n",
      "[\n",
      " \"def convert_datetime_to_dayofweek(datetime_string):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    This function takes date in the format MON DAY YEAR  HH:MM(PM/AM)\\n\",\n",
      " \"    and returns the day of the week\\n\",\n",
      " \"    Assume input string is UTC\\n\",\n",
      " \"    \\n\",\n",
      " \"    >>> convert_datetime_to_dayofweek('Jun 1 2005  1:33PM')\\n\",\n",
      " \"    'Wednesday'\\n\",\n",
      " \"    >>> convert_datetime_to_dayofweek('Oct 25 2012  2:17AM')\\n\",\n",
      " \"    'Thursday'\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    date_to_day=pd.Timestamp(datetime_string)\\n\",\n",
      " \"    day=date_to_day.weekday_name\\n\",\n",
      " \"    return day\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.dt.weekday_name.html\\n\",\n",
      " \"#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.dayofweek.html\\n\",\n",
      " \"#citation:https://docs.python.org/3/library/datetime.html\"\n",
      "]\n",
      "[\n",
      " \"convert_datetime_to_dayofweek('Jun 1 2005  1:33PM')\"\n",
      "]\n",
      "[\n",
      " \"convert_datetime_to_dayofweek('Oct 25 2012  2:17AM')\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"#Started by importing faker libraries and pandas\\n\",\n",
      " \"from faker import Faker\\n\",\n",
      " \"\\n\",\n",
      " \"import pandas as pd\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation: https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python\"\n",
      "]\n",
      "[\n",
      " \"import random\\n\",\n",
      " \"import csv\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation : https://www.programiz.com/python-programming/working-csv-files\"\n",
      "]\n",
      "[\n",
      " \"pip install csvfaker\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python\"\n",
      "]\n",
      "[\n",
      " \"pip install Faker\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python\"\n",
      "]\n",
      "[\n",
      " \"#initialise Faker generator \\n\",\n",
      " \"\\n\",\n",
      " \"fk1 = Faker()\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"#create an example of generating a fake data for a random name:\\n\",\n",
      " \"fk1.name()\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python\"\n",
      "]\n",
      "[\n",
      " \"#Attempts to generate more than one fake data record at a time\\n\",\n",
      " \"for n in range(10):\\n\",\n",
      " \"    print(fk1.name())\\n\",\n",
      " \"    \\n\",\n",
      " \"#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python\"\n",
      "]\n",
      "[\n",
      " \"#wanted to generate a custom list of fake data types and create a pandas data frame with these fake data points:\\n\",\n",
      " \"\\n\",\n",
      " \"df = []\\n\",\n",
      " \"List = ['own', 'rent']\\n\",\n",
      " \"\\n\",\n",
      " \"#taking imput from the user as of how many entities of fake data has to be generated.\\n\",\n",
      " \"\\n\",\n",
      " \"number=int(input(\\\"number of entities =\\\"))\\n\",\n",
      " \"\\n\",\n",
      " \"for n in range(number):\\n\",\n",
      " \"    df.append({'Name': fk1.name()\\n\",\n",
      " \"        , 'email': fk1.email()\\n\",\n",
      " \"        , 'Address' : fk1.address()\\n\",\n",
      " \"        ,'City' : fk1.city()\\n\",\n",
      " \"        , 'State': fk1.state()\\n\",\n",
      " \"        , 'Year of birth' : fk1.year()\\n\",\n",
      " \"        , 'no.of children' : random.randint(1, 2)\\n\",\n",
      " \"        , 'Annual Income' : random.randrange(10000,90000, 2)\\n\",\n",
      " \"        ,'Number of speeding tickets in past year' : random.randrange(1,10, 2)\\n\",\n",
      " \"        ,'Phone Number' : random.randrange(4000000000,4999999999)\\n\",\n",
      " \"        , 'House' : random.choice(List)\\n\",\n",
      " \"               \\n\",\n",
      " \"              })\\n\",\n",
      " \"    \\n\",\n",
      " \"#Data arrangement\\n\",\n",
      " \"df = pd.DataFrame(df)\\n\",\n",
      " \"df = df[['Name', 'email', 'Address', 'City', 'State','Year of birth','no.of children','Annual Income','House','Number of speeding tickets in past year','Phone Number']]\\n\",\n",
      " \"df\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation : https://feliperego.github.io/blog/2019/01/11/Creating-Fake-Mock-Data-Python\\n\",\n",
      " \"#Citation : https://stackoverflow.com/questions/45574191/using-python-faker-generate-different-data-for-5000-rows\\n\",\n",
      " \"#citation : https://faker.readthedocs.io/en/master/\\n\",\n",
      " \"#Citation : https://faker.readthedocs.io/en/master/providers/faker.providers.phone_number.html\\n\",\n",
      " \"#citation : https://www.tutorialspoint.com/python/number_randrange.htm\"\n",
      "]\n",
      "[\n",
      " \"df.to_csv(\\\"Assign_Week3.csv\\\",sep=' ')\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation :https://medium.com/@kasiarachuta/importing-and-exporting-csv-files-in-python-7fa6e4d9f408\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"\\n\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"print(\\\"hello\\\")\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"# see \\\"Profile Path\\\" on the page chrome://version/\\n\",\n",
      " \"\\n\",\n",
      " \"#!cp /Users/ben/Library/Application\\\\ Support/Google/Chrome/Default/History .\"\n",
      "]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"import sqlite3\\n\",\n",
      " \"print('pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"conn = sqlite3.connect(\\\"History\\\") # connect to the file\\n\",\n",
      " \"type(conn)\"\n",
      "]\n",
      "[\n",
      " \"query_str = \\\"SELECT strftime('%s', '1601-01-01 00:00');\\\" \\n\",\n",
      " \"\\n\",\n",
      " \"curr_time = pandas.read_sql_query(query_str,conn)\"\n",
      "]\n",
      "[\n",
      " \"type(curr_time)\"\n",
      "]\n",
      "[\n",
      " \"curr_time\"\n",
      "]\n",
      "[\n",
      " \"curr_time.iloc[0][0]\"\n",
      "]\n",
      "[\n",
      " \"query_str = \\\"SELECT \\n\",\n",
      " \"               datetime(last_visit_time/1000000+strftime('%s', '1601-01-01 00:00:00')-(60*60*4),'unixepoch','localtime'), \\n\",\n",
      " \"               url FROM urls ORDER BY last_visit_time DESC;\\\"\"\n",
      "]\n",
      "[\n",
      " \"conn = sqlite3.connect(\\\"History\\\")\\n\",\n",
      " \"hist_df = pandas.read_sql_query(query_str, conn)\"\n",
      "]\n",
      "[\n",
      " \"hist_df.columns=['date_time','url']\\n\",\n",
      " \"hist_df.head()\"\n",
      "]\n",
      "[\n",
      " \"hist_df.shape\"\n",
      "]\n",
      "[\n",
      " \"hist_df.tail()\"\n",
      "]\n",
      "[\n",
      " \"hist_df.dtypes\"\n",
      "]\n",
      "[\n",
      " \"hist_df['date_time'] = pandas.to_datetime(hist_df['date_time'],\\n\",\n",
      " \"                                          format='%Y-%m-%d %H:%M:%S',\\n\",\n",
      " \"                                          errors='coerce')\"\n",
      "]\n",
      "[\n",
      " \"hist_df.head()\"\n",
      "]\n",
      "[\n",
      " \"hist_df.tail()\"\n",
      "]\n",
      "[\n",
      " \"hist_df['date_time'].max()\"\n",
      "]\n",
      "[\n",
      " \"hist_df['date_time'].min()\"\n",
      "]\n",
      "[\n",
      " \"pandas.isnull(hist_df['date_time']).sum()\"\n",
      "]\n",
      "[\n",
      " \"import matplotlib.pyplot as plt\"\n",
      "]\n",
      "[\n",
      " \"hist_df[\\\"date_time\\\"].groupby(hist_df[\\\"date_time\\\"].dt.month).count().plot(kind=\\\"bar\\\")\\n\",\n",
      " \"plt.xlabel('month', fontsize=14)\\n\",\n",
      " \"plt.ylabel('URL count', fontsize=14)\\n\",\n",
      " \"plt.setp(plt.gca().get_xticklabels(), rotation=20, fontsize=12) # https://stackoverflow.com/questions/6390393/matplotlib-make-tick-labels-font-size-smaller\\n\",\n",
      " \"plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"# sanity check:\\n\",\n",
      " \"# URLs per day?\\n\",\n",
      " \"8500/30\"\n",
      "]\n",
      "[\n",
      " \"hist_plot = hist_df[\\\"date_time\\\"].groupby([hist_df[\\\"date_time\\\"].dt.year, \\n\",\n",
      " \"                                          hist_df[\\\"date_time\\\"].dt.month]).count().plot(kind=\\\"bar\\\")\\n\",\n",
      " \"plt.xlabel('year,month', fontsize=14)\\n\",\n",
      " \"plt.ylabel('URL count', fontsize=14)\\n\",\n",
      " \"plt.setp(plt.gca().get_xticklabels(), rotation=45, fontsize=12)\\n\",\n",
      " \"plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"fig = plt.figure(figsize=(10,6))\\n\",\n",
      " \"#plt.figure(figsize=(20,20))\\n\",\n",
      " \"\\n\",\n",
      " \"hist_df[\\\"date_time\\\"].groupby(hist_df[\\\"date_time\\\"].dt.day).count().plot(kind=\\\"bar\\\")\\n\",\n",
      " \"\\n\",\n",
      " \"plt.xlabel('day of month', fontsize=14)\\n\",\n",
      " \"plt.ylabel('URL count', fontsize=14)\\n\",\n",
      " \"plt.setp(plt.gca().get_xticklabels(), rotation=45, fontsize=12)\\n\",\n",
      " \"plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"fig = plt.figure(figsize=(10,6))\\n\",\n",
      " \"\\n\",\n",
      " \"hist_plot = hist_df[\\\"date_time\\\"].groupby([hist_df[\\\"date_time\\\"].dt.month, hist_df[\\\"date_time\\\"].dt.day]).count().plot(kind=\\\"bar\\\")\\n\",\n",
      " \"# https://stackoverflow.com/questions/20337664/cleanest-way-to-hide-every-nth-tick-label-in-matplotlib-colorbar\\n\",\n",
      " \"for label in hist_plot.xaxis.get_ticklabels()[::2]:\\n\",\n",
      " \"    label.set_visible(False)\\n\",\n",
      " \"plt.xlabel('day', fontsize=14)\\n\",\n",
      " \"plt.ylabel('URL count', fontsize=14)\\n\",\n",
      " \"\\n\",\n",
      " \"plt.setp(plt.gca().get_xticklabels(), rotation=80, fontsize=12)\\n\",\n",
      " \"plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n\",\n",
      " \"\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"fig = plt.figure(figsize=(10,6))\\n\",\n",
      " \"\\n\",\n",
      " \"hist_df[\\\"date_time\\\"].groupby(hist_df[\\\"date_time\\\"].dt.hour).count().plot(kind=\\\"bar\\\")\\n\",\n",
      " \"plt.xlabel('hour', fontsize=14)\\n\",\n",
      " \"plt.ylabel('URL count', fontsize=14)\\n\",\n",
      " \"\\n\",\n",
      " \"plt.setp(plt.gca().get_xticklabels(), rotation=40, fontsize=12)\\n\",\n",
      " \"plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"#Importing Pandas, numpy, random, matplot, excelwiter from pandas, excelfile from pandass, lag_plot from pandas.plotting\\n\",\n",
      " \"\\n\",\n",
      " \"import pandas as pd\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"import random\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"from pandas import ExcelWriter\\n\",\n",
      " \"from pandas import ExcelFile\\n\",\n",
      " \"from pandas.plotting import lag_plot\\n\",\n",
      " \"\\n\",\n",
      " \"#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n\",\n",
      " \"#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\"\n",
      "]\n",
      "[\n",
      " \"#reading the excel using pandas\\n\",\n",
      " \"#the excel taken 'imputation_homework_29.xlsx'\\n\",\n",
      " \"df1 = pd.read_excel('imputation_homework_29.xlsx', sheetname='Sheet1')\\n\",\n",
      " \"df1\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pythonspot.com/read-excel-with-pandas/\"\n",
      "]\n",
      "[\n",
      " \"#reading the excel using pandas\\n\",\n",
      " \"#the excel taken 'imputation_homework_04.xlsx'\\n\",\n",
      " \"\\n\",\n",
      " \"df2 = pd.read_excel('imputation_homework_04.xls', sheetname='Sheet1')\\n\",\n",
      " \"df2\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pythonspot.com/read-excel-with-pandas/\"\n",
      "]\n",
      "[\n",
      " \"#reading the excel using pandas\\n\",\n",
      " \"#the excel taken 'imputation_homework_91.xlsx'\\n\",\n",
      " \"\\n\",\n",
      " \"df3 = pd.read_excel('imputation_homework_91.xlsx', sheetname='Sheet1')\\n\",\n",
      " \"df3\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pythonspot.com/read-excel-with-pandas/\"\n",
      "]\n",
      "[\n",
      " \"#checking for null values\\n\",\n",
      " \"\\n\",\n",
      " \"df1.isnull()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"#checking for number of null values present in the data set\\n\",\n",
      " \"\\n\",\n",
      " \"df1.isnull().sum()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"#checking for any null values present in the data set\\n\",\n",
      " \"\\n\",\n",
      " \"df2.isnull()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"#checking for number of null values present in the data set\\n\",\n",
      " \"\\n\",\n",
      " \"df2.isnull().sum()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"#checking for any null values present in the data set\\n\",\n",
      " \"\\n\",\n",
      " \"df3.isnull()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"#checking for number of null values present in the data set\\n\",\n",
      " \"\\n\",\n",
      " \"df3.isnull().sum()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\"\n",
      "]\n",
      "[\n",
      " \"#Histogram for the first dataset using the column 'x'\\n\",\n",
      " \"\\n\",\n",
      " \"plt.hist(df1['x'])\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"#Histogram for the second dataset using the column 'value'\\n\",\n",
      " \"\\n\",\n",
      " \"plt.hist(df2['value'])\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"#Histogram for the third dataset using the column 'value'\\n\",\n",
      " \"plt.hist(df3['value'])\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"plt.figure(figsize=(7,7))\\n\",\n",
      " \"lag_plot(df1['x'])\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-lag\"\n",
      "]\n",
      "[\n",
      " \"plt.figure(figsize=(7,7))\\n\",\n",
      " \"lag_plot(df2['value'])\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-lag\"\n",
      "]\n",
      "[\n",
      " \"plt.figure(figsize=(7,7))\\n\",\n",
      " \"lag_plot(df3['value'])\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-lag\"\n",
      "]\n",
      "[\n",
      " \"#inserting values for the missing values, we are interpolating the values using the method 'linear'.\\n\",\n",
      " \"\\n\",\n",
      " \"df1['x'] = df1['x'].interpolate(method='linear')\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html\"\n",
      "]\n",
      "[\n",
      " \"df1.plot.scatter(x = 'x', y ='y', figsize=(7,7), c='red')\\n\",\n",
      " \"plt.title('imputation_homework_29.xlsx')\\n\",\n",
      " \"#plotting a scatter plot for the fiirst dataframe 'df1'\"\n",
      "]\n",
      "[\n",
      " \"#inserting values for the missing values, we are interpolating the values using the method 'linear'.\\n\",\n",
      " \"\\n\",\n",
      " \"df2['value'] = df2['value'].interpolate(method='linear')\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html\"\n",
      "]\n",
      "[\n",
      " \"df2.plot.scatter(x = 'time', y ='value', figsize=(7,7), c='red')\\n\",\n",
      " \"plt.title('imputation_homework_04.xls')\\n\",\n",
      " \"#plotting a scatter plot for the second dataframe 'df2'\"\n",
      "]\n",
      "[\n",
      " \"#inserting values in the non linear set \\n\",\n",
      " \"\\n\",\n",
      " \"df3['value'] = df3['value'].apply(lambda x: np.random.choice(df3['value'].dropna().values) if np.isnan(x) else x)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/36413314/filling-missing-data-by-random-choosing-from-non-missing-values-in-pandas-datafr\"\n",
      "]\n",
      "[\n",
      " \"\\n\",\n",
      " \"plt.figure(figsize=(17,7))\\n\",\n",
      " \"plt.plot_date(x = df3['date'], y =df3['value'], xdate=True, c='red')\\n\",\n",
      " \"plt.xlabel('date')\\n\",\n",
      " \"plt.ylabel('value')\\n\",\n",
      " \"plt.title('imputation_homework_91.xlsx')\\n\",\n",
      " \"#citation:https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.plot_date.html\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import re\"\n",
      "]\n",
      "[\n",
      " \"# %load text.py\\n\",\n",
      " \"this_text=\\\"I go to the store. A car is parked. \\\\\\n\",\n",
      " \"Many cars are parked or moving. Some are blue. \\\\\\n\",\n",
      " \"Some are tan. They have windows. In the store, \\\\\\n\",\n",
      " \"there are items for sale. These include such \\\\\\n\",\n",
      " \"things as soap, detergent, magazines, and lettuce. \\\\\\n\",\n",
      " \"You can enhance your life with these products. \\\\\\n\",\n",
      " \"Soap can be used for bathing, be it in a bathtub \\\\\\n\",\n",
      " \"or in a shower. My email address is myname@sc.edu. \\\\\\n\",\n",
      " \"Apply the soap to your body and rinse. My phone \\\\\\n\",\n",
      " \"number is 452-953-2942. Detergent is used to \\\\\\n\",\n",
      " \"wash clothes. Place your dirty clothes \\\\\\n\",\n",
      " \"into a washing machine and add some detergent \\\\\\n\",\n",
      " \"as directed on the box. Your email is \\\\\\n\",\n",
      " \"aperson@farm.com and your cell is 595-942-2424. \\\\\\n\",\n",
      " \"Select the appropriate settings on your \\\\\\n\",\n",
      " \"Alexs question 953-242 \\\\\\n\",\n",
      " \"washing machine and you should be ready to \\\\\\n\",\n",
      " \"begin. Magazines are stapled reading material \\\\\\n\",\n",
      " \"made with glossy paper, and they cover a wide \\\\\\n\",\n",
      " \"variety of topics, ranging from news and \\\\\\n\",\n",
      " \"politics to business and stock market information.\\\"\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"953\\\",this_text)\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d-\\\\d\\\\d\\\\d\\\\d\\\",this_text)\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"\\\\d{3}-\\\\d{3}-\\\\d{4}\\\",this_text)\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"@[a-z]\\\",this_text)\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"@[a-z]+\\\",this_text)\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"[a-z]+@[a-z]+\\\",this_text)\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"[a-z]+@[a-z]+\\\\.[a-z]+\\\",this_text)\"\n",
      "]\n",
      "[\n",
      " \"re.findall(r\\\"\\\"\\\"[a-z]+  # user name\\n\",\n",
      " \"               @\\n\",\n",
      " \"               [a-z]+  # domain\\n\",\n",
      " \"               \\\\.\\n\",\n",
      " \"               [a-z]+  # TLD, https://en.wikipedia.org/wiki/Top-level_domain\\n\",\n",
      " \"               \\\"\\\"\\\",this_text, re.VERBOSE)\"\n",
      "]\n",
      "[\n",
      " \"re.sub(r\\\"\\\\d{4}\\\", \\\"XXXX\\\", this_text)\"\n",
      "]\n",
      "[\n",
      " \"myregex = re.compile(r\\\"\\\\d{4}\\\")\\n\",\n",
      " \"for line in list_of_lines:\\n\",\n",
      " \"    print(myregex.sub(\\\"XXXX\\\", line))\"\n",
      "]\n",
      "[\n",
      " \"a = re.compile(r\\\"\\\\d\\\\.\\\\d*\\\")\"\n",
      "]\n",
      "[\n",
      " \"a = re.compile(r\\\"\\\"\\\"\\\\d +  # the integer part\\n\",\n",
      " \"                   \\\\.    # the decimal point, aka radix: https://en.wikipedia.org/wiki/Radix\\n\",\n",
      " \"                   \\\\d *  # some fractional digits\\\"\\\"\\\", re.VERBOSE)\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import matplotlib.pylab as plt\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"from sklearn.datasets.samples_generator import make_blobs\\n\",\n",
      " \"\\n\",\n",
      " \"# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html\\n\",\n",
      " \"# Compute minimum distances between one point and a set of points.\\n\",\n",
      " \"from sklearn.metrics import pairwise_distances_argmin\"\n",
      "]\n",
      "[\n",
      " \"number_of_clusters=4\\n\",\n",
      " \"X, y_true = make_blobs(n_samples=300, centers=number_of_clusters,\\n\",\n",
      " \"                       cluster_std=0.60, random_state=0)\\n\",\n",
      " \"\\n\",\n",
      " \"rng = np.random.RandomState(42)\\n\",\n",
      " \"centers = [0, 4] + rng.randn(number_of_clusters, 2)\"\n",
      "]\n",
      "[\n",
      " \"def draw_points(ax, c, factor=1):\\n\",\n",
      " \"    ax.scatter(X[:, 0], X[:, 1], c=c, cmap='viridis',\\n\",\n",
      " \"               s=50 * factor, alpha=0.3)\\n\",\n",
      " \"    \\n\",\n",
      " \"def draw_centers(ax, centers, factor=1, alpha=1.0):\\n\",\n",
      " \"    ax.scatter(centers[:, 0], centers[:, 1],\\n\",\n",
      " \"               c=np.arange(number_of_clusters), cmap='viridis', s=200 * factor,\\n\",\n",
      " \"               alpha=alpha)\\n\",\n",
      " \"    ax.scatter(centers[:, 0], centers[:, 1],\\n\",\n",
      " \"               c='black', s=50 * factor, alpha=alpha)\\n\",\n",
      " \"\\n\",\n",
      " \"def make_ax(fig, gs):\\n\",\n",
      " \"    ax = fig.add_subplot(gs)\\n\",\n",
      " \"    ax.xaxis.set_major_formatter(plt.NullFormatter())\\n\",\n",
      " \"    ax.yaxis.set_major_formatter(plt.NullFormatter())\\n\",\n",
      " \"    return ax\"\n",
      "]\n",
      "[\n",
      " \"fig = plt.figure()\\n\",\n",
      " \"ax = plt.gca()\\n\",\n",
      " \"draw_points(ax, 'gray', factor=2)\\n\",\n",
      " \"draw_centers(ax, centers, factor=2)\\n\",\n",
      " \"print(\\\"initial centers\\\")\\n\",\n",
      " \"print(centers)\\n\",\n",
      " \"plt.title('Random Initialization')\\n\",\n",
      " \"\\n\",\n",
      " \"for indx in range(3):    \\n\",\n",
      " \"    # E-step\\n\",\n",
      " \"    fig = plt.figure()\\n\",\n",
      " \"    ax = plt.gca()\\n\",\n",
      " \"    y_pred = pairwise_distances_argmin(X, centers)\\n\",\n",
      " \"    draw_points(ax, y_pred)\\n\",\n",
      " \"    draw_centers(ax, centers)\\n\",\n",
      " \"    plt.title('Expectation step '+str(indx))\\n\",\n",
      " \"    \\n\",\n",
      " \"    # M-step\\n\",\n",
      " \"    fig = plt.figure()\\n\",\n",
      " \"    ax = plt.gca()\\n\",\n",
      " \"    new_centers = np.array([X[y_pred == i].mean(0) for i in range(number_of_clusters)])\\n\",\n",
      " \"    # where the centers are:\\n\",\n",
      " \"    print(\\\"centers \\\"+str(indx))\\n\",\n",
      " \"    print(new_centers)\\n\",\n",
      " \"    draw_points(ax, y_pred)\\n\",\n",
      " \"    draw_centers(ax, centers, alpha=0.3)\\n\",\n",
      " \"    draw_centers(ax, new_centers)\\n\",\n",
      " \"    for loop_index in range(number_of_clusters):\\n\",\n",
      " \"        ax.annotate('', new_centers[loop_index], centers[loop_index],\\n\",\n",
      " \"                     arrowprops=dict(arrowstyle='->', linewidth=1))\\n\",\n",
      " \"    plt.title('Maximization step '+str(indx))\\n\",\n",
      " \"    \\n\",\n",
      " \"    # Finish iteration\\n\",\n",
      " \"    centers = new_centers\\n\",\n",
      " \"\\n\",\n",
      " \"# Final E-step    \\n\",\n",
      " \"fig = plt.figure()\\n\",\n",
      " \"ax = plt.gca()\\n\",\n",
      " \"y_pred = pairwise_distances_argmin(X, centers)\\n\",\n",
      " \"draw_points(ax, y_pred, factor=2)\\n\",\n",
      " \"draw_centers(ax, centers, factor=2)\\n\",\n",
      " \"_=plt.title(\\\"Final Clustering\\\")\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import matplotlib.pylab as plt\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"from sklearn.datasets.samples_generator import make_blobs\\n\",\n",
      " \"\\n\",\n",
      " \"# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html\\n\",\n",
      " \"# Compute minimum distances between one point and a set of points.\\n\",\n",
      " \"from sklearn.metrics import pairwise_distances_argmin\"\n",
      "]\n",
      "[\n",
      " \"def draw_points(X,ax, c, factor=1):\\n\",\n",
      " \"    ax.scatter(X[:, 0], X[:, 1], c=c, cmap='viridis',\\n\",\n",
      " \"               s=50 * factor, alpha=0.3)\\n\",\n",
      " \"    \\n\",\n",
      " \"def draw_centers(number_of_clusters,ax, centers, factor=1, alpha=1.0):\\n\",\n",
      " \"    ax.scatter(centers[:, 0], centers[:, 1],\\n\",\n",
      " \"               c=np.arange(number_of_clusters), cmap='viridis', s=200 * factor,\\n\",\n",
      " \"               alpha=alpha)\\n\",\n",
      " \"    ax.scatter(centers[:, 0], centers[:, 1],\\n\",\n",
      " \"               c='black', s=50 * factor, alpha=alpha)\\n\",\n",
      " \"\\n\",\n",
      " \"def make_ax(fig, gs):\\n\",\n",
      " \"    ax = fig.add_subplot(gs)\\n\",\n",
      " \"    ax.xaxis.set_major_formatter(plt.NullFormatter())\\n\",\n",
      " \"    ax.yaxis.set_major_formatter(plt.NullFormatter())\\n\",\n",
      " \"    return ax\\n\"\n",
      "]\n",
      "[\n",
      " \"def kmeans(number_of_clusters,num_iterations,plot_figs):\\n\",\n",
      " \"    # generate blobs of data points\\n\",\n",
      " \"    X, y_true = make_blobs(n_samples=300, centers=4,\\n\",\n",
      " \"                           cluster_std=0.60, random_state=0)\\n\",\n",
      " \"\\n\",\n",
      " \"    # initialize random centroid locations\\n\",\n",
      " \"    rng = np.random.RandomState(42)\\n\",\n",
      " \"    centers = [0, number_of_clusters] + rng.randn(number_of_clusters, 2)\\n\",\n",
      " \"\\n\",\n",
      " \"    if plot_figs:\\n\",\n",
      " \"        fig = plt.figure()\\n\",\n",
      " \"        ax = plt.gca()\\n\",\n",
      " \"        draw_points(X,ax, 'gray', factor=1)\\n\",\n",
      " \"        draw_centers(number_of_clusters,ax, centers, factor=1)\\n\",\n",
      " \"        print(\\\"initial centers\\\")\\n\",\n",
      " \"        print(centers)\\n\",\n",
      " \"        plt.title('Random Initialization')\\n\",\n",
      " \"\\n\",\n",
      " \"    for indx in range(num_iterations):    \\n\",\n",
      " \"        # E-step\\n\",\n",
      " \"        y_pred = pairwise_distances_argmin(X, centers) # given a set of point and a set of centroids, determine which group each point is most likely in\\n\",\n",
      " \"        if plot_figs:\\n\",\n",
      " \"            fig = plt.figure()\\n\",\n",
      " \"            ax = plt.gca()\\n\",\n",
      " \"            draw_points(X,ax, y_pred)\\n\",\n",
      " \"            draw_centers(number_of_clusters,ax, centers)\\n\",\n",
      " \"            plt.title('Expectation step '+str(indx))\\n\",\n",
      " \"    \\n\",\n",
      " \"        # M-step\\n\",\n",
      " \"        new_centers = np.array([X[y_pred == i].mean(0) for i in range(number_of_clusters)])\\n\",\n",
      " \"        if plot_figs:\\n\",\n",
      " \"            fig = plt.figure()\\n\",\n",
      " \"            ax = plt.gca()\\n\",\n",
      " \"            # where the centers are:\\n\",\n",
      " \"            print(\\\"centers \\\"+str(indx))\\n\",\n",
      " \"            print(new_centers)\\n\",\n",
      " \"            draw_points(X,ax, y_pred)\\n\",\n",
      " \"            draw_centers(number_of_clusters,ax, centers, alpha=0.3)\\n\",\n",
      " \"            draw_centers(number_of_clusters,ax, new_centers)\\n\",\n",
      " \"            for loop_index in range(number_of_clusters):\\n\",\n",
      " \"                ax.annotate('', new_centers[loop_index], centers[loop_index],\\n\",\n",
      " \"                             arrowprops=dict(arrowstyle='->', linewidth=1))\\n\",\n",
      " \"            plt.title('Maximization step '+str(indx))\\n\",\n",
      " \"    \\n\",\n",
      " \"        # Finish iteration\\n\",\n",
      " \"        centers = new_centers\\n\",\n",
      " \"\\n\",\n",
      " \"    # Final E-step    \\n\",\n",
      " \"    y_pred = pairwise_distances_argmin(X, centers)\\n\",\n",
      " \"    if plot_figs:\\n\",\n",
      " \"        fig = plt.figure()\\n\",\n",
      " \"        ax = plt.gca()\\n\",\n",
      " \"        draw_points(X, ax, y_pred, factor=1)\\n\",\n",
      " \"        draw_centers(number_of_clusters,ax, centers, factor=1)\\n\",\n",
      " \"        _=plt.title(\\\"Final Clustering\\\")\\n\",\n",
      " \"    return X,y_pred,centers\"\n",
      "]\n",
      "[\n",
      " \"X,y_pred,centers = kmeans(number_of_clusters=3,num_iterations=4,plot_figs=False)\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"list_of_sumofsq=[]\\n\",\n",
      " \"num_clstr_range=range(1,7)\\n\",\n",
      " \"for number_of_clusters in num_clstr_range:\\n\",\n",
      " \"    X,y_pred,centers = kmeans(number_of_clusters,num_iterations=4,plot_figs=False)\\n\",\n",
      " \"    sumofsq=0\\n\",\n",
      " \"    centroid_indx=-1\\n\",\n",
      " \"    for this_centroid_coords in centers:\\n\",\n",
      " \"        centroid_indx+=1\\n\",\n",
      " \"        for this_dot in X[y_pred==centroid_indx]:\\n\",\n",
      " \"            # https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\\n\",\n",
      " \"            sumofsq+=np.linalg.norm(this_dot - this_centroid_coords)**2\\n\",\n",
      " \"    list_of_sumofsq.append(sumofsq)\\n\",\n",
      " \"    print(number_of_clusters,sumofsq)\"\n",
      "]\n",
      "[\n",
      " \"_=plt.scatter(num_clstr_range,y=list_of_sumofsq)\\n\",\n",
      " \"_=plt.xlabel('k',fontsize=12)\\n\",\n",
      " \"_=plt.ylabel('sum of squared errors',fontsize=12)\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"url = \\\"https://data.cityofnewyork.us/api/views/w7w3-xahh/rows.csv?accessType=DOWNLOAD&bom=true&format=true\\\"\\n\",\n",
      " \"response = urllib.request.urlopen(url)\\n\",\n",
      " \"df = pd.read_csv(response,low_memory=False)\"\n",
      "]\n",
      "[\n",
      " \"\\n\",\n",
      " \"import pandas as pd\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"import chardet\\n\",\n",
      " \"#import ascii\\n\",\n",
      " \"import string\\n\",\n",
      " \"import sys\\n\",\n",
      " \"import re\\n\",\n",
      " \"import csv\\n\",\n",
      " \"import os\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"from pandas.plotting import lag_plot\\n\",\n",
      " \"import matplotlib.patches as mpatches\\n\",\n",
      " \"from matplotlib import rc\\n\",\n",
      " \"from urllib.request import urlopen\\n\",\n",
      " \"import networkx as net\"\n",
      "]\n",
      "[\n",
      " \"url = \\\"https://data.cityofnewyork.us/api/views/w7w3-xahh/rows.csv?accessType=DOWNLOAD&bom=true&format=true\\\"\\n\",\n",
      " \"#response = urllib.request.urlopen(url)\\n\",\n",
      " \"df = pd.read_csv(url,low_memory=False)\"\n",
      "]\n",
      "[\n",
      " \"df['Industry'].unique().sum()\"\n",
      "]\n",
      "[\n",
      " \"g=df.groupby(\\\"Industry\\\")\"\n",
      "]\n",
      "[\n",
      " \"df['Industry'].value_counts().plot(kind='barh',figsize=(20,20))\"\n",
      "]\n",
      "[\n",
      " \"df.shape\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import time\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"import pandas\\n\",\n",
      " \"print('Pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"start_time = time.time()\\n\",\n",
      " \"loans_2007 = pandas.read_csv('https://github.com/yhat/demo-lending-club/blob/master/model/LoanStats3a.csv?raw=true', \\n\",\n",
      " \"                             skiprows=1, low_memory=False)\\n\",\n",
      " \"print(time.time() - start_time,'seconds')\"\n",
      "]\n",
      "[\n",
      " \"loans_2007.shape\"\n",
      "]\n",
      "[\n",
      " \"list(loans_2007.columns)\"\n",
      "]\n",
      "[\n",
      " \"print(loans_2007.dtypes)\"\n",
      "]\n",
      "[\n",
      " \"loans_2007['installment'].hist()\\n\",\n",
      " \"plt.xlabel('installment')\\n\",\n",
      " \"plt.ylabel('count')\"\n",
      "]\n",
      "[\n",
      " \"plt.xlabel('installment')\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import random\"\n",
      "]\n",
      "[\n",
      " \"#sequence_of_interest = (1, 2, 1, 1)\\n\",\n",
      " \"#sequence_of_interest = (1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2) #0.5 seconds\\n\",\n",
      " \"sequence_of_interest = (1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2)\\n\",\n",
      " \"print(\\\"length of sequence: \\\",len(sequence_of_interest))\"\n",
      "]\n",
      "[\n",
      " \"def create_random_sequence(len_of_seq,number_of_coin_flips):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    create a sequence of given length containing integer values, either 1 or 2\\n\",\n",
      " \"\\n\",\n",
      " \"    this is intended to represent the outcome of a sequence of coin flips\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    this_run=[]\\n\",\n",
      " \"    for indx in range(len_of_seq):\\n\",\n",
      " \"        coin = random.randint(1, 2) # random coin flip results in 1 or 2\\n\",\n",
      " \"        number_of_coin_flips+=1\\n\",\n",
      " \"        this_run.append(coin)       # add coin flip result to list\\n\",\n",
      " \"    return this_run,number_of_coin_flips\"\n",
      "]\n",
      "[\n",
      " \"experiment_result, running_count = create_random_sequence(5,0)\\n\",\n",
      " \"print('experiment result:',experiment_result)\\n\",\n",
      " \"print('running count:',running_count)\"\n",
      "]\n",
      "[\n",
      " \"import timeit\\n\",\n",
      " \"start_time = timeit.default_timer()\\n\",\n",
      " \"\\n\",\n",
      " \"number_of_coin_flips=0\\n\",\n",
      " \"this_run=[]  # store the results of the random coin flips in a list\\n\",\n",
      " \"while True:\\n\",\n",
      " \"    this_run,number_of_coin_flips = create_random_sequence(len(sequence_of_interest),\\n\",\n",
      " \"                                                           number_of_coin_flips)\\n\",\n",
      " \"    found_match=False\\n\",\n",
      " \"    for flip_indx in range(len(sequence_of_interest)):\\n\",\n",
      " \"#            print(\\\"flip index is \\\",flip_indx)\\n\",\n",
      " \"#            print(sequence_of_interest)\\n\",\n",
      " \"        if (sequence_of_interest[flip_indx] == this_run[flip_indx]):\\n\",\n",
      " \"            found_match=True\\n\",\n",
      " \"        else: # patterns don't match\\n\",\n",
      " \"            found_match=False\\n\",\n",
      " \"#                print(\\\"patterns don't match\\\",this_run)\\n\",\n",
      " \"            this_run=[]\\n\",\n",
      " \"            break\\n\",\n",
      " \"    if (found_match==True):\\n\",\n",
      " \"        print(this_run)\\n\",\n",
      " \"        break\\n\",\n",
      " \"\\n\",\n",
      " \"print(\\\"number of coin flips:\\\",number_of_coin_flips)\\n\",\n",
      " \"elapsed = timeit.default_timer() - start_time\\n\",\n",
      " \"print(str(elapsed) + \\\" seconds\\\")\"\n",
      "]\n",
      "[\n",
      " \"import random\\n\",\n",
      " \"import timeit\\n\",\n",
      " \"import time\"\n",
      "]\n",
      "[\n",
      " \"def create_sequence_of_interest(length_of_seq):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    create a sequence of given length containing integer values, either 1 or 2\\n\",\n",
      " \"\\n\",\n",
      " \"    this is intended to represent the outcome of a sequence of coin flips\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    list_of_flips=[]\\n\",\n",
      " \"    for this_flip in range(length_of_seq):\\n\",\n",
      " \"        list_of_flips.append(random.randint(1, 2))\\n\",\n",
      " \"    return tuple(list_of_flips)\"\n",
      "]\n",
      "[\n",
      " \"create_sequence_of_interest(5)\"\n",
      "]\n",
      "[\n",
      " \"def find_match(sequence_of_interest):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    given a sequence of integers (outcome of either 1 or 2), \\n\",\n",
      " \"    flip a coin until that sequence is found\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    start_time = timeit.default_timer()\\n\",\n",
      " \"    number_of_coin_flips=0\\n\",\n",
      " \"    this_run=[]  # store the results of the random coin flips in a list\\n\",\n",
      " \"    while True:\\n\",\n",
      " \"        coin = random.randint(1, 2) # random coin flip results in 1 or 2\\n\",\n",
      " \"        number_of_coin_flips+=1\\n\",\n",
      " \"        this_run.append(coin)       # add coin flip result to list\\n\",\n",
      " \"        if (len(this_run)==len(sequence_of_interest)):\\n\",\n",
      " \"            found_match=False\\n\",\n",
      " \"            for flip_indx in range(len(sequence_of_interest)):\\n\",\n",
      " \"                if (sequence_of_interest[flip_indx] == this_run[flip_indx]):\\n\",\n",
      " \"                    found_match=True\\n\",\n",
      " \"                else: # patterns don't match\\n\",\n",
      " \"                    found_match=False\\n\",\n",
      " \"                    this_run=[]\\n\",\n",
      " \"                    break\\n\",\n",
      " \"            if (found_match==True):\\n\",\n",
      " \"                #print(this_run)\\n\",\n",
      " \"                break\\n\",\n",
      " \"\\n\",\n",
      " \"    #print(\\\"number of coin flips:\\\",number_of_coin_flips)\\n\",\n",
      " \"    elapsed = timeit.default_timer() - start_time\\n\",\n",
      " \"    #print(str(elapsed) + \\\" seconds\\\")\\n\",\n",
      " \"    return elapsed, number_of_coin_flips\"\n",
      "]\n",
      "[\n",
      " \"seq_of_interest = create_sequence_of_interest(5)\\n\",\n",
      " \"print(seq_of_interest)\\n\",\n",
      " \"find_match(seq_of_interest)\"\n",
      "]\n",
      "[\n",
      " \"number_of_experiments_per_loop=1000\\n\",\n",
      " \"start_length=3\\n\",\n",
      " \"end_length=10\"\n",
      "]\n",
      "[\n",
      " \"list_of_result_dicts=[]\\n\",\n",
      " \"start_time=time.time()\\n\",\n",
      " \"for length_of_seq in range(3,10):\\n\",\n",
      " \"    for this_try in range(number_of_experiments_per_loop):\\n\",\n",
      " \"        sequence_of_interest = create_sequence_of_interest(length_of_seq)\\n\",\n",
      " \"#        print(sequence_of_interest)\\n\",\n",
      " \"\\n\",\n",
      " \"        elapsed, number_of_coin_flips = find_match(sequence_of_interest)\\n\",\n",
      " \"#        print(elapsed,\\\"seconds\\\")\\n\",\n",
      " \"#        print(number_of_coin_flips)\\n\",\n",
      " \"        this_result={}\\n\",\n",
      " \"        this_result['elapsed time in seconds'] = elapsed\\n\",\n",
      " \"        this_result['number of flips'] = number_of_coin_flips\\n\",\n",
      " \"        this_result['sequence length'] = length_of_seq\\n\",\n",
      " \"        list_of_result_dicts.append(this_result)\\n\",\n",
      " \"        \\n\",\n",
      " \"print('elapsed:',time.time()-start_time,'seconds')\"\n",
      "]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"pandas.__version__\"\n",
      "]\n",
      "[\n",
      " \"results_df = pandas.DataFrame(list_of_result_dicts)\\n\",\n",
      " \"results_df.head(10)\"\n",
      "]\n",
      "[\n",
      " \"results_df.shape\"\n",
      "]\n",
      "[\n",
      " \"import matplotlib\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"matplotlib.__version__\"\n",
      "]\n",
      "[\n",
      " \"results_df.plot.scatter(x='sequence length',y='elapsed time in seconds')\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"results_df.plot.scatter(x='sequence length',y='number of flips')\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"results_df.groupby('sequence length').describe()\"\n",
      "]\n",
      "[\n",
      " \"import seaborn\\n\",\n",
      " \"print('seaborn',seaborn.__version__)\"\n",
      "]\n",
      "[\n",
      " \"# https://seaborn.pydata.org/generated/seaborn.stripplot.html\\n\",\n",
      " \"_=seaborn.stripplot(x=\\\"sequence length\\\", \\n\",\n",
      " \"                        y=\\\"number of flips\\\", \\n\",\n",
      " \"                        data=results_df)\"\n",
      "]\n",
      "[\n",
      " \"# https://seaborn.pydata.org/generated/seaborn.boxplot.html\\n\",\n",
      " \"_=seaborn.boxplot(x=\\\"sequence length\\\", \\n\",\n",
      " \"                        y=\\\"number of flips\\\", \\n\",\n",
      " \"                        data=results_df)\"\n",
      "]\n",
      "[\n",
      " \"# https://seaborn.pydata.org/generated/seaborn.violinplot.html\\n\",\n",
      " \"_ = seaborn.violinplot(x=\\\"sequence length\\\", \\n\",\n",
      " \"                        y=\\\"number of flips\\\", \\n\",\n",
      " \"                        data=results_df,\\n\",\n",
      " \"                        showmeans=True,\\n\",\n",
      " \"                        showextrema=True,\\n\",\n",
      " \"                        showmedians=True)\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import random\\n\",\n",
      " \"import pandas\\n\",\n",
      " \"print('pandas',pandas.__version__)\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"import numpy\\n\",\n",
      " \"print('numpy',numpy.__version__)\"\n",
      "]\n",
      "[\n",
      " \"count_heads=0\\n\",\n",
      " \"num_flips=50\\n\",\n",
      " \"for indx in range(num_flips):\\n\",\n",
      " \"    coin = random.randint(1, 2) # random coin flip results in 1 (tail) or 2 (heads)\\n\",\n",
      " \"    count_heads += coin-1\"\n",
      "]\n",
      "[\n",
      " \"count_heads\"\n",
      "]\n",
      "[\n",
      " \"df = pandas.DataFrame([{'head count':count_heads,'tail count':num_flips-count_heads}])\\n\",\n",
      " \"df\"\n",
      "]\n",
      "[\n",
      " \"_=df.plot.bar()\\n\",\n",
      " \"#_=plt.xticks(numpy.arange(2), ('Heads','Tails'))\\n\",\n",
      " \"_=plt.xticks([])\\n\",\n",
      " \"plt.savefig('coin_flips')\"\n",
      "]\n",
      "[\n",
      " \"count_heads=0\\n\",\n",
      " \"num_flips=500\\n\",\n",
      " \"for indx in range(num_flips):\\n\",\n",
      " \"    coin = random.randint(1, 2) # random coin flip results in 1 or 2\\n\",\n",
      " \"    count_heads += coin-1\"\n",
      "]\n",
      "[\n",
      " \"count_heads\"\n",
      "]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"df = pandas.DataFrame([{'head count':count_heads,'tail count':num_flips-count_heads}])\\n\",\n",
      " \"df\"\n",
      "]\n",
      "[\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"import numpy\\n\",\n",
      " \"_=df.plot.bar()\\n\",\n",
      " \"#_=plt.xticks(numpy.arange(2), ('Heads','Tails'))\\n\",\n",
      " \"_=plt.xticks([])\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import pandas as pd\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"import chardet\\n\",\n",
      " \"#import ascii\\n\",\n",
      " \"import string\\n\",\n",
      " \"import sys\\n\",\n",
      " \"import re\\n\",\n",
      " \"import csv\\n\",\n",
      " \"import os\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"from pandas.plotting import lag_plot\"\n",
      "]\n",
      "[\n",
      " \"path_to_file = \\\"DOHMH_HIV_AIDS_Annual_Report.csv\\\"\\n\",\n",
      " \"data = pd.read_csv(path_to_file, encoding='utf-8')\"\n",
      "]\n",
      "[\n",
      " \"data\"\n",
      "]\n",
      "[\n",
      " \"data.dtypes\"\n",
      "]\n",
      "[\n",
      " \"data.Year = pd.to_datetime(data.Year, format='%Y')\"\n",
      "]\n",
      "[\n",
      " \"data.dtypes\"\n",
      "]\n",
      "[\n",
      " \"data.shape\"\n",
      "]\n",
      "[\n",
      " \"data.isnull()\"\n",
      "]\n",
      "[\n",
      " \"data.isnull().sum()\"\n",
      "]\n",
      "[\n",
      " \"data.info()\"\n",
      "]\n",
      "[\n",
      " \"data.head()\"\n",
      "]\n",
      "[\n",
      " \"data.tail()\"\n",
      "]\n",
      "[\n",
      " \"data1=data.replace(to_replace =99999, \\n\",\n",
      " \"                 value =0) \\n\",\n",
      " \"data1\"\n",
      "]\n",
      "[\n",
      " \"data1.describe()\"\n",
      "]\n",
      "[\n",
      " \"g1 = data1.groupby( [ \\\"Year\\\", \\\"HIV-related death rate\\\"] ).sum()\\n\",\n",
      " \"g1\"\n",
      "]\n",
      "[\n",
      " \"sum1=data1.groupby('Year', as_index=False).agg({\\\"HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sum1\"\n",
      "]\n",
      "[\n",
      " \"sum2=data1.groupby('Year', as_index=False).agg({\\\"Non-HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sum2\"\n",
      "]\n",
      "[\n",
      " \"sum3=data1.groupby('Year', as_index=False).agg({\\\"Death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sum3\"\n",
      "]\n",
      "[\n",
      " \"df=[sum1,sum2,sum3]\\n\",\n",
      " \"#df\\n\",\n",
      " \"result = pd.concat(df)\\n\",\n",
      " \"result\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"df1 = pd.merge(sum1, sum2, on='Year',how='inner')\\n\",\n",
      " \"df1\"\n",
      "]\n",
      "[\n",
      " \"data2 = pd.merge(df1, sum3, on='Year',how='inner')\\n\",\n",
      " \"data2\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"lag_plot(data1['Non-HIV-related death rate'])\\n\",\n",
      " \"#Death rate\\n\",\n",
      " \"#s2=lag_plot(data1['Death rate'])\"\n",
      "]\n",
      "[\n",
      " \"lag_plot(data1['Death rate'])\"\n",
      "]\n",
      "[\n",
      " \"lag_plot(data1['HIV-related death rate'])\"\n",
      "]\n",
      "[\n",
      " \"n_groups = 5\\n\",\n",
      " \"fig, ax = plt.subplots(figsize=(13,5))\\n\",\n",
      " \"bar_width = 0.20\\n\",\n",
      " \"opacity = 0.8\\n\",\n",
      " \"index = np.arange(n_groups)\\n\",\n",
      " \"\\n\",\n",
      " \"rects1 = plt.bar(index - bar_width, data2['HIV-related death rate'], bar_width,alpha=opacity,color='b',label='HIV-related death rate')\\n\",\n",
      " \"rects2 = plt.bar(index, data2['Non-HIV-related death rate'], bar_width,alpha=opacity,color='g',label='Non-HIV-related death rate')\\n\",\n",
      " \"rects3 = plt.bar(index + bar_width, data2['Death rate'], bar_width,alpha=opacity,color='r',label='Death rate')\\n\",\n",
      " \"plt.xlabel('years')\\n\",\n",
      " \"plt.ylabel('death count')\\n\",\n",
      " \"plt.title('Deaths')\\n\",\n",
      " \"plt.xticks(index + bar_width, ('2011', '2012', '2013', '2014', '2015'))\\n\",\n",
      " \"plt.legend()\\n\",\n",
      " \"plt.tight_layout()\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"fig, ax = plt.subplots(figsize=(10,5))\\n\",\n",
      " \"plt.plot('Year' ,'HIV-related death rate', data=data2, marker='', markerfacecolor='blue', color='skyblue', linewidth=4)\\n\",\n",
      " \"plt.plot( 'Year','Non-HIV-related death rate',  data=data2, marker='', color='red', linewidth=3)\\n\",\n",
      " \"plt.plot('Year', 'Death rate', data=data2, marker='', color='olive', linewidth=3, label=\\\"Death rate\\\")\\n\",\n",
      " \"plt.xlabel('years')\\n\",\n",
      " \"plt.ylabel('death count')\\n\",\n",
      " \"plt.title('Deaths')\\n\",\n",
      " \"plt.legend()\\n\"\n",
      "]\n",
      "[\n",
      " \"#if data1['year'] = 2011 \\n\",\n",
      " \"\\n\",\n",
      " \"data_f = data1[data1.Year == '2011']\\n\",\n",
      " \"data_f.head(5)\"\n",
      "]\n",
      "[\n",
      " \"data_f\\n\",\n",
      " \"#data1['Year'] = pd.to_year(data1['Year'])\"\n",
      "]\n",
      "[\n",
      " \"data_f['Year'].head()\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"!pip install nltk\"\n",
      "]\n",
      "[\n",
      " \"import nltk\"\n",
      "]\n",
      "[\n",
      " \"nltk.download('stopwords')\"\n",
      "]\n",
      "[\n",
      " \"nltk.download('punkt')\"\n",
      "]\n",
      "[\n",
      " \"this_text=\\\"I go to the store. A car is parked. \\\\\\n\",\n",
      " \"Many cars are parked or moving. Some are blue. \\\\\\n\",\n",
      " \"Some are tan. They have windows. In the store, \\\\\\n\",\n",
      " \"there are items for sale. These include such \\\\\\n\",\n",
      " \"things as soap, detergent, magazines, and lettuce. \\\\\\n\",\n",
      " \"You can enhance your life with these products. \\\\\\n\",\n",
      " \"Soap can be used for bathing, be it in a bathtub \\\\\\n\",\n",
      " \"or in a shower. My email address is myname@sc.edu. \\\\\\n\",\n",
      " \"Apply the soap to your body and rinse. My phone \\\\\\n\",\n",
      " \"number is 452-953-2942. Detergent is used to \\\\\\n\",\n",
      " \"wash clothes. Place your dirty clothes \\\\\\n\",\n",
      " \"into a washing machine and add some detergent \\\\\\n\",\n",
      " \"as directed on the box. Your email is \\\\\\n\",\n",
      " \"aperson@farm.com and your cell is 595-942-2424. \\\\\\n\",\n",
      " \"Select the appropriate settings on your \\\\\\n\",\n",
      " \"washing machine and you should be ready to \\\\\\n\",\n",
      " \"begin. Magazines are stapled reading material \\\\\\n\",\n",
      " \"made with glossy paper, and they cover a wide \\\\\\n\",\n",
      " \"variety of topics, ranging from news and \\\\\\n\",\n",
      " \"politics to business and stock market information.\\\"\"\n",
      "]\n",
      "[\n",
      " \"from nltk.tokenize import sent_tokenize,word_tokenize \"\n",
      "]\n",
      "[\n",
      " \"sent_tokenize_list = sent_tokenize(this_text)\\n\",\n",
      " \"sent_tokenize_list\"\n",
      "]\n",
      "[\n",
      " \"for this_sent in sent_tokenize_list:\\n\",\n",
      " \"    word_tokens = word_tokenize(this_sent) \\n\",\n",
      " \"    print(word_tokens)\"\n",
      "]\n",
      "[\n",
      " \"from nltk.corpus import stopwords\\n\",\n",
      " \"en_stops = set(stopwords.words('english'))\"\n",
      "]\n",
      "[\n",
      " \"en_stops\"\n",
      "]\n",
      "[\n",
      " \"len(en_stops)\"\n",
      "]\n",
      "[\n",
      " \"for this_sent in sent_tokenize_list:\\n\",\n",
      " \"    filtered_sentence = [] \\n\",\n",
      " \"    word_tokens = word_tokenize(this_sent) \\n\",\n",
      " \"    for w in word_tokens: \\n\",\n",
      " \"        if w not in en_stops: \\n\",\n",
      " \"            filtered_sentence.append(w) \\n\",\n",
      " \"    print(filtered_sentence)\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"#Importing Pandas, numpy, chardet, ascii, string, sys, re, csv,pyplot\\n\",\n",
      " \"\\n\",\n",
      " \"import pandas as pd\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"import chardet\\n\",\n",
      " \"import ascii\\n\",\n",
      " \"import string\\n\",\n",
      " \"import sys\\n\",\n",
      " \"import re\\n\",\n",
      " \"import csv\\n\",\n",
      " \"import os\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"\\n\",\n",
      " \"#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n\",\n",
      " \"#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\"\n",
      "]\n",
      "[\n",
      " \"#Loading the data into pandas data frame\\n\",\n",
      " \"\\n\",\n",
      " \"#https://www.statcrunch.com/app/index.php?dataid=2188684 ---> csv data used\\n\",\n",
      " \"\\n\",\n",
      " \"path_to_file = \\\"Project_Movies_Data.csv\\\"\\n\",\n",
      " \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\",\n",
      " \"#loading the csv file to a variable 'data'\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\\n\"\n",
      "]\n",
      "[\n",
      " \"#to view the data type of each column\\n\",\n",
      " \"\\n\",\n",
      " \"data.dtypes\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\"\n",
      "]\n",
      "[\n",
      " \"data.shape  #Finding the rows and columns of the dataset\"\n",
      "]\n",
      "[\n",
      " \"#viewing the complete data present in the data frame\\n\",\n",
      " \"\\n\",\n",
      " \"data\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\\n\"\n",
      "]\n",
      "[\n",
      " \"#checking for null values\\n\",\n",
      " \"\\n\",\n",
      " \"data.isnull()\"\n",
      "]\n",
      "[\n",
      " \"#checking for any null values present in the data set\\n\",\n",
      " \"data.isnull().sum()\"\n",
      "]\n",
      "[\n",
      " \"#checking the total number of null values present in the data set\\n\",\n",
      " \"data.isnull().sum().sum()\"\n",
      "]\n",
      "[\n",
      " \"#summary of the dataframe\\n\",\n",
      " \"\\n\",\n",
      " \"data.info()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://www.google.com/search?q=data.info()+in+python&rlz=1C1SQJL_enUS862US862&oq=data.info()&aqs=chrome.1.69i57j0l5.2068j0j4&sourceid=chrome&ie=UTF-8\"\n",
      "]\n",
      "[\n",
      " \"#checking for any missing values in the data\\n\",\n",
      " \"\\n\",\n",
      " \"missing_values = [\\\"n/a\\\", \\\"na\\\", \\\"--\\\"]\\n\",\n",
      " \"data = pd.read_csv(\\\"Project_Movies_Data.csv\\\", na_values = missing_values)\\n\",\n",
      " \"missing_values\\n\",\n",
      " \"data.isnull().values.any()\"\n",
      "]\n",
      "[\n",
      " \"data.head()\\n\",\n",
      " \"\\n\",\n",
      " \"#consistent data for data headings\"\n",
      "]\n",
      "[\n",
      " \"#checking for any special characters in the data\\n\",\n",
      " \"\\n\",\n",
      " \"alphabet = string.ascii_letters+string.punctuation\\n\",\n",
      " \"\\n\",\n",
      " \"data.Movie.str.strip(alphabet).astype(bool).any()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/53375623/pandas-keep-default-na-false-does-not-work\\n\",\n",
      " \"#citation:https://www.geeksforgeeks.org/python-program-check-string-contains-special-character/\"\n",
      "]\n",
      "[\n",
      " \"#listing out the special characters in the data\\n\",\n",
      " \"\\n\",\n",
      " \"data.Movie.str.strip(alphabet)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/53375623/pandas-keep-default-na-false-does-not-work\\n\",\n",
      " \"#citation:https://www.geeksforgeeks.org/python-program-check-string-contains-special-character/\"\n",
      "]\n",
      "[\n",
      " \"#no null values \\n\",\n",
      " \"null_values=data.columns[data.isnull().any()]\\n\",\n",
      " \"data[null_values].isnull().sum()\\n\",\n",
      " \"null_values\\n\"\n",
      "]\n",
      "[\n",
      " \"#no null values\\n\",\n",
      " \"data.Movie.isnull().sum()\"\n",
      "]\n",
      "[\n",
      " \"#to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values\\n\",\n",
      " \"data.describe()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://minimaxir.com/2018/07/imdb-data-analysis/\"\n",
      "]\n",
      "[\n",
      " \"data.Movie.head()\\n\"\n",
      "]\n",
      "[\n",
      " \"data.Day.head()\"\n",
      "]\n",
      "[\n",
      " \"data.Year.head()\"\n",
      "]\n",
      "[\n",
      " \"data['Movie'].is_unique\"\n",
      "]\n",
      "[\n",
      " \"movies = data['Movie'].unique().tolist()\\n\",\n",
      " \"movies\\n\"\n",
      "]\n",
      "[\n",
      " \"#plotting a scatter plot against Budget invested in the movie and the Domestic gross earned by a particular film.\\n\",\n",
      " \"fig, ax = plt.subplots()\\n\",\n",
      " \"ax.scatter(data['Budget($M)'], data['Domestic Gross($M)'])\\n\",\n",
      " \"ax.set_title('Movies Dataset for budget and domestic gross(without plot values)')\\n\",\n",
      " \"ax.set_xlabel('Budget($M)')\\n\",\n",
      " \"ax.set_ylabel('Domestic Gross($M)')\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/32319166/highlight-last-data-point-in-scatter-plot-with-pandas\\n\",\n",
      " \"#citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e\\n\",\n",
      " \"#citation:https://plot.ly/python/line-and-scatter/\"\n",
      "]\n",
      "[\n",
      " \"#plotting a scatter plot against Budget invested in the movie and the Domestic gross earned by a particular film.\\n\",\n",
      " \"#scatter plots with the values\\n\",\n",
      " \"plt.clf()\\n\",\n",
      " \"\\n\",\n",
      " \"# using Budget and Domestic gross\\n\",\n",
      " \"xs = (data['Budget($M)'])\\n\",\n",
      " \"ys = (data['Domestic Gross($M)'])\\n\",\n",
      " \"plt.figure(figsize=(10,10))\\n\",\n",
      " \"plt.scatter(xs,ys,s=70, c='darkred')\\n\",\n",
      " \"\\n\",\n",
      " \"# zip joins x and y coordinates in pairs\\n\",\n",
      " \"for x,y in zip(xs,ys):\\n\",\n",
      " \"\\n\",\n",
      " \"    label = \\\"{:.0f}\\\".format(y)\\n\",\n",
      " \"\\n\",\n",
      " \"    plt.annotate(label,# this is the text\\n\",\n",
      " \"                 (x,y), # this is the point to label\\n\",\n",
      " \"                 textcoords=\\\"offset points\\\", # how to position the text\\n\",\n",
      " \"                 xytext=(0,10), # distance from text to points (x,y)\\n\",\n",
      " \"                 ha='center') # horizontal alignment can be left, right or center\\n\",\n",
      " \"\\n\",\n",
      " \"plt.xticks(np.arange(0,500,100))\\n\",\n",
      " \"plt.yticks(np.arange(0,1000,100))\\n\",\n",
      " \"plt.title('Movies Dataset(A scatter plot for budget and the domestic gross)(scatterplot with plot values)')\\n\",\n",
      " \"plt.xlabel('Budget of the movie in millions')\\n\",\n",
      " \"plt.ylabel('Domestic Gross of the movie in millions')\\n\",\n",
      " \"\\n\",\n",
      " \"plt.show()\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation: https://pythonprogramming.net/legends-titles-labels-matplotlib-tutorial/\\n\",\n",
      " \"#Citation: https://stackoverflow.com/questions/29188757/matplotlib-specify-format-of-floats-for-tick-lables\\n\",\n",
      " \"#Citation: http://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples\\n\",\n",
      " \"#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html\"\n",
      "]\n",
      "[\n",
      " \"#plotting a scatter plot against Budget invested in the movie and the Worldwide Gross earned by a particular film.\\n\",\n",
      " \"fig, ax = plt.subplots()\\n\",\n",
      " \"ax.scatter(data['Budget($M)'], data['Worldwide Gross($M)'])\\n\",\n",
      " \"ax.set_title('Movies Dataset for budget and worldwide gross(scatter plot without plot values)')\\n\",\n",
      " \"ax.set_xlabel('Budget($M)')\\n\",\n",
      " \"ax.set_ylabel('Worldwide Gross($M)')\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation:https://stackoverflow.com/questions/32319166/highlight-last-data-point-in-scatter-plot-with-pandas\\n\",\n",
      " \"#Citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e\\n\",\n",
      " \"#Citation:https://plot.ly/python/line-and-scatter/\"\n",
      "]\n",
      "[\n",
      " \"#plotting a scatter plot against Budget invested in the movie and the Worldwide Gross earned by a particular film.\\n\",\n",
      " \"#scatter plots with the values\\n\",\n",
      " \"plt.clf()\\n\",\n",
      " \"#using budget and worldwide gross\\n\",\n",
      " \"xs = (data['Budget($M)'])\\n\",\n",
      " \"ys = (data['Worldwide Gross($M)'])\\n\",\n",
      " \"plt.figure(figsize=(10,10))\\n\",\n",
      " \"plt.scatter(xs,ys,s=70, c='darkblue')\\n\",\n",
      " \"\\n\",\n",
      " \"# zip joins x and y coordinates in pairs\\n\",\n",
      " \"for x,y in zip(xs,ys):\\n\",\n",
      " \"\\n\",\n",
      " \"    label = \\\"{:.2f}\\\".format(y)\\n\",\n",
      " \"\\n\",\n",
      " \"    plt.annotate(label, # this is the text\\n\",\n",
      " \"                 (x,y), # this is the point to label\\n\",\n",
      " \"                 textcoords=\\\"offset points\\\", # how to position the text\\n\",\n",
      " \"                 xytext=(0,10), # distance from text to points (x,y)\\n\",\n",
      " \"                 ha='center') # horizontal alignment can be left, right or center\\n\",\n",
      " \"\\n\",\n",
      " \"plt.xticks(np.arange(0,600,100))\\n\",\n",
      " \"plt.yticks(np.arange(0,4000,1000))\\n\",\n",
      " \"plt.title('Movies Dataset(A scatter plot for budget and the WorldWide gross)(scatter plot with plot values)')\\n\",\n",
      " \"plt.xlabel('Budget of the movie in millions')\\n\",\n",
      " \"plt.ylabel('Worldwide Gross of the movie in millions')\\n\",\n",
      " \"\\n\",\n",
      " \"plt.show()\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation: https://pythonprogramming.net/legends-titles-labels-matplotlib-tutorial/\\n\",\n",
      " \"#Citation: https://stackoverflow.com/questions/29188757/matplotlib-specify-format-of-floats-for-tick-lables\\n\",\n",
      " \"#Citation: http://queirozf.com/entries/add-labels-and-text-to-matplotlib-plots-annotation-examples\\n\",\n",
      " \"#citation:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html\"\n",
      "]\n",
      "[\n",
      " \"#Histogram for the Worldwide Gross in millions\\n\",\n",
      " \"# create figure and axis\\n\",\n",
      " \"fig, ax = plt.subplots()\\n\",\n",
      " \"# plot histogram\\n\",\n",
      " \"ax.hist(data['Worldwide Gross($M)'])\\n\",\n",
      " \"# set title and labels\\n\",\n",
      " \"ax.set_title('Movies Dataset')\\n\",\n",
      " \"ax.set_ylabel('Movies Count')\\n\",\n",
      " \"ax.set_xlabel('Worldwide Gross($M)')\\n\",\n",
      " \"data.hist(column=\\\"Worldwide Gross($M)\\\", bins=5)\\n\",\n",
      " \"plt.title('Movies Dataset(A scatter plot for Movie count and the Domestic profits earned)')\\n\",\n",
      " \"plt.ylabel('Number of movies')\\n\",\n",
      " \"plt.xlabel('Domestic Gross of the movie in millions')\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation:https://wellsr.com/python/python-create-histogram-from-pandas-dataframe/\\n\",\n",
      " \"#Citation:https://www.mathworks.com/matlabcentral/answers/436999-histogram-x-axis-range\\n\",\n",
      " \"#Citation:https://stackoverflow.com/questions/6986986/bin-size-in-matplotlib-histogram\\n\",\n",
      " \"#Citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e\"\n",
      "]\n",
      "[\n",
      " \"#Histogram for the Domestic Gross in millions\\n\",\n",
      " \"fig, ax = plt.subplots()\\n\",\n",
      " \"# plot histogram\\n\",\n",
      " \"ax.hist(data['Domestic Gross($M)'])\\n\",\n",
      " \"# set title and labels\\n\",\n",
      " \"ax.set_title('Movies Dataset')\\n\",\n",
      " \"ax.set_ylabel('Movies count')\\n\",\n",
      " \"ax.set_xlabel('Domestic Gross($M)')\\n\",\n",
      " \"data.hist(column=\\\"Domestic Gross($M)\\\", bins=5)\\n\",\n",
      " \"plt.title('Movies Dataset(A scatter plot for Movie count and the profits earned)')\\n\",\n",
      " \"plt.ylabel('Number of movies')\\n\",\n",
      " \"plt.xlabel('Worldwide Gross of the movie in millions')\\n\",\n",
      " \"\\n\",\n",
      " \"#Citation:https://wellsr.com/python/python-create-histogram-from-pandas-dataframe/\\n\",\n",
      " \"#Citation:https://www.mathworks.com/matlabcentral/answers/436999-histogram-x-axis-range\\n\",\n",
      " \"#Citation:https://stackoverflow.com/questions/6986986/bin-size-in-matplotlib-histogram\\n\",\n",
      " \"#Citation:https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"#Loading the data into pandas data frame through URL\\n\",\n",
      " \"\\n\",\n",
      " \"#https://catalog.data.gov/dataset/dohmh-hiv-aids-annual-report --> CSV File used\\n\",\n",
      " \"\\n\",\n",
      " \"path_to_file = \\\"https://data.cityofnewyork.us/api/views/fju2-rdad/rows.csv?accessType=DOWNLOAD\\\"\\n\",\n",
      " \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://www.geeksforgeeks.org/different-ways-to-import-csv-file-in-pandas/\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"!pip install nltk\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n\",\n",
      " \"#citation:https://docs.python.org/3/howto/unicode.html\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/35715353/glob-iglob-to-find-all-txt-files-in-all-sub-directories-yields-error\"\n",
      "]\n",
      "[\n",
      " \"pip install --pre python-docx\"\n",
      "]\n",
      "[\n",
      " \"pip install glob2\"\n",
      "]\n",
      "[\n",
      " \"import math\\n\",\n",
      " \"import string\\n\",\n",
      " \"import os\\n\",\n",
      " \"import re\\n\",\n",
      " \"import os.path\\n\",\n",
      " \"import glob\\n\",\n",
      " \"import fnmatch\\n\",\n",
      " \"import nltk\\n\",\n",
      " \"import errno\\n\",\n",
      " \"from nltk.corpus import stopwords \\n\",\n",
      " \"from nltk.tokenize import word_tokenize \\n\",\n",
      " \"from nltk.tokenize import RegexpTokenizer\\n\",\n",
      " \"from docx import Document\\n\",\n",
      " \"from nltk.tokenize import sent_tokenize,word_tokenize \\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n\",\n",
      " \"#citation:https://docs.python.org/3/howto/unicode.html\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/35715353/glob-iglob-to-find-all-txt-files-in-all-sub-directories-yields-error\"\n",
      "]\n",
      "[\n",
      " \"#downloading stop words\\n\",\n",
      " \"\\n\",\n",
      " \"nltk.download('stopwords')\\n\",\n",
      " \"\\n\",\n",
      " \"#reference: http://localhost:8889/notebooks/tfidf.ipynb (lecture notebook)\"\n",
      "]\n",
      "[\n",
      " \"nltk.download('punkt')\\n\",\n",
      " \"\\n\",\n",
      " \"#reference: http://localhost:8889/notebooks/tfidf.ipynb (lecture notebook)\"\n",
      "]\n",
      "[\n",
      " \"#reading the text file and converting it into lower case\\n\",\n",
      " \"\\n\",\n",
      " \"file=open('how_rubber_goods_are_made.txt','r',encoding='UTF-8')\\n\",\n",
      " \"file1= file.read()\\n\",\n",
      " \"#print(file1)\\n\",\n",
      " \"string1= file1\\n\",\n",
      " \"file_after=string1.lower()\\n\",\n",
      " \"file_after\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html\\n\",\n",
      " \"#citation:https://realpython.com/working-with-files-in-python/#listing-all-files-in-a-directory\\n\",\n",
      " \"#citation:https://www.quora.com/How-do-I-read-mutiple-txt-files-from-folder-in-python\"\n",
      "]\n",
      "[\n",
      " \"#the file after converting to lowercase, removing the punctuation marks, tokenizing the string, removing digits, removing stop words\\n\",\n",
      " \"\\n\",\n",
      " \"my_string= file_after\\n\",\n",
      " \"table=str.maketrans(\\\"!,.?\\u2019\\u201d\\u201c':\\\", 9*\\\" \\\") \\n\",\n",
      " \"file_1=my_string.translate(table)\\n\",\n",
      " \"#file_1\\n\",\n",
      " \"sent_tokenize_list = sent_tokenize(file_1)\\n\",\n",
      " \"sent_tokenize_list\\n\",\n",
      " \"for this_file in sent_tokenize_list:\\n\",\n",
      " \"    word_tokens = word_tokenize(this_file) \\n\",\n",
      " \"   # print(word_tokens)\\n\",\n",
      " \"for n in sent_tokenize_list:\\n\",\n",
      " \"    filtered_sentence = [] \\n\",\n",
      " \"    word_tokens = word_tokenize(n) \\n\",\n",
      " \"    for w in word_tokens: \\n\",\n",
      " \"        if w not in file_stop: \\n\",\n",
      " \"            filtered_sentence.append(w) \\n\",\n",
      " \"    print(filtered_sentence)\\n\",\n",
      " \"    result = ''.join([i for i in filtered_sentence if not i.isdigit()])\\n\",\n",
      " \"    result\\n\",\n",
      " \"    \\n\",\n",
      " \"#citation: https://www.youtube.com/watch?v=ru5jQA2Nre8\\n\",\n",
      " \"#citation: https://swcarpentry.github.io/python-novice-inflammation/06-func/index.html\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n\",\n",
      " \"#reference: http://localhost:8889/notebooks/tfidf.ipynb (lecture notebook)\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/12851791/removing-numbers-from-string/12851835\"\n",
      "]\n",
      "[\n",
      " \"file_stop = set(stopwords.words('english'))\"\n",
      "]\n",
      "[\n",
      " \"file_stop \"\n",
      "]\n",
      "[\n",
      " \"#taking the given files in a list\\n\",\n",
      " \"\\n\",\n",
      " \"file_all=['52256-0.txt',\\n\",\n",
      " \" '53031-0.txt',\\n\",\n",
      " \" '58108-0.txt',\\n\",\n",
      " \" 'blind_text.txt',\\n\",\n",
      " \" 'dr_yawn.txt',\\n\",\n",
      " \" 'how_rubber_goods_are_made.txt',\\n\",\n",
      " \" 'most_boring_ever.txt',\\n\",\n",
      " \" 'most_boring_part2.txt',\\n\",\n",
      " \" 'pg12814.txt',\\n\",\n",
      " \" 'pg14895.txt',\\n\",\n",
      " \" 'pg43994.txt',\\n\",\n",
      " \" 'random_text.txt',\\n\",\n",
      " \" 'smiley_the_bunny.txt']\"\n",
      "]\n",
      "[\n",
      " \"file_all\"\n",
      "]\n",
      "[\n",
      " \"#for all the files, converting them into lowercase, removing punctuatuon marks, removing links, replacing '_', removing phonenumbers\\n\",\n",
      " \"#removing stop words\\n\",\n",
      " \"#printing in \\\"File 1.txt\\\" : word1, word2, word3, word7 format\\n\",\n",
      " \"#saving in outputfile2.dat\\n\",\n",
      " \"\\n\",\n",
      " \"for n in file_all: \\n\",\n",
      " \"    file= open (n , 'rb')\\n\",\n",
      " \"    lines= file.read().decode('UTF-8')\\n\",\n",
      " \"    sentence= lines.lower()\\n\",\n",
      " \"    url_remove = sentence.replace(\\\"www.\\\",\\\"\\\")\\n\",\n",
      " \"    tokenizer = RegexpTokenizer(r'\\\\w+')\\n\",\n",
      " \"    tokens= tokenizer.tokenize(url_remove)\\n\",\n",
      " \"    words = list(filter(lambda n: n not in string.punctuation, tokens))\\n\",\n",
      " \"    stop_words = set(stopwords.words(\\\"english\\\"))\\n\",\n",
      " \"    cleaned=list(filter(lambda n:n not in stop_words, words))\\n\",\n",
      " \"    result = ' '.join([i for i in cleaned if not i.isdigit()])\\n\",\n",
      " \"    final_text = ''.join(result)\\n\",\n",
      " \"    finals_text = final_text.replace(\\\"_\\\", \\\" \\\")\\n\",\n",
      " \"    print('\\\"'+ n + '\\\"', ':', '\\\\n','\\\\n', finals_text, '\\\\n','\\\\n')\\n\",\n",
      " \"    saveFile = open('outputFile2.dat','w')\\n\",\n",
      " \"    saveFile.write(finals_text)\\n\",\n",
      " \"    #saveFile.close()\\n\",\n",
      " \"    \\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/7794208/how-can-i-remove-duplicate-words-in-a-string-with-python\\n\",\n",
      " \"#citation: https://www.youtube.com/watch?v=ru5jQA2Nre8\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/12851791/removing-numbers-from-string/12851835\\n\",\n",
      " \"#citation: https://swcarpentry.github.io/python-novice-inflammation/06-func/index.html\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n\",\n",
      " \"#citation: https://www.youtube.com/watch?v=O2onA4r5UaY\\n\",\n",
      " \"#citation: https://www.youtube.com/watch?v=EVhx6kjJ85U\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/12851791/removing-numbers-from-string/12851835\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/3411771/best-way-to-replace-multiple-characters-in-a-string\\n\",\n",
      " \"\\n\"\n",
      "]\n",
      "[\n",
      " \"#reading the content in the document file\\n\",\n",
      " \"\\n\",\n",
      " \"document = Document('week_10_document1.docx')\\n\",\n",
      " \"for para in document.paragraphs:\\n\",\n",
      " \"    print(para.text)\"\n",
      "]\n",
      "[\n",
      " \"#removing stop words, punctuations, phonenumbers for the first document\\n\",\n",
      " \"document = Document('week_10_document1.docx')\\n\",\n",
      " \"for para in document.paragraphs:\\n\",\n",
      " \"    #print(para.text)\\n\",\n",
      " \"    sentence= para.text.lower()\\n\",\n",
      " \"    finals1 = sentence.replace(\\\".\\\", \\\" \\\")\\n\",\n",
      " \"    finals2 = finals1.replace(\\\",\\\", \\\" \\\")\\n\",\n",
      " \"    words = list(filter(lambda n: n not in string.punctuation, tokens))\\n\",\n",
      " \"    #stop_words = set(stopwords.words('english')) \\n\",\n",
      " \"    #words = list(filter(lambda n: n not in string.punctuation, tokens))\\n\",\n",
      " \"    #stop_words = set(stopwords.words(\\\"english\\\"))\\n\",\n",
      " \"    #cleaned=list(filter(lambda n:n not in stop_words, words))\\n\",\n",
      " \"    word_tokens = word_tokenize(finals2) \\n\",\n",
      " \"    filtered_sentence1 = [w for w in word_tokens if not w in stop_words] \\n\",\n",
      " \"    #filtered_sentence10 = [] \\n\",\n",
      " \"    for w in word_tokens: \\n\",\n",
      " \"        if w not in stop_words: \\n\",\n",
      " \"            filtered_sentence.append(w) \\n\",\n",
      " \"#print(word_tokens) \\n\",\n",
      " \"print(filtered_sentence1)  \\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/25228106/how-to-extract-text-from-an-existing-docx-file-using-python-docx\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n\",\n",
      " \"#citation : https://python-docx.readthedocs.io/en/latest/user/documents.html\"\n",
      "]\n",
      "[\n",
      " \"#removing stop words, punctuations, phonenumbers for the second document\\n\",\n",
      " \"document2 = Document('week_10_document2.docx')\\n\",\n",
      " \"for p in document2.paragraphs:\\n\",\n",
      " \"    #print(p.text)\\n\",\n",
      " \"    sentence2= p.text.lower()\\n\",\n",
      " \"    finals3 = sentence.replace(\\\".\\\", \\\" \\\")\\n\",\n",
      " \"    finals4 = finals3.replace(\\\",\\\", \\\" \\\")\\n\",\n",
      " \"    stop= set(stopwords.words('english')) \\n\",\n",
      " \"    word_tokens = word_tokenize(finals4) \\n\",\n",
      " \"    filtered_sentence2 = [wt for wt in word_tokens if not wt in stop] \\n\",\n",
      " \"    #filtered_sentence2 = [] \\n\",\n",
      " \"    for wt in word_tokens: \\n\",\n",
      " \"        if wt not in stop: \\n\",\n",
      " \"            filtered_sentence.append(wt) \\n\",\n",
      " \"#print(word_tokens) \\n\",\n",
      " \"print(filtered_sentence2) \\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://stackoverflow.com/questions/25228106/how-to-extract-text-from-an-existing-docx-file-using-python-docx\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\\n\",\n",
      " \"#citation : https://python-docx.readthedocs.io/en/latest/user/documents.html\"\n",
      "]\n",
      "[\n",
      " \"#list of unique values \\n\",\n",
      " \"\\n\",\n",
      " \"list1 = filtered_sentence1\\n\",\n",
      " \"print(\\\"the unique values from 1st document is\\\") \\n\",\n",
      " \"unique(filtered_sentence1) \\n\",\n",
      " \"\\n\",\n",
      " \"list2 =filtered_sentence2\\n\",\n",
      " \"print(\\\"\\\\nthe unique values from 2nd document is\\\") \\n\",\n",
      " \"unique(filtered_sentence2)\\n\",\n",
      " \"  \\n\",\n",
      " \"#citation : https://www.geeksforgeeks.org/python-get-unique-values-list/\\n\"\n",
      "]\n",
      "[\n",
      " \"#saving the content in the file\\n\",\n",
      " \"saveFile = open('outputFile.dat','w')\\n\",\n",
      " \"saveFile.write(finals)\\n\",\n",
      " \"saveFile.close()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.youtube.com/watch?v=f6zeuk5UjuE\\n\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import openpyxl\"\n",
      "]\n",
      "[\n",
      " \"wb = openpyxl.load_workbook('week_05_in-class_activity.xlsx')\"\n",
      "]\n",
      "[\n",
      " \"# select the one available sheet; assign to a variable\\n\",\n",
      " \"sheet = wb['Sheet1']\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import math\\n\",\n",
      " \"import os\\n\",\n",
      " \"import fnmatch # https://docs.python.org/3/library/fnmatch.html\"\n",
      "]\n",
      "[\n",
      " \"def term_freq(term, list_of_words_in_document):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    computes \\\"term frequency\\\" which is the number of times a word appears in a document, \\n\",\n",
      " \"    normalized by dividing by the total number of words in document. \\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    return list_of_words_in_document.count(term)/(len(list_of_words_in_document)*1.0)\\n\",\n",
      " \"\\n\",\n",
      " \"def number_of_documents_containing(term,all_documents):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    Returns the number of documents containing word. \\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    countr=0\\n\",\n",
      " \"    for this_doc in all_documents:\\n\",\n",
      " \"        if (term in this_doc):\\n\",\n",
      " \"            countr+=1\\n\",\n",
      " \"    return countr\\n\",\n",
      " \"\\n\",\n",
      " \"def inverse_doc_freq(term, all_documents):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    computes \\\"inverse document frequency\\\" which measures \\n\",\n",
      " \"    how common a word is among all documents in corpus. \\n\",\n",
      " \"    The more common a word is, the lower its idf. \\n\",\n",
      " \"    Take the ratio of the total number of documents to the number of documents containing word, \\n\",\n",
      " \"    then take the log of that. Add 1 to the divisor to prevent division by zero.\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    return math.log(len(all_documents) / ( 1.0 + number_of_documents_containing(term, all_documents)))\\n\",\n",
      " \"\\n\",\n",
      " \"def tfidf(term, list_of_words_in_document, all_documents):\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    computes the TF-IDF score. It's the product of tf and idf.\\n\",\n",
      " \"    \\\"\\\"\\\"\\n\",\n",
      " \"    return term_freq(term, list_of_words_in_document) * inverse_doc_freq(term, all_documents)\"\n",
      "]\n",
      "[\n",
      " \"all_documents={}\\n\",\n",
      " \"all_words_from_all_docs=[]\\n\",\n",
      " \"all_terms=[]\\n\",\n",
      " \"foldr='../week_03_getting_data/essays/'\\n\",\n",
      " \"fname='*.dat'\\n\",\n",
      " \"for file_name in os.listdir(foldr):\\n\",\n",
      " \"    #print(file_name)\\n\",\n",
      " \"    if fnmatch.fnmatch(file_name, fname): # Unix shell-style wildcards\\n\",\n",
      " \"        print(file_name)\\n\",\n",
      " \"        with open(foldr+file_name,'r') as fil:\\n\",\n",
      " \"            words_in_file=fil.read().split(\\\"\\\\n\\\")\\n\",\n",
      " \"        # remove empty strings from list of words\\n\",\n",
      " \"        while \\\"\\\" in words_in_file:\\n\",\n",
      " \"            words_in_file.remove(\\\"\\\")\\n\",\n",
      " \"        # save the words per file as value in a dictionary\\n\",\n",
      " \"        all_documents[file_name]=words_in_file\\n\",\n",
      " \"        print('has',len(words_in_file),'words\\\\n')\\n\",\n",
      " \"        # also save all the words to a list\\n\",\n",
      " \"        for this_word in words_in_file:\\n\",\n",
      " \"            all_words_from_all_docs.append(this_word)\\n\",\n",
      " \"            \"\n",
      "]\n",
      "[\n",
      " \"len(all_documents)\"\n",
      "]\n",
      "[\n",
      " \"all_words_from_all_docs = list(set(all_words_from_all_docs))\\n\",\n",
      " \"len(all_words_from_all_docs)\"\n",
      "]\n",
      "[\n",
      " \"for doc_name, word_list_in_this_doc in all_documents.items():\\n\",\n",
      " \"    if (len(word_list_in_this_doc)==0):\\n\",\n",
      " \"        print(\\\"error: empty input file\\\"+doc_name)\\n\",\n",
      " \"    else:\\n\",\n",
      " \"        dic_of_terms={}\\n\",\n",
      " \"        for this_term in word_list_in_this_doc:\\n\",\n",
      " \"            dic_of_terms[this_term] = tfidf(this_term, word_list_in_this_doc, all_words_from_all_docs)\\n\",\n",
      " \"        #print(dic_of_terms)\\n\",\n",
      " \"        print('\\\\n'+doc_name)\\n\",\n",
      " \"        terms_in_doc_sorted_by_score=sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\\n\",\n",
      " \"        # first 40 words by importance\\n\",\n",
      " \"        for this_tup in terms_in_doc_sorted_by_score[0:40]:\\n\",\n",
      " \"            print(this_tup)\\n\",\n",
      " \"#        for indx in range(10):\\n\",\n",
      " \"#            print(terms_in_doc_sorted_by_score[indx][0] + \\\":\\\"+str(terms_in_doc_sorted_by_score[indx][1]))\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"#Importing Pandas, numpy, chardet, ascii, string, sys, re, csv,pyplot\\n\",\n",
      " \"\\n\",\n",
      " \"import pandas as pd\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"import chardet\\n\",\n",
      " \"#import ascii\\n\",\n",
      " \"import string\\n\",\n",
      " \"import sys\\n\",\n",
      " \"import re\\n\",\n",
      " \"import csv\\n\",\n",
      " \"import os\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"from pandas.plotting import lag_plot\\n\",\n",
      " \"import matplotlib.patches as mpatches\\n\",\n",
      " \"from matplotlib import rc\\n\",\n",
      " \"\\n\",\n",
      " \"#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n\",\n",
      " \"#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\"\n",
      "]\n",
      "[\n",
      " \"#Loading the data into pandas data frame through URL\\n\",\n",
      " \"\\n\",\n",
      " \"#https://catalog.data.gov/dataset/dohmh-hiv-aids-annual-report --> CSV File used\\n\",\n",
      " \"\\n\",\n",
      " \"path_to_file = \\\"https://data.cityofnewyork.us/api/views/fju2-rdad/rows.csv?accessType=DOWNLOAD\\\"\\n\",\n",
      " \"data = pd.read_csv(path_to_file, encoding='utf-8')\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://www.geeksforgeeks.org/different-ways-to-import-csv-file-in-pandas/\"\n",
      "]\n",
      "[\n",
      " \"#viewing the complete data present in the data frame\\n\",\n",
      " \"\\n\",\n",
      " \"data\"\n",
      "]\n",
      "[\n",
      " \"#to view the data type of each column\\n\",\n",
      " \"data.dtypes\"\n",
      "]\n",
      "[\n",
      " \"#converting the Year column to date time format\\n\",\n",
      " \"\\n\",\n",
      " \"data.Year = pd.to_datetime(data.Year, format='%Y')\"\n",
      "]\n",
      "[\n",
      " \"##to view the data type of each column after conversion\\n\",\n",
      " \"\\n\",\n",
      " \"data.dtypes\"\n",
      "]\n",
      "[\n",
      " \"#Finding the rows and columns of the dataset\\n\",\n",
      " \"data.shape\"\n",
      "]\n",
      "[\n",
      " \"#checking for null values\\n\",\n",
      " \"\\n\",\n",
      " \"data.isnull()\"\n",
      "]\n",
      "[\n",
      " \"#checking for any null values present in the data set\\n\",\n",
      " \"\\n\",\n",
      " \"data.isnull().sum()\"\n",
      "]\n",
      "[\n",
      " \"#summary of the dataframe\\n\",\n",
      " \"data.info()\"\n",
      "]\n",
      "[\n",
      " \"#consistent data for data headings\\n\",\n",
      " \"\\n\",\n",
      " \"data.head()\"\n",
      "]\n",
      "[\n",
      " \"data.tail()\"\n",
      "]\n",
      "[\n",
      " \"#Data cleaning was required for HIV-related death rates and Non-HIV realted death rates, many values were entered as '99999.0'.\\n\",\n",
      " \"#hence replaced '99999.0' with '0'\\n\",\n",
      " \"data1=data.replace(to_replace =99999, \\n\",\n",
      " \"                 value =0) \\n\",\n",
      " \"data1\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\\n\",\n",
      " \"#citation :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-replace/\"\n",
      "]\n",
      "[\n",
      " \"#to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values\\n\",\n",
      " \"\\n\",\n",
      " \"data1.describe()\"\n",
      "]\n",
      "[\n",
      " \"##total count of the Years\\n\",\n",
      " \"\\n\",\n",
      " \"dfYear= pd.value_counts(data1['Year'].values)\\n\",\n",
      " \"dfYear\"\n",
      "]\n",
      "[\n",
      " \"sum1=data1.groupby('Year', as_index=False).agg({\\\"HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sum1\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"sum2=data1.groupby('Year', as_index=False).agg({\\\"Non-HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sum2\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"\\n\",\n",
      " \"sum3=data1.groupby('Year', as_index=False).agg({\\\"Death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sum3\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"#checking if the data got aligned properly.\\n\",\n",
      " \"\\n\",\n",
      " \"df=[sum1,sum2,sum3]\\n\",\n",
      " \"#df\\n\",\n",
      " \"result = pd.concat(df)\\n\",\n",
      " \"result\\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\\n\",\n",
      " \"#citation : https://datacarpentry.org/python-ecology-lesson/05-merging-data/\"\n",
      "]\n",
      "[\n",
      " \"#merging two data frames using inner join and assigning it to df1\\n\",\n",
      " \"\\n\",\n",
      " \"df1 = pd.merge(sum1, sum2, on='Year',how='inner')\\n\",\n",
      " \"df1\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns\"\n",
      "]\n",
      "[\n",
      " \"#merging two data frames (df1 and sum3) using inner join and assigning it to data2\\n\",\n",
      " \"\\n\",\n",
      " \"data2 = pd.merge(df1, sum3, on='Year',how='inner')\\n\",\n",
      " \"data2\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns\"\n",
      "]\n",
      "[\n",
      " \"#plotting a grouped bar graph to check the death rates and comparing the HIV and Non-Hiv related death rates.\\n\",\n",
      " \"\\n\",\n",
      " \"n_groups = 5\\n\",\n",
      " \"fig, ax = plt.subplots(figsize=(13,5))\\n\",\n",
      " \"bar_width = 0.20\\n\",\n",
      " \"opacity = 0.8\\n\",\n",
      " \"index = np.arange(n_groups)\\n\",\n",
      " \"\\n\",\n",
      " \"rects1 = plt.bar(index - bar_width, data2['HIV-related death rate'], bar_width,alpha=opacity,color='b',label='HIV-related death rate')\\n\",\n",
      " \"rects2 = plt.bar(index, data2['Non-HIV-related death rate'], bar_width,alpha=opacity,color='g',label='Non-HIV-related death rate')\\n\",\n",
      " \"rects3 = plt.bar(index + bar_width, data2['Death rate'], bar_width,alpha=opacity,color='r',label='Death rate')\\n\",\n",
      " \"plt.xlabel('Years', fontsize=18)\\n\",\n",
      " \"plt.ylabel('sum of death rates', fontsize=18)\\n\",\n",
      " \"plt.title('Deaths over the years (2011 - 2015)',fontsize=23)\\n\",\n",
      " \"plt.xticks(index + bar_width, ('2011', '2012', '2013', '2014', '2015'))\\n\",\n",
      " \"plt.legend()\\n\",\n",
      " \"plt.tight_layout()\\n\",\n",
      " \"plt.show()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://pythonspot.com/matplotlib-bar-chart/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/47838680/matplotlib-xticks-values-in-bar-chart\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/51080491/matplotlib-how-to-change-figsize-for-double-bar-plot/51081443\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/14270391/python-matplotlib-multiple-bars\"\n",
      "]\n",
      "[\n",
      " \"#checking if the HIV death rates are decreasing with respect to the Non-HIV death rates\\n\",\n",
      " \"#and checking if the total death rates are getting affected.\\n\",\n",
      " \"\\n\",\n",
      " \"fig, ax = plt.subplots(figsize=(10,7))\\n\",\n",
      " \"plt.plot('Year' ,'HIV-related death rate', data=data2, marker='', markerfacecolor='blue', color='skyblue', linewidth=4)\\n\",\n",
      " \"plt.plot( 'Year','Non-HIV-related death rate',  data=data2, marker='', color='red', linewidth=3)\\n\",\n",
      " \"plt.plot('Year', 'Death rate', data=data2, marker='', color='olive', linewidth=3, label=\\\"Death rate\\\")\\n\",\n",
      " \"plt.xlabel('years', fontsize=18)\\n\",\n",
      " \"plt.ylabel('sum of death rates ', fontsize=18)\\n\",\n",
      " \"plt.title('Increase or Decrease of deaths ', fontsize=23)\\n\",\n",
      " \"plt.legend()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://python-graph-gallery.com/line-chart/\\n\",\n",
      " \"#citation: http://www.learningaboutelectronics.com/Articles/How-to-change-the-line-width-of-a-graph-plot-in-matplotlib-with-Python.php\"\n",
      "]\n",
      "[\n",
      " \"#flitering the data1 dataframe with Borough == 'Bronx' and assigning to data_fBB\\n\",\n",
      " \"\\n\",\n",
      " \"data_fBB = data1[data1.Borough == 'Bronx']\\n\",\n",
      " \"data_fBB\\n\"\n",
      "]\n",
      "[\n",
      " \"#flitering the data1 dataframe with Borough == 'Brooklyn' and assigning to data_fBR\\n\",\n",
      " \"\\n\",\n",
      " \"data_fBR = data1[data1.Borough == 'Brooklyn']\\n\",\n",
      " \"data_fBR\"\n",
      "]\n",
      "[\n",
      " \"#flitering the data1 dataframe with Borough == 'Manhattan' and assigning to data_fBM\\n\",\n",
      " \"\\n\",\n",
      " \"data_fBM = data1[data1.Borough == 'Manhattan']\\n\",\n",
      " \"data_fBM\"\n",
      "]\n",
      "[\n",
      " \"#flitering the data1 dataframe with Borough == 'Queens' and assigning to data_fBQ\\n\",\n",
      " \"\\n\",\n",
      " \"data_fBQ = data1[data1.Borough == 'Queens']\\n\",\n",
      " \"data_fBQ\"\n",
      "]\n",
      "[\n",
      " \"#flitering the data1 dataframe with Borough == 'Staten Island' and assigning to data_fBS\\n\",\n",
      " \"data_fBS = data1[data1.Borough == 'Staten Island']\\n\",\n",
      " \"data_fBS\\n\"\n",
      "]\n",
      "[\n",
      " \"#flitering the data1 dataframe with Borough == 'All' and assigning to data_fBA\\n\",\n",
      " \"# All Boroughs are the Boroughs for which the names aren't mentioned in the list\\n\",\n",
      " \"\\n\",\n",
      " \"data_fBA = data1[data1.Borough == 'All']\\n\",\n",
      " \"data_fBA\"\n",
      "]\n",
      "[\n",
      " \"#total count of the Boroughs\\n\",\n",
      " \"dfBorough= pd.value_counts(data1['Borough'].values)\\n\",\n",
      " \"dfBorough\"\n",
      "]\n",
      "[\n",
      " \"\\n\",\n",
      " \"sumB=data1.groupby('Borough', as_index=False).agg({\\\"Death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sumB\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"sumNH=data1.groupby('Borough', as_index=False).agg({\\\"Non-HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sumNH\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"sumH=data1.groupby('Borough', as_index=False).agg({\\\"HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sumH\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"#merging two data frames (sumH and sumNH) using inner join and assigning it to df2\\n\",\n",
      " \"#merging two data frames (df2 and sumB) using inner join and assigning it to data3\\n\",\n",
      " \"\\n\",\n",
      " \"df2 = pd.merge(sumH, sumNH, on='Borough',how='inner')\\n\",\n",
      " \"data3 = pd.merge(df2, sumB, on='Borough',how='inner')\\n\",\n",
      " \"data3\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns\"\n",
      "]\n",
      "[\n",
      " \"#plotting a stacked graph with the dataframe(data3)\\n\",\n",
      " \"#checking which all Boroughs have more HIV death rates when compared with the non HIV death rates and the effect of actual death rates.\\n\",\n",
      " \"colors = [\\\"blue\\\", \\\"yellow\\\",\\\"red\\\"]\\n\",\n",
      " \"x=['All','Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\",\n",
      " \"#names = ['All','Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\",\n",
      " \"#xticks(np.arange(5), ('All','Bronx','Brookyln','Manhattan','Queens','Staten Island'))\\n\",\n",
      " \"rect=data3.loc[:,['HIV-related death rate','Non-HIV-related death rate','Death rate']].plot.bar(stacked=True, color=colors ,figsize=(10,7), fontsize=13)\\n\",\n",
      " \"plt.xticks(range(6),['All','Bronx','Brookyln','Manhattan','Queens','Staten Island'])\\n\",\n",
      " \"plt.xlabel('Boroughs',fontsize=18) \\n\",\n",
      " \"plt.ylabel('sum of death rates', fontsize=18) \\n\",\n",
      " \"plt.title('Deaths with respect to boroughs', fontsize=23)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/bar_stacked.html\\n\",\n",
      " \"#citation: https://www.smartsheet.com/stacked-bar-chart-graph\\n\",\n",
      " \"#citation: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xticks.html\"\n",
      "]\n",
      "[\n",
      " \"#total count of the Races\\n\",\n",
      " \"\\n\",\n",
      " \"dfRace= pd.value_counts(data1['Race'].values)\\n\",\n",
      " \"dfRace\"\n",
      "]\n",
      "[\n",
      " \"sumAll=data1.groupby('Race', as_index=False).agg({\\\"Death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sumAll\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"sumHiv=data1.groupby('Race', as_index=False).agg({\\\"HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sumHiv\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"sumN_Hiv=data1.groupby('Race', as_index=False).agg({\\\"Non-HIV-related death rate\\\": \\\"sum\\\"})\\n\",\n",
      " \"sumN_Hiv\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\\n\",\n",
      " \"#citation :https://www.geeksforgeeks.org/python-pandas-dataframe-add/\\n\",\n",
      " \"#citation: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\"\n",
      "]\n",
      "[\n",
      " \"#merging two data frames (sumAll and sumHiv) using inner join and assigning it to df_Race\\n\",\n",
      " \"#merging two data frames (df_Race and sumN_Hiv) using inner join and assigning it to data_Race\\n\",\n",
      " \"\\n\",\n",
      " \"df_Race = pd.merge(sumAll, sumHiv, on='Race',how='inner')\\n\",\n",
      " \"data_Race = pd.merge(df_Race, sumN_Hiv, on='Race',how='inner')\\n\",\n",
      " \"data_Race\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns\"\n",
      "]\n",
      "[\n",
      " \"#dropping the row 'All' because it is the sum of the death rates of all the races.\\n\",\n",
      " \"\\n\",\n",
      " \"data_race_1=data_Race.drop([data_Race.index[0]])\\n\",\n",
      " \"data_race_1\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\"\n",
      "]\n",
      "[\n",
      " \"#plotting a stacked graph with the dataframe(data_race_1)\\n\",\n",
      " \"#checking which all Races have more HIV death rates when compared with the non HIV death rates and the effect of actual death rates.\\n\",\n",
      " \"\\n\",\n",
      " \"colors = [\\\"black\\\",\\\"grey\\\", \\\"#b2beb5\\\"]\\n\",\n",
      " \"#x=['Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\",\n",
      " \"#plt.xlabel('x') \\n\",\n",
      " \"#names = ['All','Bronx','Brookyln','Manhattan','Queens','Staten Island']\\n\",\n",
      " \"#xticks(np.arange(5), ('All','Bronx','Brookyln','Manhattan','Queens','Staten Island'))\\n\",\n",
      " \"rect=data_race_1.loc[:,['HIV-related death rate','Non-HIV-related death rate','Death rate']].plot.bar(stacked=True, color=colors, label = x ,figsize=(10,10),fontsize=18)\\n\",\n",
      " \"plt.xticks(range(5),['Asian/Pacific Islander','Black','Latino/Hispanic','Other/Unknown','White'],fontsize=13)\\n\",\n",
      " \"plt.xlabel('Races', fontsize=18) \\n\",\n",
      " \"plt.ylabel('sum of death rates', fontsize=18) \\n\",\n",
      " \"plt.title('Deaths with respect to race', fontsize=23)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/bar_stacked.html\\n\",\n",
      " \"#citation: https://www.smartsheet.com/stacked-bar-chart-graph\\n\",\n",
      " \"#citation: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xticks.html\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"print('pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"df = pandas.read_csv(\\\"https://people.sc.fsu.edu/~jburkardt/data/csv/mlb_players.csv\\\", \\n\",\n",
      " \"                     skiprows=[1035],\\n\",\n",
      " \"                     skipinitialspace=True)\\n\",\n",
      " \"df.head()\"\n",
      "]\n",
      "[\n",
      " \"for this_column in df.columns:\\n\",\n",
      " \"    print(\\\"==== \\\",this_column,\\\"has\\\",df[this_column].nunique(),\\\"unique entries ====\\\")\\n\",\n",
      " \"    print(df[this_column].value_counts().head(10))\"\n",
      "]\n",
      "[\n",
      " \"!cat myfunctions.py\"\n",
      "]\n",
      "[\n",
      " \"%run myfunctions.py\"\n",
      "]\n",
      "[\n",
      " \"unique_entries_in_frame(df,5)\"\n",
      "]\n",
      "[\n",
      " \"%load myfunctions.py\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"#Importing Pandas, numpy, chardet, string, sys, re, csv,pyplot\\n\",\n",
      " \"import pandas as pd\\n\",\n",
      " \"import numpy as np\\n\",\n",
      " \"import chardet\\n\",\n",
      " \"import string\\n\",\n",
      " \"import sys\\n\",\n",
      " \"import re\\n\",\n",
      " \"import csv\\n\",\n",
      " \"import os\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"import pandas\\n\",\n",
      " \"import sqlite3\\n\",\n",
      " \"print('pandas',pandas.__version__)\\n\",\n",
      " \"#citations: https://jakevdp.github.io/PythonDataScienceHandbook/04.05-histograms-and-binnings.html\\n\",\n",
      " \"#citations: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\"\n",
      "]\n",
      "[\n",
      " \"#Loading the data into pandas data frame\\n\",\n",
      " \"#First 7 days of data extracted and saved in RollingSystemDemand_20191015_1556.csv \\n\",\n",
      " \"\\n\",\n",
      " \"path_to_file = \\\"RollingSystemDemand_20191015_1556.csv\\\"\\n\",\n",
      " \"data1 = pd.read_csv(path_to_file, encoding='utf-8')\\n\",\n",
      " \"#loading the csv file to a variable 'data1'\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\"\n",
      "]\n",
      "[\n",
      " \"#viewing the complete data present in the data frame\\n\",\n",
      " \"\\n\",\n",
      " \"data1\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\\n\"\n",
      "]\n",
      "[\n",
      " \"#dropping the last row with (FTR\\t2000\\tNaN) and storing the newly created dataset into df1\\n\",\n",
      " \"\\n\",\n",
      " \"df1=data1.drop(data1.index[2000])\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\"\n",
      "]\n",
      "[\n",
      " \"df1\"\n",
      "]\n",
      "[\n",
      " \"#Loading the data into pandas data frame\\n\",\n",
      " \"#next 7 days of data extracted and saved in RollingSystemDemand_20191015_1556 (1).csv\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"path_to_file = \\\"RollingSystemDemand_20191015_1556 (1).csv\\\"\\n\",\n",
      " \"data2 = pd.read_csv(path_to_file, encoding='utf-8')\\n\",\n",
      " \"\\n\",\n",
      " \"#loading the csv file to a variable 'data2'\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\"\n",
      "]\n",
      "[\n",
      " \"data2\"\n",
      "]\n",
      "[\n",
      " \"#dropping the last row with (FTR\\t2000\\tNaN) and storing the newly created dataset into df2\\n\",\n",
      " \"\\n\",\n",
      " \"\\n\",\n",
      " \"df2=data2.drop(data2.index[2000])\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\"\n",
      "]\n",
      "[\n",
      " \"df2\"\n",
      "]\n",
      "[\n",
      " \"#concatinating two dataframes\\n\",\n",
      " \"\\n\",\n",
      " \"result = [df1, df2]\\n\",\n",
      " \"\\n\",\n",
      " \"data = pd.concat(result)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\"\n",
      "]\n",
      "[\n",
      " \"data\"\n",
      "]\n",
      "[\n",
      " \"#checking for the data types\\n\",\n",
      " \"data.dtypes\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\"\n",
      "]\n",
      "[\n",
      " \"#changing the column names\\n\",\n",
      " \"\\n\",\n",
      " \"data.columns=['time_and_date','demand_level']\\n\",\n",
      " \"\\n\",\n",
      " \"#https://cmdlinetips.com/2018/03/how-to-change-column-names-and-row-indexes-in-pandas/\"\n",
      "]\n",
      "[\n",
      " \"data.head()\"\n",
      "]\n",
      "[\n",
      " \"#importing the datetime libraries\\n\",\n",
      " \"\\n\",\n",
      " \"from datetime import datetime\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime\"\n",
      "]\n",
      "[\n",
      " \"#converting the time of measurement to datetime format\\n\",\n",
      " \"\\n\",\n",
      " \"data['time_and_date']=pd.to_datetime(data['time_and_date'],format='%Y%m%d%H%M%S',errors=\\\"coerce\\\")\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime\"\n",
      "]\n",
      "[\n",
      " \"data.shape\"\n",
      "]\n",
      "[\n",
      " \"data\"\n",
      "]\n",
      "[\n",
      " \"#maximum of the datetime\\n\",\n",
      " \"\\n\",\n",
      " \"data['time_and_date'].max()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime\"\n",
      "]\n",
      "[\n",
      " \"#minimum of the datetime\\n\",\n",
      " \"\\n\",\n",
      " \"data['time_and_date'].min()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://stackoverflow.com/questions/9744775/how-to-convert-integer-timestamp-to-python-datetime\"\n",
      "]\n",
      "[\n",
      " \"#checking if the datatype got changed to datetime for \\\"time of measurement\\\".\\n\",\n",
      " \"\\n\",\n",
      " \"data.dtypes\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\"\n",
      "]\n",
      "[\n",
      " \"#plotting bar graph has 24 bars, each bar is the average across all days for that hour\\n\",\n",
      " \"\\n\",\n",
      " \"data.groupby(data[\\\"time_and_date\\\"].dt.hour)['demand_level'].mean().plot(kind=\\\"bar\\\")\\n\",\n",
      " \"\\n\",\n",
      " \"plt.setp(plt.gca().get_xticklabels(), rotation=40, fontsize=12)\\n\",\n",
      " \"plt.setp(plt.gca().get_yticklabels(), fontsize=12)\\n\",\n",
      " \"plt.xlabel('hours', fontsize=10)\\n\",\n",
      " \"plt.ylabel('average for that hour for all taken days', fontsize=10)\\n\",\n",
      " \"plt.title('power consumption per that particular hour for all days')\\n\",\n",
      " \"\\n\",\n",
      " \"plt.show()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://python-graph-gallery.com/10-barplot-with-number-of-observation/\\n\",\n",
      " \"#citation:https://chrisalbon.com/python/data_wrangling/group_pandas_data_by_hour_of_the_day/\"\n",
      "]\n",
      "[\n",
      " \"#plotting bar graph has 24*(number of days) bars. i.e., 24*(14 days) = 336 bars\\n\",\n",
      " \"\\n\",\n",
      " \"data.groupby([data[\\\"time_and_date\\\"].dt.day,data[\\\"time_and_date\\\"].dt.hour])['demand_level'].mean().plot(kind=\\\"bar\\\",figsize=(100,20))\\n\",\n",
      " \"#fig = plt.figure(figsize=(30,70))\\n\",\n",
      " \"#plt.xlabel('hour', fontsize=14)\\n\",\n",
      " \"#plt.ylabel('count', fontsize=14)\\n\",\n",
      " \"\\n\",\n",
      " \"plt.xlabel('days', fontsize=100)\\n\",\n",
      " \"plt.ylabel('hour average', fontsize=100)\\n\",\n",
      " \"plt.title('electricity consumption per that particular hour for each day',fontsize=100)\\n\",\n",
      " \"\\n\",\n",
      " \"plt.show()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation:https://python-graph-gallery.com/10-barplot-with-number-of-observation/\\n\",\n",
      " \"#citation:https://chrisalbon.com/python/data_wrangling/group_pandas_data_by_hour_of_the_day/\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"print('pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"!head RollingSystemDemand_20190314_0043.csv\"\n",
      "]\n",
      "[\n",
      " \"dframe = pandas.read_csv(\\\"RollingSystemDemand_20190314_0043.csv\\\")\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"#dataframe.reset_index() # not sure why this didn't work\\n\",\n",
      " \"dframe.index=range(len(dframe))\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe.columns=['time of measurement','demand level']\"\n",
      "]\n",
      "[\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"import matplotlib\\n\",\n",
      " \"print('matplotlib',matplotlib.__version__)\\n\",\n",
      " \"import matplotlib.pyplot as plt\"\n",
      "]\n",
      "[\n",
      " \"dframe.plot(x='time of measurement', y='demand level')\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"plt.scatter(dframe['time of measurement'], dframe['demand level'])\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"from datetime import datetime\"\n",
      "]\n",
      "[\n",
      " \"!head -n 3 RollingSystemDemand_20190314_0043.csv\"\n",
      "]\n",
      "[\n",
      " \"# see https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior for details\\n\",\n",
      " \"\\n\",\n",
      " \"# using the first entry from the output of !head above \\n\",\n",
      " \"datetime_object = datetime.strptime('20190301000000', '%Y%m%d%H%M%S')\\n\",\n",
      " \"datetime_object.strftime('%Y-%m-%d %H:%M:%S') # print the result in a human readable format\"\n",
      "]\n",
      "[\n",
      " \"dframe['time of measurement'] = pandas.to_datetime(dframe['time of measurement'],\\n\",\n",
      " \"                                                   format='%Y%m%d%H%M%S')\"\n",
      "]\n",
      "[\n",
      " \"dframe.dtypes\"\n",
      "]\n",
      "[\n",
      " \"dframe['time of measurement']=dframe['time of measurement'].to_string()\"\n",
      "]\n",
      "[\n",
      " \"dframe['time of measurement'] = pandas.to_datetime(dframe['time of measurement'],\\n\",\n",
      " \"                                                   format='%Y%m%d%H%M%S')\"\n",
      "]\n",
      "[\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"print('pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"!head RollingSystemDemand_20180901_0129.csv\"\n",
      "]\n",
      "[\n",
      " \"dframe = pandas.read_csv(\\\"RollingSystemDemand_20180901_0129.csv\\\")\"\n",
      "]\n",
      "[\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe.dtypes\"\n",
      "]\n",
      "[\n",
      " \"dframe.index=range(len(dframe))\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe.dtypes\"\n",
      "]\n",
      "[\n",
      " \"dframe.columns=['time of measurement','demand level']\"\n",
      "]\n",
      "[\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe['time of measurement'][0:5]\"\n",
      "]\n",
      "[\n",
      " \"dframe['time of measurement'][0:5].to_string()\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"print('pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"!head RollingSystemDemand_20180901_0129.csv\"\n",
      "]\n",
      "[\n",
      " \"dframe = pandas.read_csv(\\\"RollingSystemDemand_20180901_0129.csv\\\",\\n\",\n",
      " \"                         index_col=False)\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe = pandas.read_csv(\\\"RollingSystemDemand_20180901_0129.csv\\\",\\n\",\n",
      " \"                         index_col=False,\\n\",\n",
      " \"                         skiprows=1)\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe = pandas.read_csv(\\\"RollingSystemDemand_20180901_0129.csv\\\",\\n\",\n",
      " \"                         index_col=False,\\n\",\n",
      " \"                         skiprows=1, \\n\",\n",
      " \"                         header=None)\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe.columns=['VD','time of measurement','value']\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe.dtypes\"\n",
      "]\n",
      "[\n",
      " \"# https://stackoverflow.com/questions/17950374/converting-a-column-within-pandas-dataframe-from-int-to-string\\n\",\n",
      " \"dframe['time of measurement']=dframe['time of measurement'].apply(str)\"\n",
      "]\n",
      "[\n",
      " \"dframe.dtypes\"\n",
      "]\n",
      "[\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"pandas.to_datetime(dframe['time of measurement'],format='%Y%m%d%H%M%S')\"\n",
      "]\n",
      "[\n",
      " \"pandas.to_datetime(dframe['time of measurement'],format='%Y%m%d%H%M%S',errors=\\\"coerce\\\")\"\n",
      "]\n",
      "[\n",
      " \"!tail RollingSystemDemand_20180901_0129.csv\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"print('pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"!head RollingSystemDemand_20180901_0129.csv\"\n",
      "]\n",
      "[\n",
      " \"dframe = pandas.read_csv(\\\"RollingSystemDemand_20180901_0129.csv\\\",\\n\",\n",
      " \"                         index_col=False,\\n\",\n",
      " \"                         skiprows=1,\\n\",\n",
      " \"                         skipfooter=1, engine='python',\\n\",\n",
      " \"                         header=None)\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe.columns=['VD','time of measurement','value']\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe.tail()\"\n",
      "]\n",
      "[\n",
      " \"dframe.dtypes\"\n",
      "]\n",
      "[\n",
      " \"dframe['time of measurement']=pandas.to_datetime(dframe['time of measurement'].astype(str),format='%Y%m%d%H%M%S')\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"import matplotlib\\n\",\n",
      " \"import matplotlib.pyplot as plt\\n\",\n",
      " \"matplotlib.__version__\"\n",
      "]\n",
      "[\n",
      " \"plt.plot_date(dframe['time of measurement'], dframe['value'])\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[\n",
      " \"plt.plot_date(dframe['time of measurement'], dframe['value'])\\n\",\n",
      " \"plt.xticks(rotation='vertical')\\n\",\n",
      " \"plt.tick_params(labelsize=14)\\n\",\n",
      " \"plt.xlabel('date',fontsize=14)\\n\",\n",
      " \"plt.ylabel('demand [MegaWatts]',fontsize=14)\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"import pandas\\n\",\n",
      " \"print('pandas',pandas.__version__)\"\n",
      "]\n",
      "[\n",
      " \"dframe = pandas.read_csv(\\\"RollingSystemDemand_20190314_0043.csv\\\",\\n\",\n",
      " \"                         index_col=False,\\n\",\n",
      " \"                         skiprows=1,\\n\",\n",
      " \"                         skipfooter=1,\\n\",\n",
      " \"                         engine='python',\\n\",\n",
      " \"                         header=None)\\n\",\n",
      " \"\\n\",\n",
      " \"dframe.columns=['VD','time of measurement','value']\\n\",\n",
      " \"\\n\",\n",
      " \"dframe.head()\"\n",
      "]\n",
      "[\n",
      " \"dframe['time of measurement']=pandas.to_datetime(\\n\",\n",
      " \"                                  dframe['time of measurement'].astype(str),\\n\",\n",
      " \"                                  format='%Y%m%d%H%M%S')\"\n",
      "]\n",
      "[\n",
      " \"import matplotlib\\n\",\n",
      " \"print('maplotlib',matplotlib.__version__)\\n\",\n",
      " \"import matplotlib.pyplot as plt\"\n",
      "]\n",
      "[\n",
      " \"plt.plot_date(dframe['time of measurement'], dframe['value'])\\n\",\n",
      " \"plt.xticks(rotation='vertical')\\n\",\n",
      " \"plt.tick_params(labelsize=14)\\n\",\n",
      " \"plt.xlabel('date',fontsize=14)\\n\",\n",
      " \"plt.ylabel('demand [MegaWatts]',fontsize=14)\\n\",\n",
      " \"plt.show()\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"#Importing the required libraries\\n\",\n",
      " \"\\n\",\n",
      " \"import glob\\n\",\n",
      " \"import io, os, sys, types\\n\",\n",
      " \"from IPython import get_ipython\\n\",\n",
      " \"from nbformat import read\\n\",\n",
      " \"from IPython.core.interactiveshell import InteractiveShell\\n\",\n",
      " \"import json\\n\",\n",
      " \"import re\\n\",\n",
      " \"import pandas as pd\"\n",
      "]\n",
      "[\n",
      " \"#Reading all the .ipynb files and printing the cell type and the source present in the JSON file\\n\",\n",
      " \"#dict1={}\\n\",\n",
      " \"for name in glob.glob('*.ipynb'):\\n\",\n",
      " \"    with open (name, encoding=\\\"utf8\\\", errors='coerce') as f:\\n\",\n",
      " \"        data = json.load(f)\\n\",\n",
      " \"        for cell in data ['cells']:\\n\",\n",
      " \"                print(cell['cell_type'],cell['source'])\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/2186525/how-to-use-glob-to-find-files-recursively\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/20186344/ipynb-import-another-ipynb-file\\n\",\n",
      " \"#citation: https://www.youtube.com/watch?v=7GFXm8HUD7s\\n\",\n",
      " \"#citation: https://pymotw.com/2/glob/\\n\"\n",
      "]\n",
      "[\n",
      " \"#printing the complete data\\n\",\n",
      " \"\\n\",\n",
      " \"print(data)\"\n",
      "]\n",
      "[\n",
      " \"\\n\",\n",
      " \"for cell in data ['cells']:\\n\",\n",
      " \"            dict1=cell['cell_type']=='code',cell['source']\\n\",\n",
      " \"           \"\n",
      "]\n",
      "[\n",
      " \"dict1\"\n",
      "]\n",
      "[\n",
      " \"type(data)\"\n",
      "]\n",
      "[\n",
      " \"#the cell_type with \\\"code\\\" are getting displayed\\n\",\n",
      " \"lis1=[]\\n\",\n",
      " \"for name in glob.glob('*.ipynb'):\\n\",\n",
      " \"    with open (name, encoding=\\\"utf8\\\", errors='coerce') as f: \\n\",\n",
      " \"        data = json.load(f)\\n\",\n",
      " \"        for cell in data ['cells']:\\n\",\n",
      " \"            if cell['cell_type'] == 'code':\\n\",\n",
      " \"                lis1=json.dumps(cell['source'], indent=1)\\n\",\n",
      " \"                print(lis1)\\n\",\n",
      " \"                \\n\",\n",
      " \"#citation : https://www.geeksforgeeks.org/working-with-json-data-in-python/\\n\",\n",
      " \"#citation: https://www.youtube.com/watch?v=Uh2ebFW8OYM\"\n",
      "]\n",
      "[\n",
      " \"type(lis1)\"\n",
      "]\n",
      "[\n",
      " \"#assigning import and from to imp and frm respectively\\n\",\n",
      " \"imp = \\\"import\\\"\\n\",\n",
      " \"frm = \\\"from\\\"\\n\"\n",
      "]\n",
      "[\n",
      " \"#printing the values with \\\"import and from\\\" and printing the remaining string values\\n\",\n",
      " \"list1=[]\\n\",\n",
      " \"for name in glob.glob('*.ipynb'):\\n\",\n",
      " \"    with open (name, mode='r',encoding=\\\"utf\\\", errors='coerce') as f: \\n\",\n",
      " \"        data = json.load(f)\\n\",\n",
      " \"        for cell in data ['cells']:\\n\",\n",
      " \"            if cell['cell_type'] == 'code':\\n\",\n",
      " \"                for i in cell['source']:\\n\",\n",
      " \"                    #print(i)\\n\",\n",
      " \"                    if (imp in i):\\n\",\n",
      " \"                        i = i.replace(imp, '')\\n\",\n",
      " \"                        i = i.replace(frm, '')\\n\",\n",
      " \"                        list1.append(i)\\n\",\n",
      " \"print(list1)\\n\",\n",
      " \"#citation : https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\\n\",\n",
      " \"#citation : https://stackoverflow.com/questions/5884517/python-assign-print-output-to-a-variable\\n\",\n",
      " \"#citation : https://stackoverflow.com/questions/12572362/get-a-string-after-a-specific-substring\\n\",\n",
      " \"#citation : https://www.programiz.com/python-programming/methods/list/append\\n\",\n",
      " \"#citation: https://towardsdatascience.com/python-basics-6-lists-and-list-manipulation-a56be62b1f95\"\n",
      "]\n",
      "[\n",
      " \"type(list1)\"\n",
      "]\n",
      "[\n",
      " \"lis2= [x.replace('\\\\n', '') for x in list1] \\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/3136689/find-and-replace-string-values-in-list\"\n",
      "]\n",
      "[\n",
      " \"lis2\"\n",
      "]\n",
      "[\n",
      " \"#converting lis2 into dataframe using pandas\\n\",\n",
      " \"\\n\",\n",
      " \"data2 = pd.DataFrame(lis2)  \\n\",\n",
      " \"\\n\",\n",
      " \"#citation :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\"\n",
      "]\n",
      "[\n",
      " \"#remaming the column\\n\",\n",
      " \"\\n\",\n",
      " \"data2.columns = ['module']\"\n",
      "]\n",
      "[\n",
      " \"data2\"\n",
      "]\n",
      "[\n",
      " \"#using strip()\\n\",\n",
      " \"\\n\",\n",
      " \"data2['module']=data2['module'].str.strip()\\n\",\n",
      " \"\\n\",\n",
      " \"#citation : https://github.com/getpelican/pelican/issues/1317\\n\",\n",
      " \"#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.strip.html\"\n",
      "]\n",
      "[\n",
      " \"#dropping the rows which contains '#' and assigning to 'ind'\\n\",\n",
      " \"ind = data2[data2['module'].str.startswith('#')]\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://howchoo.com/g/zdvmogrlngz/python-regexes-findall-search-and-match\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/54485186/remove-all-rows-with-a-special-character-in-pandas\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/52229998/remove-all-rows-that-meet-regex-condition/52230096\"\n",
      "]\n",
      "[\n",
      " \"#displaying the index of 'ind'\\n\",\n",
      " \"ind.index\"\n",
      "]\n",
      "[\n",
      " \"#the indexes with '#' are dropped.\\n\",\n",
      " \"\\n\",\n",
      " \"d=data2.drop(ind.index)\\n\",\n",
      " \"d\\n\",\n",
      " \"#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\\n\"\n",
      "]\n",
      "[\n",
      " \"#unique values in the data frame are displayed.\\n\",\n",
      " \"\\n\",\n",
      " \"d1= d.module.unique()\\n\",\n",
      " \"d1\\n\",\n",
      " \"#citation: https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/\"\n",
      "]\n",
      "[]\n",
      "[]\n",
      "[\n",
      " \"# word count \\n\",\n",
      " \"\\n\",\n",
      " \"OUT = 0\\n\",\n",
      " \"IN = 1\\n\",\n",
      " \"\\n\",\n",
      " \"#function definition \\n\",\n",
      " \"def countwords(string):\\n\",\n",
      " \"    state = OUT\\n\",\n",
      " \"    wc = 0\\n\",\n",
      " \"\\n\",\n",
      " \"    # Scan all characters one by one\\n\",\n",
      " \"    for i in range(len(string)):\\n\",\n",
      " \"        # If next character is a separator,set the state as OUT\\n\",\n",
      " \"        # set the state as OUT\\n\",\n",
      " \"        if (string[i] == ' ' or string[i] == '\\\\n' or\\n\",\n",
      " \"                string[i] == '\\\\t'):\\n\",\n",
      " \"            state = OUT\\n\",\n",
      " \"\\n\",\n",
      " \"        # If next character is not a word\\n\",\n",
      " \"        # separator and state is OUT, then\\n\",\n",
      " \"        # set the state as IN and increment\\n\",\n",
      " \"        # word count\\n\",\n",
      " \"        elif state == OUT:\\n\",\n",
      " \"            state = IN\\n\",\n",
      " \"            wc += 1\\n\",\n",
      " \"\\n\",\n",
      " \"    # Return the number of words\\n\",\n",
      " \"    return wc\\n\",\n",
      " \"\\n\",\n",
      " \"string = input(\\\"enter a sentence --->  \\\")\\n\",\n",
      " \"countwords(string)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/2932511/word-count-on-a-string\"\n",
      "]\n",
      "[\n",
      " \"#Character count\\n\",\n",
      " \"\\n\",\n",
      " \"#accepting input from the user\\n\",\n",
      " \"my_string=input(\\\"enter a word or a string ----> \\\");\\n\",\n",
      " \"\\n\",\n",
      " \"#function definition\\n\",\n",
      " \"def char_count(my_string):\\n\",\n",
      " \"    #counting the letters and not ounting spaces if any\\n\",\n",
      " \"    count = len(my_string) - my_string.count(\\\" \\\")\\n\",\n",
      " \"    return(count)\\n\",\n",
      " \"char_count(my_string)\\n\",\n",
      " \"\\n\",\n",
      " \"#citation: https://stackoverflow.com/questions/2932511/letter-count-on-a-string\"\n",
      "]\n",
      "[]\n",
      "[\n",
      " \"!pip install xlsxwriter\"\n",
      "]\n",
      "[\n",
      " \"import pandas as pd\\n\",\n",
      " \"print('pandas',pd.__version__)\"\n",
      "]\n",
      "[\n",
      " \"# Create some Pandas dataframes from some data.\\n\",\n",
      " \"df1 = pd.DataFrame({'Data': [11, 12, 13, 14]})\\n\",\n",
      " \"df2 = pd.DataFrame({'Data': [21, 22, 23, 24]})\\n\",\n",
      " \"df3 = pd.DataFrame({'Data': [31, 32, 33, 34]})\"\n",
      "]\n",
      "[\n",
      " \"# Create a Pandas Excel writer using XlsxWriter as the engine.\\n\",\n",
      " \"excel_file='tmp_multiple.xlsx'\\n\",\n",
      " \"writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\"\n",
      "]\n",
      "[\n",
      " \"# Write each dataframe to a different worksheet.\\n\",\n",
      " \"df1.to_excel(writer, sheet_name='Sheet1')\\n\",\n",
      " \"df2.to_excel(writer, sheet_name='Sheet2')\\n\",\n",
      " \"df3.to_excel(writer, sheet_name='Sheet3')\"\n",
      "]\n",
      "[\n",
      " \"# Close the Pandas Excel writer and output the Excel file.\\n\",\n",
      " \"writer.save()\"\n",
      "]\n",
      "[\n",
      " \"df=pd.read_excel(excel_file)\\n\",\n",
      " \"df\"\n",
      "]\n",
      "[\n",
      " \"df1=pd.read_excel(excel_file,sheet_name=1)\\n\",\n",
      " \"df1\"\n",
      "]\n",
      "[\n",
      " \"xls = pd.ExcelFile(excel_file)\\n\",\n",
      " \"df1 = pd.read_excel(xls, 'Sheet1')\\n\",\n",
      " \"df2 = pd.read_excel(xls, 'Sheet2')\"\n",
      "]\n",
      "[\n",
      " \"df1\"\n",
      "]\n",
      "[\n",
      " \"df2\"\n",
      "]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#the cell_type with \"code\" are getting displayed\n",
    "lis1=[]\n",
    "for name in glob.glob('*.ipynb'):\n",
    "    with open (name, encoding=\"utf8\", errors='coerce') as f: \n",
    "        data = json.load(f)\n",
    "        for cell in data ['cells']:\n",
    "            if cell['cell_type'] == 'code':\n",
    "                lis1=json.dumps(cell['source'], indent=1)\n",
    "                print(lis1)\n",
    "                \n",
    "#citation : https://www.geeksforgeeks.org/working-with-json-data-in-python/\n",
    "#citation: https://www.youtube.com/watch?v=Uh2ebFW8OYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning import and from to imp and frm respectively\n",
    "imp = \"import\"\n",
    "frm = \"from\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#ing openpyxl\\n', ' openpyxl\\n', ' pandas as pd\\n', '#ing the data to the new xlsx file into our local.\\n', ' xml.etree.ElementTree as ET\\n', ' bs4  BeautifulSoup\\n', ' lxml  etree\\n', '# the data. Reading the file  disk:\\n', ' xml.etree.ElementTree as ET\\n', ' bs4  BeautifulSoup\\n', ' lxml  etree\\n', '# the data. Reading the file  disk:\\n', ' os \\n', ' nltk\\n', ' nltk.tokenize  RegexpTokenizer\\n', ' string  punctuation\\n', ' nltk.tokenize  sent_tokenize,word_tokenize \\n', ' nltk.corpus  stopwords\\n', ' string\\n', ' re\\n', ' os.path\\n', ' glob', ' nltk.corpus  stopwords\\n', '     nltk.corpus  stopwords\\n', ' string  punctuation\\n', ' nltk.tokenize  sent_tokenize,word_tokenize \\n', ' pandas as pd\\n', ' os', ' xlsxwriter\\n', ' pandas\\n', ' openpyxl', ' random', ' pandas', ' doctest', '#Started by ing faker libraries and pandas\\n', ' faker  Faker\\n', ' pandas as pd\\n', ' random\\n', ' csv\\n', '#Citation :https://medium.com/@kasiarachuta/ing-and-exporting-csv-files-in-python-7fa6e4d9f408', ' pandas\\n', ' sqlite3\\n', ' matplotlib.pyplot as plt', ' pandas as pd\\n', ' numpy as np\\n', ' random\\n', ' matplotlib.pyplot as plt\\n', ' pandas  ExcelWriter\\n', ' pandas  ExcelFile\\n', ' pandas.plotting  lag_plot\\n', ' re', ' matplotlib.pylab as plt\\n', ' numpy as np\\n', ' sklearn.datasets.samples_generator  make_blobs\\n', ' sklearn.metrics  pairwise_distances_argmin', ' matplotlib.pylab as plt\\n', ' numpy as np\\n', ' sklearn.datasets.samples_generator  make_blobs\\n', ' sklearn.metrics  pairwise_distances_argmin', ' pandas as pd\\n', ' numpy as np\\n', ' chardet\\n', '# ascii\\n', ' string\\n', ' sys\\n', ' re\\n', ' csv\\n', ' os\\n', ' matplotlib.pyplot as plt\\n', ' pandas.plotting  lag_plot\\n', ' matplotlib.patches as mpatches\\n', ' matplotlib  rc\\n', ' urllib.request  urlopen\\n', ' networkx as net', ' time\\n', ' matplotlib.pyplot as plt\\n', ' pandas\\n', ' random', ' timeit\\n', ' random\\n', ' timeit\\n', ' time', ' pandas\\n', ' matplotlib\\n', ' matplotlib.pyplot as plt\\n', ' seaborn\\n', ' random\\n', ' pandas\\n', ' matplotlib.pyplot as plt\\n', ' numpy\\n', ' pandas\\n', ' matplotlib.pyplot as plt\\n', ' numpy\\n', ' pandas as pd\\n', ' numpy as np\\n', ' chardet\\n', '# ascii\\n', ' string\\n', ' sys\\n', ' re\\n', ' csv\\n', ' os\\n', ' matplotlib.pyplot as plt\\n', ' pandas.plotting  lag_plot', ' nltk', ' nltk.tokenize  sent_tokenize,word_tokenize ', ' nltk.corpus  stopwords\\n', ' pandas as pd\\n', ' numpy as np\\n', ' chardet\\n', ' ascii\\n', ' string\\n', ' sys\\n', ' re\\n', ' csv\\n', ' os\\n', ' matplotlib.pyplot as plt\\n', '#citation:https://www.geeksforgeeks.org/different-ways-to--csv-file-in-pandas/', ' math\\n', ' string\\n', ' os\\n', ' re\\n', ' os.path\\n', ' glob\\n', ' fnmatch\\n', ' nltk\\n', ' errno\\n', ' nltk.corpus  stopwords \\n', ' nltk.tokenize  word_tokenize \\n', ' nltk.tokenize  RegexpTokenizer\\n', ' docx  Document\\n', ' nltk.tokenize  sent_tokenize,word_tokenize \\n', ' openpyxl', ' math\\n', ' os\\n', ' fnmatch # https://docs.python.org/3/library/fnmatch.html', '        # first 40 words by ance\\n', ' pandas as pd\\n', ' numpy as np\\n', ' chardet\\n', '# ascii\\n', ' string\\n', ' sys\\n', ' re\\n', ' csv\\n', ' os\\n', ' matplotlib.pyplot as plt\\n', ' pandas.plotting  lag_plot\\n', ' matplotlib.patches as mpatches\\n', ' matplotlib  rc\\n', '#citation:https://www.geeksforgeeks.org/different-ways-to--csv-file-in-pandas/', ' pandas\\n', ' pandas as pd\\n', ' numpy as np\\n', ' chardet\\n', ' string\\n', ' sys\\n', ' re\\n', ' csv\\n', ' os\\n', ' matplotlib.pyplot as plt\\n', ' pandas\\n', ' sqlite3\\n', '#ing the datetime libraries\\n', ' datetime  datetime\\n', ' pandas\\n', ' matplotlib\\n', ' matplotlib.pyplot as plt', ' datetime  datetime', ' pandas\\n', ' pandas\\n', ' pandas\\n', ' matplotlib\\n', ' matplotlib.pyplot as plt\\n', ' pandas\\n', ' matplotlib\\n', ' matplotlib.pyplot as plt', ' glob\\n', ' io, os, sys, types\\n', ' IPython  get_ipython\\n', ' nbformat  read\\n', ' IPython.core.interactiveshell  InteractiveShell\\n', ' json\\n', ' re\\n', ' pandas as pd', '#citation: https://stackoverflow.com/questions/20186344/ipynb--another-ipynb-file\\n', '#assigning  and  to imp and frm respectively\\n', 'imp = \"\"\\n', '#printing the values with \" and \" and printing the remaining string values\\n', ' pandas as pd\\n']\n"
     ]
    }
   ],
   "source": [
    "#printing the values with \"import and from\" and printing the remaining string values\n",
    "list1=[]\n",
    "for name in glob.glob('*.ipynb'):\n",
    "    with open (name, mode='r',encoding=\"utf\", errors='coerce') as f: \n",
    "        data = json.load(f)\n",
    "        for cell in data ['cells']:\n",
    "            if cell['cell_type'] == 'code':\n",
    "                for i in cell['source']:\n",
    "                    #print(i)\n",
    "                    if (imp in i):\n",
    "                        i = i.replace(imp, '')\n",
    "                        i = i.replace(frm, '')\n",
    "                        list1.append(i)\n",
    "print(list1)\n",
    "#citation : https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "#citation : https://stackoverflow.com/questions/5884517/python-assign-print-output-to-a-variable\n",
    "#citation : https://stackoverflow.com/questions/12572362/get-a-string-after-a-specific-substring\n",
    "#citation : https://www.programiz.com/python-programming/methods/list/append\n",
    "#citation: https://towardsdatascience.com/python-basics-6-lists-and-list-manipulation-a56be62b1f95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis2= [x.replace('\\n', '') for x in list1] \n",
    "\n",
    "#citation: https://stackoverflow.com/questions/3136689/find-and-replace-string-values-in-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ing openpyxl',\n",
       " ' openpyxl',\n",
       " ' pandas as pd',\n",
       " '#ing the data to the new xlsx file into our local.',\n",
       " ' xml.etree.ElementTree as ET',\n",
       " ' bs4  BeautifulSoup',\n",
       " ' lxml  etree',\n",
       " '# the data. Reading the file  disk:',\n",
       " ' xml.etree.ElementTree as ET',\n",
       " ' bs4  BeautifulSoup',\n",
       " ' lxml  etree',\n",
       " '# the data. Reading the file  disk:',\n",
       " ' os ',\n",
       " ' nltk',\n",
       " ' nltk.tokenize  RegexpTokenizer',\n",
       " ' string  punctuation',\n",
       " ' nltk.tokenize  sent_tokenize,word_tokenize ',\n",
       " ' nltk.corpus  stopwords',\n",
       " ' string',\n",
       " ' re',\n",
       " ' os.path',\n",
       " ' glob',\n",
       " ' nltk.corpus  stopwords',\n",
       " '     nltk.corpus  stopwords',\n",
       " ' string  punctuation',\n",
       " ' nltk.tokenize  sent_tokenize,word_tokenize ',\n",
       " ' pandas as pd',\n",
       " ' os',\n",
       " ' xlsxwriter',\n",
       " ' pandas',\n",
       " ' openpyxl',\n",
       " ' random',\n",
       " ' pandas',\n",
       " ' doctest',\n",
       " '#Started by ing faker libraries and pandas',\n",
       " ' faker  Faker',\n",
       " ' pandas as pd',\n",
       " ' random',\n",
       " ' csv',\n",
       " '#Citation :https://medium.com/@kasiarachuta/ing-and-exporting-csv-files-in-python-7fa6e4d9f408',\n",
       " ' pandas',\n",
       " ' sqlite3',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas as pd',\n",
       " ' numpy as np',\n",
       " ' random',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas  ExcelWriter',\n",
       " ' pandas  ExcelFile',\n",
       " ' pandas.plotting  lag_plot',\n",
       " ' re',\n",
       " ' matplotlib.pylab as plt',\n",
       " ' numpy as np',\n",
       " ' sklearn.datasets.samples_generator  make_blobs',\n",
       " ' sklearn.metrics  pairwise_distances_argmin',\n",
       " ' matplotlib.pylab as plt',\n",
       " ' numpy as np',\n",
       " ' sklearn.datasets.samples_generator  make_blobs',\n",
       " ' sklearn.metrics  pairwise_distances_argmin',\n",
       " ' pandas as pd',\n",
       " ' numpy as np',\n",
       " ' chardet',\n",
       " '# ascii',\n",
       " ' string',\n",
       " ' sys',\n",
       " ' re',\n",
       " ' csv',\n",
       " ' os',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas.plotting  lag_plot',\n",
       " ' matplotlib.patches as mpatches',\n",
       " ' matplotlib  rc',\n",
       " ' urllib.request  urlopen',\n",
       " ' networkx as net',\n",
       " ' time',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas',\n",
       " ' random',\n",
       " ' timeit',\n",
       " ' random',\n",
       " ' timeit',\n",
       " ' time',\n",
       " ' pandas',\n",
       " ' matplotlib',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' seaborn',\n",
       " ' random',\n",
       " ' pandas',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' numpy',\n",
       " ' pandas',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' numpy',\n",
       " ' pandas as pd',\n",
       " ' numpy as np',\n",
       " ' chardet',\n",
       " '# ascii',\n",
       " ' string',\n",
       " ' sys',\n",
       " ' re',\n",
       " ' csv',\n",
       " ' os',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas.plotting  lag_plot',\n",
       " ' nltk',\n",
       " ' nltk.tokenize  sent_tokenize,word_tokenize ',\n",
       " ' nltk.corpus  stopwords',\n",
       " ' pandas as pd',\n",
       " ' numpy as np',\n",
       " ' chardet',\n",
       " ' ascii',\n",
       " ' string',\n",
       " ' sys',\n",
       " ' re',\n",
       " ' csv',\n",
       " ' os',\n",
       " ' matplotlib.pyplot as plt',\n",
       " '#citation:https://www.geeksforgeeks.org/different-ways-to--csv-file-in-pandas/',\n",
       " ' math',\n",
       " ' string',\n",
       " ' os',\n",
       " ' re',\n",
       " ' os.path',\n",
       " ' glob',\n",
       " ' fnmatch',\n",
       " ' nltk',\n",
       " ' errno',\n",
       " ' nltk.corpus  stopwords ',\n",
       " ' nltk.tokenize  word_tokenize ',\n",
       " ' nltk.tokenize  RegexpTokenizer',\n",
       " ' docx  Document',\n",
       " ' nltk.tokenize  sent_tokenize,word_tokenize ',\n",
       " ' openpyxl',\n",
       " ' math',\n",
       " ' os',\n",
       " ' fnmatch # https://docs.python.org/3/library/fnmatch.html',\n",
       " '        # first 40 words by ance',\n",
       " ' pandas as pd',\n",
       " ' numpy as np',\n",
       " ' chardet',\n",
       " '# ascii',\n",
       " ' string',\n",
       " ' sys',\n",
       " ' re',\n",
       " ' csv',\n",
       " ' os',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas.plotting  lag_plot',\n",
       " ' matplotlib.patches as mpatches',\n",
       " ' matplotlib  rc',\n",
       " '#citation:https://www.geeksforgeeks.org/different-ways-to--csv-file-in-pandas/',\n",
       " ' pandas',\n",
       " ' pandas as pd',\n",
       " ' numpy as np',\n",
       " ' chardet',\n",
       " ' string',\n",
       " ' sys',\n",
       " ' re',\n",
       " ' csv',\n",
       " ' os',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas',\n",
       " ' sqlite3',\n",
       " '#ing the datetime libraries',\n",
       " ' datetime  datetime',\n",
       " ' pandas',\n",
       " ' matplotlib',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' datetime  datetime',\n",
       " ' pandas',\n",
       " ' pandas',\n",
       " ' pandas',\n",
       " ' matplotlib',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' pandas',\n",
       " ' matplotlib',\n",
       " ' matplotlib.pyplot as plt',\n",
       " ' glob',\n",
       " ' io, os, sys, types',\n",
       " ' IPython  get_ipython',\n",
       " ' nbformat  read',\n",
       " ' IPython.core.interactiveshell  InteractiveShell',\n",
       " ' json',\n",
       " ' re',\n",
       " ' pandas as pd',\n",
       " '#citation: https://stackoverflow.com/questions/20186344/ipynb--another-ipynb-file',\n",
       " '#assigning  and  to imp and frm respectively',\n",
       " 'imp = \"\"',\n",
       " '#printing the values with \" and \" and printing the remaining string values',\n",
       " ' pandas as pd']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting lis2 into dataframe using pandas\n",
    "\n",
    "data2 = pd.DataFrame(lis2)  \n",
    "\n",
    "#citation :https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remaming the column\n",
    "\n",
    "data2.columns = ['module']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>module</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#ing openpyxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openpyxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#ing the data to the new xlsx file into our lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xml.etree.ElementTree as ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bs4  BeautifulSoup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lxml  etree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td># the data. Reading the file  disk:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xml.etree.ElementTree as ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bs4  BeautifulSoup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lxml  etree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td># the data. Reading the file  disk:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nltk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nltk.tokenize  RegexpTokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>string  punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nltk.tokenize  sent_tokenize,word_tokenize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nltk.corpus  stopwords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>os.path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>glob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nltk.corpus  stopwords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nltk.corpus  stopwords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>string  punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nltk.tokenize  sent_tokenize,word_tokenize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>xlsxwriter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>sqlite3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>#ing the datetime libraries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>datetime  datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>matplotlib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>datetime  datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>matplotlib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>matplotlib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>glob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>io, os, sys, types</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>IPython  get_ipython</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>nbformat  read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>IPython.core.interactiveshell  InteractiveShell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>#citation: https://stackoverflow.com/questions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>#assigning  and  to imp and frm respectively</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>imp = \"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>#printing the values with \" and \" and printing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                module\n",
       "0                                        #ing openpyxl\n",
       "1                                             openpyxl\n",
       "2                                         pandas as pd\n",
       "3    #ing the data to the new xlsx file into our lo...\n",
       "4                          xml.etree.ElementTree as ET\n",
       "5                                   bs4  BeautifulSoup\n",
       "6                                          lxml  etree\n",
       "7                  # the data. Reading the file  disk:\n",
       "8                          xml.etree.ElementTree as ET\n",
       "9                                   bs4  BeautifulSoup\n",
       "10                                         lxml  etree\n",
       "11                 # the data. Reading the file  disk:\n",
       "12                                                 os \n",
       "13                                                nltk\n",
       "14                      nltk.tokenize  RegexpTokenizer\n",
       "15                                 string  punctuation\n",
       "16         nltk.tokenize  sent_tokenize,word_tokenize \n",
       "17                              nltk.corpus  stopwords\n",
       "18                                              string\n",
       "19                                                  re\n",
       "20                                             os.path\n",
       "21                                                glob\n",
       "22                              nltk.corpus  stopwords\n",
       "23                              nltk.corpus  stopwords\n",
       "24                                 string  punctuation\n",
       "25         nltk.tokenize  sent_tokenize,word_tokenize \n",
       "26                                        pandas as pd\n",
       "27                                                  os\n",
       "28                                          xlsxwriter\n",
       "29                                              pandas\n",
       "..                                                 ...\n",
       "160                           matplotlib.pyplot as plt\n",
       "161                                             pandas\n",
       "162                                            sqlite3\n",
       "163                        #ing the datetime libraries\n",
       "164                                 datetime  datetime\n",
       "165                                             pandas\n",
       "166                                         matplotlib\n",
       "167                           matplotlib.pyplot as plt\n",
       "168                                 datetime  datetime\n",
       "169                                             pandas\n",
       "170                                             pandas\n",
       "171                                             pandas\n",
       "172                                         matplotlib\n",
       "173                           matplotlib.pyplot as plt\n",
       "174                                             pandas\n",
       "175                                         matplotlib\n",
       "176                           matplotlib.pyplot as plt\n",
       "177                                               glob\n",
       "178                                 io, os, sys, types\n",
       "179                               IPython  get_ipython\n",
       "180                                     nbformat  read\n",
       "181    IPython.core.interactiveshell  InteractiveShell\n",
       "182                                               json\n",
       "183                                                 re\n",
       "184                                       pandas as pd\n",
       "185  #citation: https://stackoverflow.com/questions...\n",
       "186       #assigning  and  to imp and frm respectively\n",
       "187                                           imp = \"\"\n",
       "188  #printing the values with \" and \" and printing...\n",
       "189                                       pandas as pd\n",
       "\n",
       "[190 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using strip()\n",
    "\n",
    "data2['module']=data2['module'].str.strip()\n",
    "\n",
    "#citation : https://github.com/getpelican/pelican/issues/1317\n",
    "#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.strip.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the rows which contains '#' and assigning to 'ind'\n",
    "ind = data2[data2['module'].str.startswith('#')]\n",
    "\n",
    "#citation: https://howchoo.com/g/zdvmogrlngz/python-regexes-findall-search-and-match\n",
    "#citation: https://stackoverflow.com/questions/54485186/remove-all-rows-with-a-special-character-in-pandas\n",
    "#citation: https://stackoverflow.com/questions/52229998/remove-all-rows-that-meet-regex-condition/52230096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 3, 7, 11, 34, 39, 62, 96, 117, 136, 140, 150, 163, 185, 186,\n",
       "            188],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the index of 'ind'\n",
    "ind.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>module</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openpyxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xml.etree.ElementTree as ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bs4  BeautifulSoup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lxml  etree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xml.etree.ElementTree as ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bs4  BeautifulSoup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lxml  etree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nltk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nltk.tokenize  RegexpTokenizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>string  punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nltk.tokenize  sent_tokenize,word_tokenize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nltk.corpus  stopwords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>os.path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>glob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nltk.corpus  stopwords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nltk.corpus  stopwords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>string  punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nltk.tokenize  sent_tokenize,word_tokenize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>xlsxwriter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>openpyxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>doctest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>sys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>os</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>sqlite3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>datetime  datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>matplotlib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>datetime  datetime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>matplotlib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>matplotlib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>matplotlib.pyplot as plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>glob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>io, os, sys, types</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>IPython  get_ipython</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>nbformat  read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>IPython.core.interactiveshell  InteractiveShell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>imp = \"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>pandas as pd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              module\n",
       "1                                           openpyxl\n",
       "2                                       pandas as pd\n",
       "4                        xml.etree.ElementTree as ET\n",
       "5                                 bs4  BeautifulSoup\n",
       "6                                        lxml  etree\n",
       "8                        xml.etree.ElementTree as ET\n",
       "9                                 bs4  BeautifulSoup\n",
       "10                                       lxml  etree\n",
       "12                                                os\n",
       "13                                              nltk\n",
       "14                    nltk.tokenize  RegexpTokenizer\n",
       "15                               string  punctuation\n",
       "16        nltk.tokenize  sent_tokenize,word_tokenize\n",
       "17                            nltk.corpus  stopwords\n",
       "18                                            string\n",
       "19                                                re\n",
       "20                                           os.path\n",
       "21                                              glob\n",
       "22                            nltk.corpus  stopwords\n",
       "23                            nltk.corpus  stopwords\n",
       "24                               string  punctuation\n",
       "25        nltk.tokenize  sent_tokenize,word_tokenize\n",
       "26                                      pandas as pd\n",
       "27                                                os\n",
       "28                                        xlsxwriter\n",
       "29                                            pandas\n",
       "30                                          openpyxl\n",
       "31                                            random\n",
       "32                                            pandas\n",
       "33                                           doctest\n",
       "..                                               ...\n",
       "156                                              sys\n",
       "157                                               re\n",
       "158                                              csv\n",
       "159                                               os\n",
       "160                         matplotlib.pyplot as plt\n",
       "161                                           pandas\n",
       "162                                          sqlite3\n",
       "164                               datetime  datetime\n",
       "165                                           pandas\n",
       "166                                       matplotlib\n",
       "167                         matplotlib.pyplot as plt\n",
       "168                               datetime  datetime\n",
       "169                                           pandas\n",
       "170                                           pandas\n",
       "171                                           pandas\n",
       "172                                       matplotlib\n",
       "173                         matplotlib.pyplot as plt\n",
       "174                                           pandas\n",
       "175                                       matplotlib\n",
       "176                         matplotlib.pyplot as plt\n",
       "177                                             glob\n",
       "178                               io, os, sys, types\n",
       "179                             IPython  get_ipython\n",
       "180                                   nbformat  read\n",
       "181  IPython.core.interactiveshell  InteractiveShell\n",
       "182                                             json\n",
       "183                                               re\n",
       "184                                     pandas as pd\n",
       "187                                         imp = \"\"\n",
       "189                                     pandas as pd\n",
       "\n",
       "[174 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the indexes with '#' are dropped.\n",
    "\n",
    "d=data2.drop(ind.index)\n",
    "d\n",
    "#citation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['openpyxl', 'pandas as pd', 'xml.etree.ElementTree as ET',\n",
       "       'bs4  BeautifulSoup', 'lxml  etree', 'os', 'nltk',\n",
       "       'nltk.tokenize  RegexpTokenizer', 'string  punctuation',\n",
       "       'nltk.tokenize  sent_tokenize,word_tokenize',\n",
       "       'nltk.corpus  stopwords', 'string', 're', 'os.path', 'glob',\n",
       "       'xlsxwriter', 'pandas', 'random', 'doctest', 'faker  Faker', 'csv',\n",
       "       'sqlite3', 'matplotlib.pyplot as plt', 'numpy as np',\n",
       "       'pandas  ExcelWriter', 'pandas  ExcelFile',\n",
       "       'pandas.plotting  lag_plot', 'matplotlib.pylab as plt',\n",
       "       'sklearn.datasets.samples_generator  make_blobs',\n",
       "       'sklearn.metrics  pairwise_distances_argmin', 'chardet', 'sys',\n",
       "       'matplotlib.patches as mpatches', 'matplotlib  rc',\n",
       "       'urllib.request  urlopen', 'networkx as net', 'time', 'timeit',\n",
       "       'matplotlib', 'seaborn', 'numpy', 'ascii', 'math', 'fnmatch',\n",
       "       'errno', 'nltk.tokenize  word_tokenize', 'docx  Document',\n",
       "       'fnmatch # https://docs.python.org/3/library/fnmatch.html',\n",
       "       'datetime  datetime', 'io, os, sys, types', 'IPython  get_ipython',\n",
       "       'nbformat  read',\n",
       "       'IPython.core.interactiveshell  InteractiveShell', 'json',\n",
       "       'imp = \"\"'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique values in the data frame are displayed.\n",
    "\n",
    "d1= d.module.unique()\n",
    "d1\n",
    "#citation: https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
